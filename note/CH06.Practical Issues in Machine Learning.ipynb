{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.06 Practical Issues in Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Bias-Variance tradeoff\n",
    "6.1.1. Bias-Variance decomposition of the MSE<br>\n",
    "For any supervised learning algorithm, we can decompose the MSE on an unseen sample $\\mathbf{x}$ as\n",
    "$$ \\mathbb{E}[(y - \\hat{f}(x; D))^2] = (Bias_D[\\hat{f}(x; D)])^2 + Var_D[\\hat{f}(x; D)] + \\sigma^2 $$\n",
    "$$ \\text{where} \\quad Bias_D[\\hat{f}(x;D)] = \\mathbb{E}_D[\\hat{f}(x;D)] - f(x) \\,\\ \\text{and} \\,\\ Var_D[\\hat{f}(x;D)] = \\mathbb{E}_D[(\\mathbb{E}_D[\\hat{f}(x ; D)] - \\hat{f}(x;D))^2 ] $$\n",
    "\n",
    "- $Bias_D[\\hat{f}(x;D)]$ is due to improper model or assumption.(e.g., When approximating a non-linear funcvtion $f(x)$ using a learning method for linear models, there will be error in the estimates $\\hat{f}(x)$ due to this assumption).\n",
    "- $Var_D[\\hat{f}(x;D)]$ is variation of an algorithm itself.\n",
    "- $\\sigma^2$ is inherent noise(irreducible error).<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF File too long](./res/ch06/note_bias-variance_tradeoff.pdf)\n",
    "\n",
    "Therefore, \n",
    "$$ \\text{MSE} = \\mathbb{E}_x \\left\\{ Bias_D[\\hat{f}(x;D)]^2 + Var_D[\\hat{f}(x;D)] \\right\\} + \\sigma^2 $$\n",
    "\n",
    "\n",
    "<img src=\"./res/ch06/fig_1_1.png\" width=\"400\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.6.1.1\n",
    "</div>\n",
    "\n",
    "\n",
    "- The more complex the model is, the more data points it will capture. $\\rightarrow$ the lower the bias will be.\n",
    "- However, the complexity will make the model vary more to capture the data points $\\rightarrow$ the larger the variance will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Generalization\n",
    "6.2.1. Overview<br>\n",
    "- The ultimate goal of machine learning is __good generalization__.\n",
    "- Data we observed is just a part of the whole.\n",
    "- We need to find a model that well explains the whole data only from a given portion of data.\n",
    "- Good explainability for unobserved data.\n",
    "\n",
    "Generalization depends on __amount of training data__ and __complexity of model__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.2. Training and test data sets<br>\n",
    "\n",
    "\n",
    "<img src=\"./res/ch06/fig_2_1.png\" width=\"400\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.6.2.1\n",
    "</div>\n",
    "\n",
    "- The whole training data is split into two parts: (i) training set and (ii) test set\n",
    "- Usually, the training set is used for training model (or a machine learning algorithm)\n",
    "- Whereas, the test set is used to evaluate the (generalization) performance of the model\n",
    "\n",
    "For example, in polynomial curve fitting with training and test sets\n",
    "\n",
    "<img src=\"./res/ch06/fig_2_2.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.6.2.2\n",
    "</div>\n",
    "\n",
    "<img src=\"./res/ch06/fig_2_3.png\" width=\"800\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.6.2.3\n",
    "</div>\n",
    "\n",
    "In above figure.6.2.3, when $ M = 0 $, it is under-fitting, because $ M $ cannot learn any data because there isn't weight value that can fit.<br>\n",
    "When $ M = 1 $, it is under-fitting, because $ M $ can fit just linear data because there is an only one weight value.<br>\n",
    "When $ M = 3 $, it is well-fitting(best model), because the model is a good representation of the nonlinearity of the data.\n",
    "When $ M = 9 $, it is over-fitting, because $ M $ is too large so that the model memorize lots of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Overfitting\n",
    "So, what is overfitting? It is very __good only for training data__ (memorization). But, it is __not good for test data__ (poor generalization). <br><br>\n",
    "\n",
    "6.3.1. How to avoid overfitting?<br>\n",
    "\n",
    "6.3.1.1. More training data<br>\n",
    "   \n",
    "__Widrow's rule of thumb__ \n",
    "\n",
    "$$\n",
    "N = O \\left( \\frac{W}{\\epsilon} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where} \\,\\ N \\,\\ \\text{is number of training samples,} \\,\\ W \\,\\ \\text{is total number of parameters,} \\,\\ \\epsilon \\,\\ \\text{is fraction of target error.}\n",
    "$$\n",
    "\n",
    "For example, when you set $ \\epsilon $ to $ 10% $, it should be\n",
    "\n",
    "$$\n",
    "\\epsilon = 0.1 \\quad \\rightarrow \\quad N \\ge 10W\n",
    "$$\n",
    "\n",
    "6.3.1.2. Reducing the number of features(e.g., by PCA)<br>\n",
    "\n",
    "We have to find a number of suitable features because if there are so many features, the model can learn noises that hinders generalization, and variation of weights is so high.\n",
    "\n",
    "6.3.1.3. Regularization<br>\n",
    "\n",
    "Regularization is restricting the model complexity. It is based on __Occam's razor__ which means the simple is the best.\n",
    "\n",
    "    - L1-Regularization(CH03.02)\n",
    "    - L2-Regularization(CH03.02)\n",
    "    - Max-Norm Constraint\n",
    "    \n",
    "$$\n",
    "|| \\mathbf{w} ||_\\infty \\le u\n",
    "$$\n",
    "\n",
    "There are two type of weights. First one is weights having significant influence on performance. __Second one have little or no influence so these can cause overfitting.__  We have to restricting those weights to be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3.1.4. Dropout<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3.1.5. Early-stopping<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3.1.6. Proper model selection<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
