{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corresponding-feature",
   "metadata": {},
   "source": [
    "# Chapter.05 Classification\n",
    "---\n",
    "### 5.2. Linear Models for Classfication\n",
    "5.2.1. Linear discriminant for two classes<br>\n",
    "If a discriminant function outputs one of the 2 classes corresponding to the input vector $ x $, a linear discriminant function(dicision boundary) for two classes takes the following form:\n",
    "\n",
    "$$ y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + w_0 $$\n",
    "\n",
    "Classification rule is goes like below.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&y \\ge 0 \\Rightarrow \\mathbf{x} \\in C_1 \\\\\n",
    "&y < 0 \\Rightarrow \\mathbf{x} \\in C_2 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Of course perceptron works in a similar way to this. In geometrically, \n",
    "\n",
    "<img src=\"./res/ch05/fig_2_1.png\" width=\"500\" height=\"500\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-aspect",
   "metadata": {},
   "source": [
    "5.2.2. Linear discriminant for multiple classes<br>\n",
    "If there are three classes, above method cannot work. What can we do for this problem?<br><br>\n",
    "\n",
    "- One-versus-the-rest method in $ K $ classes<br>\n",
    "Build $ K-1 $ classifiers, between $ C_k $ and all others.\n",
    "\n",
    "<img src=\"./res/ch05/fig_2_2.png\" width=\"300\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.2\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-confirmation",
   "metadata": {},
   "source": [
    "The first hyperplane can be obtained by classifying $ C_1 $ and $ non-C_1 $. The second hyperplane can be obtained by classifying $ C_2 $ and $ non-C_2 $. So, the region $ R_1 $ must contain $ C_1 $ elements,  $ R_2 $ must contain $ C_2 $ elements, and $ R_3 $ must contain $ C_3 $ elements because that is not $ C_1 $ and $ C_2 $.  However, we can't know about the green area because that can be both $ C_1 $ and $ C_2 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-chester",
   "metadata": {},
   "source": [
    "- One-versus-one method in $ K $ classes<br>\n",
    "Build $ K(K-1)/2 $ classifiers, between all pairs.\n",
    "\n",
    "<img src=\"./res/ch05/fig_2_3.png\" width=\"300\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.3\n",
    "</div>\n",
    "\n",
    "The first hyperplane can be obtained by classifying $ C_1 $ and $ C_2 $. The second hyperplane can be obtained by classifying $ C_1 $ and $ C_3 $. The third hyperplane can be obtained by classifying $ C_2 $ and $ C_3 $. So, the region $ R_1 $ must contain $ C_1 $ elements,  $ R_2 $ must contain $ C_2 $ elements, and $ R_3 $ must contain $ C_3 $ elements. However, we can't know about the green area because that can be $ C_1 $, $ C_2 $, and $ C_3 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-darkness",
   "metadata": {},
   "source": [
    "- Piecewise linear discriminant in $ K $ classes<br>\n",
    "A solution is to build $ K $ linear function :\n",
    "\n",
    "$$\n",
    "y_k(\\mathbf{x}) = \\mathbf{w}_k^T \\mathbf{x} + w_{k0}\n",
    "$$\n",
    "\n",
    "Classification rule : \n",
    "\n",
    "$$\n",
    "^\\forall j, \\quad k = \\arg\\max_j y_j(\\mathbf{x}) \\quad \\Rightarrow \\quad \\mathbf{x} \\in C_k\n",
    "$$\n",
    "\n",
    "<img src=\"./res/ch05/fig_2_4.png\" width=\"500\" height=\"350\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.4\n",
    "</div>\n",
    "\n",
    "Now, decision regions are singly connected and convex.<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\lambda \\mathbf{x}_A + (1 - \\lambda)\\mathbf{x}_B \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\quad y_k(\\hat{\\mathbf{x}}) = \\lambda y_k(\\mathbf{x}_A) + (1 - \\lambda)y_k(\\mathbf{x}_B) \\qquad \\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-format",
   "metadata": {},
   "source": [
    "5.2.3. Linear models for classification<br>\n",
    "There are three major linear discriminant methods for classification.\n",
    " - Least squares for classification\n",
    " - Fisher's linear discriminant(FLT)\n",
    " - Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-recipe",
   "metadata": {},
   "source": [
    "5.2.3.1. Linear model for classification : Least squares for classification<br>\n",
    "Find $ W $ to minimize squared error over all examples and all components of the label vector (in fact MSE is not good at cost function of classification maybe cross entropy cost function is good for classification problem.): \n",
    "\n",
    "$$\n",
    "E(\\mathbf{W}) = \\frac{1}{2} \\sum_{n = 1}^{N} \\sum_{k = 1}^{K} (y_k(\\mathbf{x})_n - t_{nk})^2\n",
    "$$\n",
    "\n",
    "Let's rewrite the squared error in an equivalent form:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{W}) = \\frac{1}{2} \\sum_{k = 1}^{K} || \\mathbf{\\tau}_k - X \\mathbf{w}_k ||^2 = \\frac{1}{2} Tr\\{(XW - T)^T (XW - T)\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where} \\,\\ \\mathbf{\\tau}_k = [t_{1k}, \\cdots, t_{NK}]^T, \\,\\ T = [\\mathbf{\\tau}_1, \\cdots, \\mathbf{\\tau}_N]^T, \\,\\ X = [\\mathbf{x}_1, \\cdots, \\mathbf{x}_N]_{N \\times M}^T, \\,\\ W = [\\mathbf{w}_1, \\cdots, \\mathbf{w}_K]_{M \\times K}, \\,\\ M \\,\\ : \\,\\ \\text{order of model}, \\,\\ N \\,\\ : \\,\\ \\text{number of training examples}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-purple",
   "metadata": {},
   "source": [
    "Differentiating the squared error w.r.t. $W$ and setting it equal to zero, we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(W) &= \\frac{1}{2} Tr\\{(XW-T)^T (XW-T)\\} \\\\\n",
    "     &= \\frac{1}{2} [Tr(W^TX^TXW) - 2 Tr(T^T X W) + Tr(T^TT)] \\\\\n",
    "     &\\Rightarrow \\frac{\\partial E(W)}{\\partial W} = 2X^TXW - 2X^TT = \\mathbf{0} \\quad \\text{(Normal equation for multi-dimensional target)}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore \\quad W_{LS} = (X^TX)^{-1}X^TT\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-index",
   "metadata": {},
   "source": [
    "<img src=\"./res/ch05/fig_2_5.png\" width=\"400\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.5\n",
    "</div>\n",
    "\n",
    "In above result, least squares decision boundary looks good as similar to logistic regression decision boundary. However, what happens if we add easy point?\n",
    "\n",
    "<img src=\"./res/ch05/fig_2_6.png\" width=\"400\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.6\n",
    "</div>\n",
    "\n",
    "It can gets worse by adding easy point because if target value is 1, points far from boundary will have high value, this is a large error so the boundary is moved. In fact this problem is caused by taking MSE as the cost function so that $ y $ must be continuous value. Therefore, it is hard to map $ \\mathbf{x} $ to discrete lable. If there is activation function like threshold function in model, it can be good at this problem. Following is result of multiple case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-processing",
   "metadata": {},
   "source": [
    "<img src=\"./res/ch05/fig_2_7.png\" width=\"900\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.7\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-producer",
   "metadata": {},
   "source": [
    "5.2.3.2. Linear model for classification : Fisher's linear discriminant<br>\n",
    "This is a new look at the linear discriminant; two-class linear discriminant acts as a __projection__, followed by thresholding:\n",
    "\n",
    "$$\n",
    "y = \\mathbf{w}^T \\mathbf{x} \\overset{C_1}{\\underset{C_2}{\\gtrless}} - w_0\n",
    "$$\n",
    "\n",
    "Look at the __Figure.5.2.1.__. In which direction $ w $ should we project? One which separates classes well<br><br>\n",
    "\n",
    "A natural idea would be to project in the direction of the line connecting class means. However, problematic if classes have variance in this direction.\n",
    "\n",
    "Cost : distance between class means\n",
    "\n",
    "$$\n",
    "\\max_{||\\mathbf{w}|| = 1} \\{m_2 - m_1 = \\mathbf{w}^T (\\mathbf{m}_2 - \\mathbf{m}_1)\\} \\quad \\text{where} \\,\\ m_k = \\mathbf{w}^T \\mathbf{m}_k, \\,\\ \\mathbf{m}_1 = \\frac{1}{N_1} \\sum_{n \\in C_1} \\mathbf{x}_n, \\,\\ \\mathbf{m}_2 = \\frac{1}{N_2} \\sum_{n \\in C_2} \\mathbf{x}_n\n",
    "$$\n",
    "\n",
    "<img src=\"./res/ch05/fig_2_8.png\" width=\"400\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.8\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-penetration",
   "metadata": {},
   "source": [
    "So Fisher's criterion is maximizing __ratio of inter-class separation (between) to intra-class variance (inside)__. <br><br>\n",
    "\n",
    "Cost : ratio of inter-class separation (between) to intra-class variance (inside)\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}} \\left\\{ J(\\mathbf{w}) = \\frac{(m_2 - m_1)^2}{s_1^2 + s_2^2} = \\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_W \\mathbf{w}} \\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where} \\,\\ s_k^2 = \\sum_{n \\in C_k} (y_n - m_k)^2 \\,\\ : \\,\\ \\text{Intra-class variance}, \\,\\ S_B = (\\mathbf{m}_2 - \\mathbf{m}_1)(\\mathbf{m}_2 - \\mathbf{m}_1)^T \\,\\ : \\,\\ \\text{Between-class covariance}, \\,\\ S_W = \\sum_{k} \\sum_{k \\in C_k}(\\mathbf{x} - \\mathbf{m}_k)(\\mathbf{x} - \\mathbf{m}_k)^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\propto S_W^{-1} (\\mathbf{m}_2 - \\mathbf{m}_1) \n",
    "$$\n",
    "\n",
    "It means there isn't unique solution of $ \\mathbf{w} $. If $ S_W $ is isotropic(등방성), FLD is the class mean difference vector $ \\mathbf{w} \\propto (\\mathbf{w}_2 - \\mathbf{w}_1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-variation",
   "metadata": {},
   "source": [
    "<img src=\"./res/ch05/fig_2_9.png\" width=\"400\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.2.9\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-trace",
   "metadata": {},
   "source": [
    "<strong>Proof.</strong><br>\n",
    "This problem can be proved as an optimization problem with constraints. \n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}} \\{\\mathbf{w}^T S_B \\mathbf{w} \\} \\quad s.t. \\quad\\mathbf{w}^T S_W \\mathbf{w} = 1\n",
    "$$\n",
    "\n",
    "\n",
    "[PDF File too long](./res/ch05/Ch.5.2.3.Optimization_proof.pdf)     $\\blacksquare$<br><br>\n",
    "\n",
    "Also, it can be proved by Cauchy-Schwartz inequality. [Proof](https://web.stanford.edu/~boyd/papers/pdf/robust_FDA.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-native",
   "metadata": {},
   "source": [
    "So, how to classify with Fisher's linear discriminant for multiple classes?\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = W^T\\mathbf{x} \\quad \\text{where} \\,\\ W = [\\mathbf{w}_1, \\cdots, \\mathbf{w}_k]_{M \\times K} \n",
    "$$\n",
    "\n",
    "(Usually suppose that $ M > K $ for dimensionality reduction) <br><br>\n",
    "\n",
    "Solution : maximize the ratio-trace cost function (suggested by [Fukunaga](https://escholarship.org/content/qt9k52756v/qt9k52756v.pdf?t=lnr96c)).\n",
    "\n",
    "$$\n",
    "\\max_{W^T W = I} \\left\\{ J(W) = Tr\\left[(W^T S_W W)^{-1} (W^T S_B W)\\right] \\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_W = \\sum_{k = 1}^{K} S_k \\quad \\text{where} \\,\\ S_k = \\sum_{n \\in C_k} (\\mathbf{x}_n - \\mathbf{m}_k)(\\mathbf{x}_n - \\mathbf{m}_k)^T, \\,\\ \\mathbf{m}_k = \\frac{1}{N_k} \\sum_{n \\in C_k} \\mathbf{x}_n \\quad \\text{Within-class covariance}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_B = \\sum_{k = 1}^{K} N_k (\\mathbf{m}_k - \\mathbf{m})(\\mathbf{m}_k - \\mathbf{m})^T \\quad \\text{where} \\,\\ \\mathbf{m} = \\frac{1}{N} \\sum_{n = 1}^{N} \\mathbf{x}_n = \\frac{1}{N} \\sum_{k = 1}^{K} N_k \\mathbf{m}_k \\quad \\text{Between-class covariance}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\mathbf{w}^* = \\text{eigenvectors of }S_{W}^{-1}S_B\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-insight",
   "metadata": {},
   "source": [
    "Summary : \n",
    "- FLD is a dimensionality reduction technique, like PCA(FLD is motive of PCA)\n",
    "- Criterion for choosing projection based on class labels\n",
    "- Still suffers from outliers(e.g., earlier least squares example)\n",
    "\n",
    "Linear model : \n",
    "$$\n",
    "\\text{(LS for classification)} < \\text{(Perceptron)} < \\text{(FLD)} < \\text{(SVM)}\n",
    "$$\n",
    "\n",
    "FLD cannot process easy point like LS. Also, FLD isn't good at multiple case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-police",
   "metadata": {},
   "source": [
    "5.2.3.3. Linear model for classification : Perceptron<br>\n",
    "We already did it in Chapter02."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-space",
   "metadata": {},
   "source": [
    "<strong>Reference.</strong><br>\n",
    "http://sas.uwaterloo.ca/~aghodsib/courses/f07stat841/notes/lecture6.pdf<br>\n",
    "https://web.stanford.edu/~boyd/papers/pdf/robust_FDA.pdf<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
