{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Appendix.02 Review of Optimization Theory\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.2.1 Optimization Problem in general programming\n",
    "Let $ f(\\mathbf{x}) \\,\\ : \\,\\ \\mathbb{R}^n \\rightarrow \\mathbb{R} $ called an object or cost function, <br>\n",
    "$ g_i(\\mathbf{x}) \\,\\ : \\,\\ \\mathbb{R}^n \\rightarrow \\mathbb{R} $ called an inequality constraint function, and <br>\n",
    "$ h_j(\\mathbf{x}) \\,\\ : \\,\\ \\mathbb{R}^n \\rightarrow \\mathbb{R} $ called an equality constraint function be.<br>\n",
    "The following nonlinear programming is called a optimization probelm.\n",
    "$$  \\underset{\\mathbf{x}} \\min \\,\\ f(\\mathbf{x}) \\,\\ s.t. \\,\\ g_i(\\mathbf{x}) \\le 0, \\,\\ i=1,2,\\cdots,m , \\,\\ h_j(\\mathbf{x}) = 0, \\,\\ j=1,2,\\cdots,k.$$\n",
    "$g_i(\\mathbf{x}) \\le 0$ is called an inequlity constraint and $h_j(\\mathbf{x}) = 0$ is called an equality constraint.<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.2.2 Lagrangian function\n",
    "In a optimization problem, \n",
    "$$ \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}, \\mathbf{\\mu}) = f(\\mathbf{x}) + \\sum_{i = 1}^{m} \\lambda_i g_i(\\mathbf{x}) + \\sum_{j = 1}^{k} \\mu_j h_j(\\mathbf{x}) \\,\\ where \\,\\ \\mathbf{\\lambda} \\,\\ : \\,\\ m \\times 1, \\,\\ \\mathbf{\\mu} \\,\\ : \\,\\ k \\times 1 $$\n",
    "$ \\lambda_i $ and $ \\mu_i $ are also called dual variables or weights. $ \\mathbf{x} $ is called a primal variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.2.1 Karush-Kuhn-Tucker(KKT) conditions\n",
    "If $ \\mathbf{x}^* \\in \\mathbb{R}^n $ is a local minimum, <br>\n",
    "then $ \\exists \\mathbf{\\lambda}^* \\in \\mathbb{R}^m \\,\\ and \\,\\ \\mathbf{\\mu}^* \\in \\mathbb{R}^k \\,\\ s.t $<br>\n",
    "$ (i) $ Stationarity : <br>\n",
    "$$ \\bigtriangledown f(\\mathbf{x}) + \\sum_i \\lambda_i \\bigtriangledown g_i(\\mathbf{x}) + \\sum_j \\mu_j h_j(\\mathbf{x}) = \\bigtriangledown_\\mathbf{x} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}, \\mathbf{\\mu}) = \\mathbf{0} $$\n",
    "$ (ii) $ Primal feasibility : <br>\n",
    "$$ g_i(\\mathbf{x}^*) \\le 0, \\,\\ {}_{}^{\\forall}i, \\,\\ h_j(\\mathbf{x}^*) = \\mathbf{0}, \\,\\ {}_{}^{\\forall}j $$\n",
    "$ (iii) $ Dual feasibility : <br>\n",
    "$$ \\lambda_i^* \\ge 0, \\,\\ i=1,\\cdots,m $$\n",
    "$ (vi) $ Complementary slackness : <br>\n",
    "$$ \\lambda_i^* g_i(\\mathbf{x}^*) = 0, \\,\\ i = 1, \\cdots , m $$\n",
    "This $ \\mathbf{x}^* $ is also called a local solution of a optimization problem. KKT Conditions are necessary(not sufficient) for global optimality.<br>\n",
    "$$ \\mathbf{x}^*(\\text{Satisfying the KKT Conditions)} \\,\\ \\leftarrow \\text{Global optimal} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "라그랑지 함수만들고, KKT-Conditions에 대해서 연립 풀면 x^*가 ㅣocal optimum <br>\n",
    "추가로 convex optimization 이면 Global 보장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.2.3 Convex function\n",
    "A function $ f(\\mathbf{x}) $ is said to be convex if \n",
    "$$ {}_{}^{\\forall}t \\in [0, 1], \\,\\ f(t\\mathbf{x}_1 + (1-t)\\mathbf{x}_2) \\le tf(\\mathbf{x}_1) + (1-t)f(\\mathbf{x}_2), \\,\\ {}_{}^{\\forall}\\mathbf{x}_1, \\mathbf{x}_2 \\in D $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.2.4 Convex set\n",
    "A set $S$ is said to be convex if \n",
    "$$ \\mathbf{x}, \\mathbf{y} \\in S \\,\\ \\rightarrow \\,\\ t\\mathbf{x} + (1-t)\\mathbf{y} \\in S, \\,\\ t \\in [0,1] $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.2.5 Convex optimization\n",
    "Let $f(\\mathbf{x}) \\,\\ : \\,\\ \\mathbb{R}^n \\rightarrow \\mathbb{R} $ and $ \\mathbf{g}(\\mathbf{x}) \\,\\ : \\,\\ \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $ be convex functions and $ \\mathbf{h}(\\mathbf{x}) \\,\\ : \\,\\ \\mathbb{R}^n \\rightarrow \\mathbb{R}^k $ be an affine function. <br>\n",
    "An optimization problem is called convex if \n",
    "$$ \\underset{\\mathbf{x}} \\min f(\\mathbf{x})  \\quad s.t. \\quad g_i(\\mathbf{x}) \\le 0, \\,\\ i = 1, \\cdots, m, \\quad h_j(\\mathbf{x}) = 0, \\,\\ j = 1, \\cdots, k. $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Affine 함수 : 내적공간 + bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<strong>Reference :</strong><br>\n",
    "Deep Learning - Yosha Benjio<br>\n",
    "https://wikipedia.org<br>\n",
    "https://ko.wikipedia.org/wiki/위키백과:TeX_문법<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}