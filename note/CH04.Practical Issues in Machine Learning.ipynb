{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.04 Practical Issues in Machine Learning\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.1. Bias-Variance tradeoff\n",
    "4.1.1. Bias-Variance decomposition of the MSE<br>\n",
    "For any supervised learning algorithm, we can decompose the MSE on an unseen sample $\\mathbf{x}$ as\n",
    "$$ \\mathbb{E}[(y - \\hat{f}(x; D))^2] = (Bias_D[\\hat{f}(x; D)])^2 + Var_D[\\hat{f}(x; D)] + \\sigma^2 $$\n",
    "$$ \\text{where} \\quad Bias_D[\\hat{f}(x;D)] = \\mathbb{E}_D[\\hat{f}(x;D)] - f(x) \\,\\ \\text{and} \\,\\ Var_D[\\hat{f}(x;D)] = \\mathbb{E}_D[(\\mathbb{E}_D[\\hat{f}(x ; D)] - \\hat{f}(x;D))^2 ] $$\n",
    "\n",
    "- $Bias_D[\\hat{f}(x;D)]$ is due to improper model or assumption.(e.g., When approximating a non-linear funcvtion $f(x)$ using a learning method for linear models, there will be error in the estimates $\\hat{f}(x)$ due to this assumption).\n",
    "- $Var_D[\\hat{f}(x;D)]$ is variation of an algorithm itself.\n",
    "- $\\sigma^2$ is inherent noise(irreducible error).<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF File too long](./res/ch04/note_bias-variance_tradeoff.pdf)\n",
    "\n",
    "Therefore, \n",
    "$$ \\text{MSE} = \\mathbb{E}_x \\left\\{ Bias_D[\\hat{f}(x;D)]^2 + Var_D[\\hat{f}(x;D)] \\right\\} + \\sigma^2 $$\n",
    "\n",
    "<img src=\"./res/ch04/fig_4_1.png\" width=\"400\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.4.1.1\n",
    "</div>\n",
    "\n",
    "- The more complex the model is, the more data points it will capture. $\\rightarrow$ the lower the bias will be.\n",
    "- However, the complexity will make the model vary more to capture the data points $\\rightarrow$ the larger the variance will be."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "4.1.2. Bias-Variance tradeoff<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.2. Generalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.3. Fitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.4. Model selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4.5. Curse of dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}