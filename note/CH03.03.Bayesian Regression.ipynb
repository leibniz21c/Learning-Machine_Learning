{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.3. Bayesian Regression\n",
    "3.3.1. Overview<br>\n",
    "Statistical inference is the process of extracting information about unknown variables or unknown models from available data. For example, a set of observations or input-output data can be chosen. However, bayesian inference is a method in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.<br><br>\n",
    "\n",
    "Let $\\theta$ is unknown parameters. Following is a table of difference between bayesian statistics and classical statistics.\n",
    "\n",
    "\n",
    "|                        | Bayesian statistics                                       |  Classical statistics                        |\n",
    "|------------------------|-----------------------------------------------------------|----------------------------------------------|\n",
    "| Property of variables  | $\\theta$ : unknown, random(with known prior distribution) | $\\theta$ : unknown, deterministic            |\n",
    "| Goal                   | Finding the posterior distribution of $\\theta$ | Finding an estimate of $\\theta$ based on the likelihood |\n",
    "<br>\n",
    "\n",
    "Suppose the prior distribution $p(\\theta)$ of unknown parameters $\\theta$ and the model $p(x | \\theta)$ of observation $X = (X_1, \\cdots, X_n)$ are given. After observing the value $x$ of $X$, we calculate the posterior distribution $p(\\theta | x)$ of $\\theta$ using the bayes rule. \n",
    "\n",
    "> (Bayesian inference) = (Inferring with the posterior distribution)\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.3.2. Maximum A Posteriori(MAP) estimation<br>\n",
    "MAP estimation of parameter is \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_{\\text{MAP}} &= \\arg\\max_\\mathbf{w} p(\\mathbf{w} | y, \\mathbf{x}) \\qquad \\text{(Posterior)} \\\\\n",
    "                        &= \\arg\\max_\\mathbf{w} p(y| \\mathbf{w}, \\mathbf{x}) p(\\mathbf{w}) \\qquad \\text{(Likelihood, prior)} \\,\\ (\\because \\,\\ \\text{Bayes rule})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Adding confidence based on the prior\n",
    "- The parameters $\\mathbf{w}$ are estimated probabilistically\n",
    "\n",
    "Equivalent form is \n",
    "$$ \\mathbf{w}_{\\text{MAP}} = \\arg\\max_\\mathbf{w} \\log p(\\mathbf{w} | y, \\mathbf{x}) $$\n",
    "\n",
    "- The MAP estimation generally leads to a nonlinear estimator.\n",
    "- It requires the knowledge of both the prior and the likelihood."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.3.3. Maximum Likelihood(ML) estimation<br>\n",
    "ML estimation of parameter is\n",
    "$$ \\mathbf{w}_\\text{ML} = \\arg \\max_\\mathbf{w} p(y | \\mathbf{w}, \\mathbf{x}) \\qquad \\text{(Likelihood, observation density)} $$\n",
    "\n",
    "- The training set we observed must be one with the highest probability of occurrence.\n",
    "- ML can be considered as a special case of MAP with an equally likely prior.\n",
    "\n",
    "Equivalent form is \n",
    "$$ \\mathbf{w}_\\text{ML} = \\arg \\max_\\mathbf{w} \\log p(y | \\mathbf{w}, \\mathbf{x}) \\quad (\\because \\,\\ \\text{logarithm function is monotonically increasing.})$$\n",
    "\n",
    "- The ML estimation generally leads to a nonlinear estimator.\n",
    "- It requires the knowledge of the likelihood function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.3.4. Bayesian linear regression with ML estimation<br>\n",
    "Consider a set of training samples $\\{\\mathbf{x}_i, y_i\\}_{i = 1}^{N}$ of a linearly regressive model with i.i.d. gaussian errors:\n",
    "$$ y_i = \\mathbf{w}^T \\mathbf{x}_i + \\epsilon_i, \\quad i = 1, 2, \\cdots, N $$\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$ is i.i.d. error with $p(\\epsilon_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 }} \\exp\\{ - \\frac{\\epsilon_i^2}{2 \\sigma^2 } \\}$\n",
    "\n",
    "- $ \\mathbf{y} = [y_1, \\cdots, y_N]_{N \\times 1}^T $\n",
    "- $ X = [\\mathbf{x_1}, \\cdots, \\mathbf{x}_N]_{N \\times m}^T $\n",
    "- $m$ : order of model\n",
    "- $N$ : number of training data\n",
    "\n",
    "ML estimation of the regressive model is \n",
    "$$ \\mathbf{w}_\\text{ML} = \\arg \\max_\\mathbf{w} p(\\mathbf{y} | X, \\mathbf{w}) $$\n",
    "\n",
    "- Likelihood function of $y_i$ given $\\mathbf{w}$ and $\\mathbf{x}_i$ :\n",
    "\n",
    "$$ p(y_i | \\mathbf{w}, \\mathbf{x}_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp( - \\frac{(y_i - \\mathbf{w}^T \\mathbf{x}_i )^2}{2 \\sigma^2} ), \\quad i = 1, 2, \\cdots, N $$\n",
    "\n",
    "- Likelihood function of $\\mathbf{y}$ given $\\mathbf{w}$ and $X$ : \n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "p(\\mathbf{y} | \\mathbf{w}, X) &= \\prod_{i = 1}^{N} p(y_i | \\mathbf{w}, \\mathbf{x}_i) \\qquad (i.i.d.) \\\\\n",
    "                              &= \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^N} \\prod_{i = 1}^{N} \\exp(- \\frac{(y_i - \\mathbf{w}^T \\mathbf{x}_i)^2}{2 \\sigma^2}) \\\\\n",
    "                              &= \\frac{1}{(\\sqrt{2 \\pi \\sigma^2})^N} \\exp(- \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^{N}(y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In above situations, \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_\\text{ML} &= \\arg\\max_\\mathbf{w} p(\\mathbf{y} | X, \\mathbf{w}) \\\\\n",
    "                     &= \\arg\\max_\\mathbf{w} \\log p(y|X, \\mathbf{w}) \\quad (\\because \\,\\ \\text{Log is monotonically increasing}) \\\\\n",
    "                     &= \\arg\\max_\\mathbf{w} \\frac{1}{2 \\sigma^2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\quad (\\because \\,\\ \\text{Exp is monotonically decreasing}) \\\\\n",
    "                     &= \\arg\\max_\\mathbf{w} || \\mathbf{y} - X \\mathbf{w} ||^2 \\quad (\\because \\,\\ \\text{Suppose that} \\,\\ N \\ge m) \\\\\n",
    "                     &= (X^T X)^{-1} X^T \\mathbf{y} = \\mathbf{w}_\\text{LS} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In above interesting result, we found that ML estimate of $\\mathbf{w}$ is the same as that of the least squares(CH03.02).\n",
    "<br><br>\n",
    "\n",
    "We can also maximize with respect to the error variance $\\sigma^2$\n",
    "$$ p(\\mathbf{y} | \\mathbf{w}, X) = (\\frac{\\beta}{\\sqrt{2 \\pi}})^\\frac{N}{2} \\exp(- \\frac{\\beta}{2} J(\\mathbf{w}))  \\qquad \\text{where} \\,\\ \\beta \\triangleq \\frac{1}{\\sigma^2}, \\,\\ J(\\mathbf{w}) \\triangleq \\frac{1}{2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 $$\n",
    "$$ \\log p(\\mathbf{y} | \\mathbf{w}, X) = \\frac{N}{2} \\log \\beta - \\frac{N}{2} \\log 2 \\pi - \\beta J(\\mathbf{w}) $$\n",
    "$$\n",
    "\\begin{align*}\n",
    " \\frac{\\partial \\log p}{\\partial \\beta} = \\frac{N}{2 \\beta} - J(\\mathbf{w}) = 0 \\quad \\Rightarrow \\quad \\sigma_{\\text{ML}}^2 \n",
    " &= \\arg\\max_{\\sigma^2} \\log p(\\mathbf{y} | X, \\mathbf{w}) \\\\\n",
    " &= \\frac{1}{\\beta} = \\frac{2 J(\\mathbf{w})}{N} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "- ML estimate of $\\sigma^2$ equals to the MSE.\n",
    "- The standard deviation $\\sigma$ is equivalent to the root mean square (RMS) error.\n",
    "$$ \\sigma_{\\text{ML}} = \\sqrt{\\frac{1}{\\beta}} = \\sqrt{\\frac{2 J(\\mathbf{w})}{N}} = \\epsilon_{\\text{RMS}} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.3.5. Bayesian linear regression with MAP estimation<br>\n",
    "Consider a set of training samples $\\{\\mathbf{x}_i, y_i\\}_{i = 1}^{N}$ of a linearly regressive model with i.i.d. gaussian errors:\n",
    "$$ y_i = \\mathbf{w}^T \\mathbf{x}_i + \\epsilon_i, \\quad i = 1, 2, \\cdots, N $$\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$ is i.i.d. error with $p(\\epsilon_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 }} \\exp\\{ - \\frac{\\epsilon_i^2}{2 \\sigma^2 } \\}$\n",
    "\n",
    "- $ \\mathbf{y} = [y_1, \\cdots, y_N]_{N \\times 1}^T $\n",
    "- $ X = [\\mathbf{x_1}, \\cdots, \\mathbf{x}_N]_{N \\times m}^T $\n",
    "- $m$ : order of model\n",
    "- $N$ : number of training data\n",
    "\n",
    "Let a priori distribution of parameters :\n",
    "$$ \\mathbf{w} \\sim N(\\boldsymbol{\\mu}, \\Sigma) \\,\\ \\text{with} \\,\\ p(\\mathbf{w}) = \\frac{1}{\\sqrt{|2 \\pi \\Sigma|}} \\exp\\{- \\frac{1}{2} (\\mathbf{w} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{w} - \\boldsymbol{\\mu}) \\}$$\n",
    "\n",
    "A posteriori probability density :\n",
    "$$ \n",
    "\\begin{align*}\n",
    "p(\\mathbf{w} | X, \\mathbf{y}) &\\propto p(\\mathbf{y} | X, \\mathbf{w}) p(\\mathbf{w}) \\\\\n",
    "                              &= K \\cdot \\exp\\{ -\\frac{1}{2 \\sigma^2} || \\mathbf{y} - X \\mathbf{w} ||^2 - \\frac{1}{2} (\\mathbf{w} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{w} - \\boldsymbol{\\mu}) \\} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In above situation, \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_{\\text{MAP}} &= \\arg\\max_{\\mathbf{w}} p(\\mathbf{w} | X, \\mathbf{y}) \\\\\n",
    "                        &= \\arg\\min_{\\mathbf{w}} \\frac{1}{2}[ - \\frac{1}{\\sigma^2} || \\mathbf{y} - X\\mathbf{w} ||^2 - (\\mathbf{w} - \\boldsymbol{\\mu})^T \\Sigma^{-1}(\\mathbf{w} - \\boldsymbol{\\mu}) ] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{w}} [\\frac{1}{\\sigma^2} || \\mathbf{y} - X\\mathbf{w} ||^2 + (\\mathbf{w} - \\boldsymbol{\\mu})^T \\Sigma^{-1}(\\mathbf{w} - \\boldsymbol{\\mu}) ] = \\Sigma^{-1} (\\mathbf{w} - \\boldsymbol{\\mu}) - \\frac{1}{\\sigma^2} X^T (\\mathbf{y}- X \\mathbf{w}) = \\mathbf{0} $$\n",
    "\n",
    "$$ \\therefore \\quad \\mathbf{w}_{\\text{MAP}} = (\\Sigma^{-1} + \\frac{1}{\\sigma^2} X^T X)^{-1}(\\Sigma^{-1} \\boldsymbol{\\mu} + \\frac{1}{\\sigma^2} X^T \\mathbf{y}) $$\n",
    "\n",
    "If the components of the weight vector $\\mathbf{w}$ is i.i.d. with zero mean and equal variance, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{w}) &= \\prod_{k = 1}^{M} p(w_k) = \\prod_{k = 1}^{M} \\frac{\\alpha}{\\sqrt{2 \\pi}} \\exp(- \\frac{\\alpha w_k^2}{2}) \\qquad \\text{where} \\,\\ \\alpha \\,\\ \\text{is inverse of variance of} \\,\\ w_k \\\\\n",
    "              &= (\\frac{\\alpha}{\\sqrt{2 \\pi}})^M \\exp(- \\frac{\\alpha}{2} \\sum_{k = 1}^{M} w_k^2) = (\\frac{\\alpha}{\\sqrt{2 \\pi}})^M \\exp(- \\frac{\\alpha}{2} || \\mathbf{w} ||^2) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "We can get \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_{\\text{MAP}} &= \\arg\\max_\\mathbf{w} p(\\mathbf{w} | X, \\mathbf{y}) \\\\\n",
    "                        &= \\arg\\min_\\mathbf{w} (\\beta || \\mathbf{y} - X \\mathbf{w} ||^2 + \\alpha || \\mathbf{w} ||^2) \\quad \\text{where} \\,\\ \\beta = \\sigma^2 \\\\\n",
    "                        &= (X^T X + \\frac{\\alpha}{\\beta} I )^{-1} X^T \\mathbf{y} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can found that above result is regularized least squares(CH03.02) with $\\lambda = \\frac{\\alpha}{\\beta}$\n",
    "<br><br>\n",
    "\n",
    "Let the prior distribution of $\\mathbf{w}$ be\n",
    "$$ p(\\mathbf{w}) = N(\\boldsymbol{\\mu}_0, \\Sigma_0) $$\n",
    "Posterior pdf of $\\mathbf{w}$ \\[for $\\mathbf{y} = X\\mathbf{w} + \\boldsymbol{\\epsilon}$ where $p(\\epsilon) = N(\\mathbf{0}, \\beta I)$\\]\n",
    "\n",
    "## -Block-\n",
    "[PDF File](./res/ch03/note_map.pdf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.3.6. Bayesian linear regression with MAP estimation<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}