{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "certified-exemption",
   "metadata": {},
   "source": [
    "# Chapter.05 Classification\n",
    "---\n",
    "### 5.3. Probabilistic Approaches for Classification\n",
    "5.3.1. Statistics vs Bayesian Classification\n",
    "\n",
    "- Statistical classification\n",
    "    - Based on the Neyman-Pearson criterion\n",
    "    - Typically used in sonar and rader systems (with unknown priors)\n",
    "    - ML estimator\n",
    "- Bayesian Classification\n",
    "    - Based on minimization of the Bayes risk(by cost function)\n",
    "    - Typically used in communications and pattern recognition systems\n",
    "    - MAP estimator (suppose we know prior e.g., gaussian)\n",
    "\n",
    "<br>\n",
    "Both are based on the Likelihood Ratio Test (LRT), just comparing the ratio of likelihoods, but to different thresholds\n",
    "\n",
    "$$\n",
    "L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\xi\n",
    "$$\n",
    "\n",
    "$ L(\\mathbf{x}) $ is a likelihood ratio and $ \\xi $ is a decision threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-presentation",
   "metadata": {},
   "source": [
    "5.3.2. Probabilities in classification<br>\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_1.png\" width=\"800\" height=\"600\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.1\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\text{mistake}) &= p(\\mathbf{x} \\in \\mathcal{R}_1, \\mathcal{C}_2) + p(\\mathbf{x} \\in \\mathcal{R}_2, \\mathcal{C}_1) \\\\\n",
    "                  &= \\int_{\\mathcal{R}_1} p(\\mathbf{x}, C_1) d\\mathbf{x} + \\int_{\\mathcal{R}_2} p(\\mathbf{x}, C_2) d\\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(correct) &= \\int_{\\mathcal{R}_1} p(\\mathbf{x}, C_1) d\\mathbf{x} + \\int_{\\mathcal{R}_2} p(\\mathbf{x}, C_2) d\\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-cathedral",
   "metadata": {},
   "source": [
    "5.3.3. A simple binary classification\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_2.png\" width=\"500\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.2\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\text{Type-1 error : } \\int_{\\mathcal{R}_2} p(x | C_1) dx = \\int_{\\frac{1}{2}}^{\\infty} = Q(0.5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Type-2 error : } \\int_{\\mathcal{R}_1} p(x | C_2) dx = \\int_{-\\infty}^{\\frac{1}{2}} = Q(0.5)\n",
    "$$\n",
    "\n",
    "What if the threshold changes?\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_3.png\" width=\"550\" height=\"430\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.3\n",
    "</div>\n",
    "\n",
    "It isn't possible to reduce both error probabilities at the same time. So there is a criterion suggested by Neyman-Pearson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-fortune",
   "metadata": {},
   "source": [
    "5.3.4. Statistical classification : Neyman-Pearson criterion<br>\n",
    "To degign the optimal(binary) classifier, one possible choice is to minimize the __Type-Ⅱ error__(false negative), or equivalently, to maximize the __Power of test__ by constraining __Type-Ⅰ error__(false positive) below a threshold $ \\alpha $:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathcal{R}_2} \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_2) d\\mathbf{x} \\quad s.t. \\quad \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_1) d\\mathbf{x} \\le \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-atlas",
   "metadata": {},
   "source": [
    "<strong>Theorem.5.3.4.1. Neyman-Pearson theorem</strong><br>\n",
    "The solution to \n",
    "\n",
    "$$\n",
    "\\max_{\\mathcal{R}_2} \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_2) d\\mathbf{x} \\quad s.t. \\quad \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_1) d\\mathbf{x} \\le \\alpha\n",
    "$$\n",
    "\n",
    "is given by \n",
    "\n",
    "$$\n",
    "R_2^* = \\left\\{ \\mathbf{x} : L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} > \\gamma \\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where the threshold} \\,\\ \\gamma \\,\\ \\text{is found such that} \\,\\ \\int_{ \\mathcal{R}_2 } p(\\mathbf{x} | C_1) d\\mathbf{x} = \\alpha \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-battlefield",
   "metadata": {},
   "source": [
    "Neyman-Pearson(Binary) Classification Rule : \n",
    "- Decide a decision threshold $ \\gamma $ from \n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_1) d\\mathbf{x} = \\alpha\n",
    "$$\n",
    "\n",
    "- Perform classification according to the likelihood ratio test : \n",
    "\n",
    "$$\n",
    "L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\gamma\n",
    "$$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "It can be proved by constraint optimization problem. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-judge",
   "metadata": {},
   "source": [
    "Let's solve the problem in 5.3.3.\n",
    "\n",
    "$$\n",
    "Pr\\{\\text{False positive(Type 1 error)}\\} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{p(x|C_2)}{p(x|C_1)} &= \\frac{\\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2}(x - 1)^2)}{\\frac{1}{\\sqrt{2 \\pi}} \\exp(- \\frac{1}{2} x^2)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\gamma \\\\\n",
    "                          &= \\exp\\left(- \\frac{1}{2} (x^2 - 2x + 1 - x^2) \\right) = \\exp \\left( x - \\frac{1}{2} \\right) \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\gamma \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr\\{\\text{False positive(Type 1 error)}\\} &= Pr\\left\\{ \\exp \\left( x - \\frac{1}{2} \\right) > \\gamma | C_1 \\right\\} \\\\\n",
    "                                          &= Pr\\left\\{ x - \\frac{1}{2} > \\ln \\gamma | C_1 \\right\\} \\\\\n",
    "                                          &= \\int_{\\ln \\gamma + \\frac{1}{2}}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2} x^2) dx = 0.5 \\quad \\Rightarrow \\quad \\ln \\gamma + \\frac{1}{2} = 0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr\\{\\text{True positive}\\} &= Pr\\left\\{ x > 0 \\right\\} \\\\\n",
    "                           &= \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp[- \\frac{1}{2}(x - 1)^2] dx = 0.84 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So are there some limitations? Although an NP criterion can be formulated for multiple classes, it seems to seldom be used in practice. More commonly the minimum $P_e$ criterion or its generalization, the Bayes risk criterion, is employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-origin",
   "metadata": {},
   "source": [
    "5.3.4. Receiver Operating Characteristics (ROC)<br> \n",
    "ROC curve : $Pr\\{\\text{True positive}\\} \\,\\ \\text{vs} \\,\\ Pr\\{\\text{False positive}\\} $\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_4.png\" width=\"550\" height=\"430\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.4\n",
    "</div>\n",
    "\n",
    "This is alternative way of summarizing the performance of a classifier and very useful to compare different classifiers and to decide which one performs best.\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_5.png\" width=\"650\" height=\"530\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-crossing",
   "metadata": {},
   "source": [
    "5.3.5. Bayesian classification : Minimum Bayes Risk Classifier for two classes<br> \n",
    "Bayes risk $ \\mathcal{R} $ is given like following\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R} &= c_{11}Pr(\\text{True Positive}) + c_{22}Pr(\\text{True Negative}) + c_{21}Pr(\\text{False Negative}) + c_{12}Pr(\\text{False Positive}) \\\\\n",
    "            &= c_{11} \\pi_1 \\int_{R_1} p(\\mathbf{x} | C_1) d\\mathbf{x} + c_{22} \\pi_2 \\int_{R_2} p(\\mathbf{x} | C_2) d\\mathbf{x} + c_{21} \\pi_1 \\int_{R_2} p(\\mathbf{x} | C_1) d\\mathbf{x} + c_{12} \\pi_2 \\int_{R_1} p(\\mathbf{x} | C_2) d\\mathbf{x} \\\\\n",
    "            &\\quad \\text{where} \\,\\ \\pi_i = P(C_i), \\,\\ i = 1, 2, \\,\\ R_i \\,\\ : \\,\\ \\text{decision region in which} \\,\\ \\mathbf{x} \\in C_i , \\,\\ c_{ij} \\,\\ : \\,\\ \\text{cost if we choose} \\,\\ C_i \\,\\ \\text{but} \\,\\ C_j \\,\\ \\text{is true} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-voltage",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Since} \\,\\ \\int_{R_1} p(\\mathbf{x} | C_i) d\\mathbf{x} + \\int_{R_2} p(\\mathbf{x} | C_i) d\\mathbf{x} = 1, \\,\\ i = 1, 2, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{We have} \\,\\ \\mathcal{R} = &c_{11} \\pi_1 \\int_{R_1} p(\\mathbf{x} | C_1) d\\mathbf{x} + c_{22} \\pi_2 \\left( 1 - \\int_{R_1} p(\\mathbf{x} | C_2) d\\mathbf{x} \\right) \\\\\n",
    "                                 &+ c_{21} \\pi_1 \\left( 1 - \\int_{R_1} p(\\mathbf{x} | C_1) d\\mathbf{x} \\right) + c_{12} \\pi_2 \\int_{R_1} p(\\mathbf{x} | C_2) d\\mathbf{x} \\\\                             \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R} = &c_{21} \\pi_1 + c_{22} \\pi_2 \\\\\n",
    "              &+ \\int_{R_1} \\left[ \\pi_2 (c_{12} - c_{22}) p(\\mathbf{x} | C_2) - \\pi_1 (c_{21} - c_{11}) p(\\mathbf{x} | C_1) \\right] d\\mathbf{x} \\quad \\text{where} \\,\\ c_{12} - c_{22} > 0 \\,\\ \\text{and} \\,\\ c_{21} - c_{11} > 0 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-ferry",
   "metadata": {},
   "source": [
    "$ c_{11} $ and $ c_{22} $ are negative to maximize TP and TN.<br>\n",
    "$ c_{12} $ and $ c_{21} $ are positive to minimize FP and FN.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-sitting",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "R_1 &= \\arg\\min \\mathcal{R} \\\\\n",
    "    &= \\arg\\min \\int_{R_1} \\left[ \\pi_2(c_{12} - c_{22})p(\\mathbf{x}|C_2) - \\pi_1 (c_{21} - c_{11}p(\\mathbf{x}|C_1))\\right] d\\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Choose} &\\,\\ R_1 \\,\\ s.t. \\\\\n",
    "              & \\pi_2 (c_{12} - c_{22}) p(\\mathbf{x} | C_2) < \\pi_1(c_{21} - c_{11})p(\\mathbf{x} | C_1) \\\\\n",
    "              & \\therefore \\,\\ \\pi_2(c_{12} - c_{22}) p(\\mathbf{x} | C_2) \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\pi_1 (c_{21} - c_{11}) p(\\mathbf{x} | C_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The likelihood ratio is compared to a threshold:\n",
    "$$\n",
    "\\Lambda(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\frac{\\pi_1(c_{21} - c_{11})}{\\pi_2(c_{12} - c_{22})} = \\xi \n",
    "$$\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_6.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.6\n",
    "</div>\n",
    "\n",
    "Equivalent form: Log-Likelihood ratio test\n",
    "$$\n",
    "\\log \\Lambda(\\mathbf{x}) \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\log \\xi\n",
    "$$\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_7.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.7\n",
    "</div>\n",
    "\n",
    "In the minimum Bayes risk classifier, the threshold is determined by the prior densities.<br>\n",
    "In the Neyman-Pearson classifier, it is determined by the type-1 error(i.e., false alarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-stomach",
   "metadata": {},
   "source": [
    "5.3.6. Minimum Error Probability Classifier for two classes<br>\n",
    "Probability of error $ P_e $(or misclassification probability)\n",
    "$$\n",
    "\\begin{align*}\n",
    "P_e &= Pr\\{\\text{decide} \\,\\ C_1, \\,\\ C_2 \\,\\ \\text{true}\\} + Pr\\{\\text{decide} \\,\\ C_2, \\,\\ C_1 \\,\\ \\text{true}\\} \\\\\n",
    "    &= \\int_{R_1} p(\\mathbf{x} | C_2) p(C_2) d \\mathbf{x} + \\int_{R_2} p(\\mathbf{x} | C_1) p(C_1) d \\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In this context, minimum $ P_e $ classifier:\n",
    "$$\n",
    "L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\frac{p(C_1)}{p(C_2)} = \\xi \\qquad : \\,\\ \\text{Maximum A Posteriori classifier}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-animation",
   "metadata": {},
   "source": [
    "- A special case of the more general minimum Bayes risk classifier\n",
    "- If $ c_{11} = c_{22} = 0 $ and $ c_{12} = c_{21} = 1 $, then $ R = P_e $\n",
    "\n",
    "> The MAP classifier minimizes the error probability.\n",
    "\n",
    "If the class-prior probabilities are equal, $p(C_1) = \\cdots = p(C_k)$, \n",
    "$$\n",
    "L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} 1 \\qquad : \\,\\ \\text{Maximum Likelihood classifier}\n",
    "$$\n",
    "\n",
    "> The ML classifier minimizes the error probability for equally likely classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-associate",
   "metadata": {},
   "source": [
    "5.3.7. Bayesian classification : Minimum Bayes Risk Classifier for multiple classes<br>\n",
    "Bayes risk $ \\mathcal{R} $(or expected cost) for multiple classes $ \\{C_1, \\cdots, C_k\\} $ \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R} &= \\sum_{i = 1}^{K} \\sum_{j = 1}^{K} c_{ij} \\int_{R_i} p(\\mathbf{x} | C_j) p(C_j) d\\mathbf{x} \\\\\n",
    "            &= \\sum_{i = 1}^{K} \\int_{R_i} \\sum_{j = 1}^{K} c_{ij} p(\\mathbf{x} | C_j) p(C_j) d\\mathbf{x} \\\\\n",
    "            &= \\sum_{i = 1}^{K} \\int_{R_i} J_i(\\mathbf{x}) p(C_j) d\\mathbf{x} \\quad \\text{where} \\,\\ J_i(\\mathbf{x}) = \\sum_{j = 1}^{K} c_{ij} p(C_j | \\mathbf{x}) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To minimize $ \\mathcal{R} $, we should choose the class that minimizes \n",
    "$$\n",
    "J_i(\\mathbf{x}) = \\sum_{j = 1}^{K} c_{ij} p(C_j | \\mathbf{x})\n",
    "$$\n",
    "\n",
    "Therefore, Minimum Bayes risk classifier for multiple class $\\{C_1, \\cdots, C_K\\}$ \n",
    "$$\n",
    "\\mathbf{x} \\in C_k \\,\\ \\text{if} \\,\\ k = \\underset{i \\in \\{i, \\cdots K\\}}{\\arg\\min} \\left\\{ J_i(\\mathbf{x}) = \\sum_{j = 1}^{K} c_{ij} p(C_j | \\mathbf{x}) \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-cloud",
   "metadata": {},
   "source": [
    "5.3.8. Minimum Error Probability Classifier for multiple classes<br>\n",
    "Probability of error $ P_e $ for multiple classes $ \\{C_1, \\cdots, C_K\\} $\n",
    "$$\n",
    "P_e = \\sum_{i = 1}^{K} \\sum_{j = 1, j \\neq i}^{K} \\int_{R_i} p(\\mathbf{x} | C_j) p(C_j) d\\mathbf{x}\n",
    "$$\n",
    "\n",
    "- $ P_e $ is a special case of $ \\mathcal{R}\\text{(i.e., } P_e = \\mathcal{R}\\text{)} $ for a paricular assignment:\n",
    "\n",
    "$$\n",
    "c_{ij} = \n",
    "\\begin{cases}\n",
    "0 & i = j \\\\\n",
    "1 & i \\neq j \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To determin the classifier that minimizes $ P_e $, we use $ J_i(\\mathbf{x}) $\n",
    "$$\n",
    "\\begin{align*}\n",
    "J_i(\\mathbf{x}) &= \\sum_{j = 1, j \\neq i}^{K} p(C_j | \\mathbf{x}) \\\\\n",
    "                &= \\sum_{j = 1}^{K} p(C_j | \\mathbf{x}) - p(C_i | \\mathbf{x}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, Minimum $ P_e $ classifier for multiple classes $\\{C_1, \\cdots C_k\\}$ \n",
    "$$\n",
    "\\begin{align*}\n",
    "C_k &= \\underset{C \\in \\{C_1, \\cdots, C_K\\}}{\\arg\\max} p(C | \\mathbf{x}) \\\\\n",
    "    &= \\underset{C \\in \\{C_1, \\cdots, C_K\\}}{\\arg\\max} p(\\mathbf{x} | C) p(C) \\qquad : \\,\\ \\text{Multiple-class Maximum A Posteriori classifier} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> The MAP classifier minimizes the chance of misclassification.\n",
    "\n",
    "If the class-prior probabilities are equal, $ p(C_1) = \\cdots = p(C_K) $, \n",
    "$$\n",
    "C_k = \\underset{C \\in \\{C_1, \\cdots, C_K\\}}{\\arg\\max} p(\\mathbf{x} | C) \\qquad : \\,\\ \\text{Multiple-class Maximum Likelihood classifier} \\\\\n",
    "$$\n",
    "\n",
    "> The ML classifier minimizes the chance of misclassification for equally likely classes.\n",
    "\n",
    "In summarize, __MAP classifier__\n",
    "- Requires the knowledge of __class-prior__  distributions and __class-conditional__  distributions.\n",
    "- Optimal in the sense of minimizing the chance of misclassification \n",
    "<br>\n",
    "\n",
    "In summarize, __ML classifier__\n",
    "- Requires the knowledge of only the __class-conditional__  densities\n",
    "- Optimal in terms of error probability only for equally likely classes\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, following is classification of gaussian features with different means.\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_8.png\" width=\"500\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.8\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\underset{k = 1, 2, 3}{\\max} p(\\mathbf{x} | C_k) = \\underset{k = 1, 2, 3}{\\min} (x - A_k)^2 \\quad \\text{where} \\,\\ A_1 = -A, \\,\\ A_2 = 0, \\,\\ A_1 = A\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "C_1 & \\text{if} \\,\\ x < -\\frac{A}{2} \\\\\n",
    "C_2 & \\text{if} \\,\\ -\\frac{A}{2} < x < \\frac{A}{2} \\\\\n",
    "C_3 & \\text{if} \\,\\ x > \\frac{A}{2} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- ML is optimal in the sense of the minimum error probability\n",
    "- In the ML, thresholds are just intersections between any two PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-blogger",
   "metadata": {},
   "source": [
    "5.3.9. Naive Bayes classifier<br>\n",
    "Naive Bayes classification assuming independent features\n",
    "- Posterior Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-bosnia",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "p(C_k | \\mathbf{x}) &\\propto p(\\mathbf{x} | C_k) p(C_k) \\\\\n",
    "                    &= p(x_1, \\cdots, x_m, C_k) \\\\\n",
    "                    &= p(x_1 | x_2, \\cdots, x_m, C_k)p(x_2 | x_3, \\cdots, x_m, C_k) \\\\\n",
    "                    & \\,\\ \\cdots p(x_{n-1} | x_m, C_k) p(x_m | C_k) p(C_k) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-youth",
   "metadata": {},
   "source": [
    "$$\n",
    "p(C_k | \\mathbf{x}) = \\frac{1}{Z} p(C_k) \\prod_{i} p(x_i | C_k) \\quad (\\because \\,\\ p(x_i | x_{i+1}, \\cdots, x_m, C_k) = p(x_i | C_k) \\,\\ \\text{Conditional independence assumption} )\n",
    "$$\n",
    "\n",
    "- Naive MAP classifier\n",
    "$$\n",
    "C = \\underset{k \\in \\{1, \\cdots, K\\}}{\\arg\\max} p(C_k) \\prod_{i} p(x_i | C_k)\n",
    "$$\n",
    "\n",
    "- Naive ML classifier\n",
    "$$\n",
    "C = \\underset{k \\in \\{1, \\cdots, K\\}}{\\arg\\max} \\prod_{i} p(x_i | C_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-citizen",
   "metadata": {},
   "source": [
    "For example, Naive Bayes classifier for binary classification(Spam e-mail classifier)\n",
    "\n",
    "1. Posteriori Probability\n",
    "$$\n",
    "p(C_k | \\mathbf{x}) = \\frac{p(C_k)p(\\mathbf{x} | C_k)}{p(\\mathbf{x})} = \\frac{p(C_k)}{p(\\mathbf{x})} \\prod_{i} p(x_i | C_k), \\,\\ k = 1, 2 \n",
    "$$\n",
    "\n",
    "2. MAP classification rule\n",
    "$$\n",
    "\\frac{p(C_1|\\mathbf{x})}{p(C_2 | \\mathbf{x})} = \\frac{p(C_1)}{p(C_2)} \\prod_{i} \\frac{p(x_i | C_1)}{p(x_i | C_2)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} 1\n",
    "$$\n",
    "    - A special kind of likelihood ratio test(independent)\n",
    "\n",
    "3. ML classification rule\n",
    "$$\n",
    "\\frac{p(\\mathbf{x}|C_1)}{p(\\mathbf{x} | C_2)} = \\prod_i \\frac{p(x_i | C_1)}{p(x_i | C_2)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} 1\n",
    "$$\n",
    "    - A special kind of likelihood ratio test(independent and same prior density)\n",
    "\n",
    "Let $ C_1 $ be spam, $ C_2 $ be legitimate with known class-prior $ p(C_k), \\,\\ k = 1, 2 $<br>\n",
    ", $ \\{w_1, \\cdots, w_n\\} $ be collection of words in an e-mail, based on which we decide whether the e-mail is spam or not.\n",
    "\n",
    "$$\n",
    "x_i = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } w_i \\,\\ \\text{appears in the e-mail} \\\\\n",
    "0, & \\text{if it does not} \\\\\n",
    "\\end{cases} \\Rightarrow \\,\\ \\text{Bernoulli R.V.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\,\\ P(C_k | x_1, \\cdots, x_n) = \\frac{\\prod_{i = 1}^{n} p(x_i | C_k) p(C_k)}{\\sum_{j = 1}^{2} \\sum_{i = 1}^{n} p(x_i | C_j) p(C_j)}, \\,\\ k = 1, 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(C_1 | x_1, \\cdots, x_n) \\overset{\\text{spam}}{\\underset{\\text{legitimate}}{\\gtrless}} P(C_2 | x_1, \\cdots, x_n)\n",
    "$$\n",
    "\n",
    "Therefore, we can do likelihood ratio test with independent class-conditional densities\n",
    "$$\n",
    "\\frac{\\prod_{i = 1}^{m} p(x_i | C_1)}{\\prod_{i = 1}^{m} p(x_i | C_2)}  \\overset{\\text{spam}}{\\underset{\\text{legitimate}}{\\gtrless}} \\frac{p(C_2)}{p(C_1)} = \\xi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-wheel",
   "metadata": {},
   "source": [
    "5.3.10. Assumptions of Naive Bayes classifier<br>\n",
    "1. Assumptions on class-prior distribution<br>\n",
    "- Equiprobable classes\n",
    "$$\n",
    "p(C_k) = \\frac{1}{\\text{Number of classes}}\n",
    "$$\n",
    "\n",
    "- Estimation of class probabilities from training samples\n",
    "$$\n",
    "p(C_k) = \\frac{\\text{Number of samples in} \\,\\ C_k}{\\text{Total number of samples}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-wings",
   "metadata": {},
   "source": [
    "2. Assumptions on class-conditional distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-cathedral",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes\n",
    "$$\n",
    "p(x | C_k) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}} \\exp \\left(- \\frac{(x - \\mu_k)^2}{2 \\sigma_k^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-delicious",
   "metadata": {},
   "source": [
    "- Multinomial Naive Bayes(e.g., document classifications)\n",
    "$$\n",
    "p(\\mathbf{x} | C_k) = \\frac{\\left(\\sum_i x_i\\right)!}{\\prod_i x_i!} \\prod_i p_{ki}^{x_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad \\text{where} \\,\\ p_{ki} : \\text{probability that event } i \\,\\ \\text{occurs given } C_k , \\,\\ x_i : \\text{the number of times event } i \\,\\ \\text{was observed} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-south",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{In Log-Likelihood form, } \\,\\ \\log p(C_k | \\mathbf{x}) &\\propto \\log \\left(p(C_k) \\prod_i p_{ki}^{x_i} \\right) \\\\\n",
    "                         &= \\log p(C_k) + \\sum_i x_i \\log p_{ki} = b_k + \\mathbf{w}_k^T \\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-preserve",
   "metadata": {},
   "source": [
    "- Binomial(or Bernoulli) Naive Bayes(e.g., document classifications with binay features)\n",
    "\n",
    "$$\n",
    "    p(\\mathbf{x} | C_k) = \\prod_i p_{ki}^{x_i} (1 - p_{ki})^{(1 - x_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad \\text{where} \\,\\ p_{ki} : \\text{probability of the term } i \\,\\ \\text{given } C_k ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_i : \\text{boolean expressing the occurrence or absence of the } i\\text{th term from the vocabulary}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-drove",
   "metadata": {},
   "source": [
    "5.3.11. Bayes Gaussian Classifier<br>\n",
    "- Assumptions \n",
    "\n",
    "$$\n",
    "\\text{Class} \\,\\ C_1 \\,\\ : \\,\\ \\mathbb{E}[\\mathbf{x}] = \\mathbf{\\mu}_1, \\,\\ \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu}_1)(\\mathbf{x} - \\mathbf{\\mu}_1)^T] = \\Sigma \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Class} \\,\\ C_2 \\,\\ : \\,\\ \\mathbb{E}[\\mathbf{x}] = \\mathbf{\\mu}_2, \\,\\ \\mathbb{E}[(\\mathbf{x} - \\mathbf{\\mu}_2)(\\mathbf{x} - \\mathbf{\\mu}_2)^T] = \\Sigma \n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} | C_i) = \\frac{1}{\\sqrt{|2 \\pi \\Sigma|}} \\exp \\left( - \\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_i)^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}_i)\\right), \\,\\ i = 1,2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-commitment",
   "metadata": {},
   "source": [
    "- Also suppose that\n",
    "\n",
    "$$\n",
    "p(C_1) = p(C_2) = \\frac{1}{2}, \\,\\ c_{21} = c_{12}, \\,\\ c_{11} = c_{22} = 0\n",
    "$$\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_9.png\" width=\"500\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.9\n",
    "</div>\n",
    "\n",
    "Bayes Gaussian classifier is just a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-match",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\log \\xi &= 0 \\\\\n",
    "\\log L(\\mathbf{x}) &= \\frac{p_\\mathbf{x} (\\mathbf{x} | C_1)}{p_\\mathbf{x} (\\mathbf{x} | C_2)} \\\\\n",
    "                   &= - \\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_1)^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}_1) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu}_2)^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}_2) \\\\\n",
    "                   &= (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)^T \\Sigma^{-1} \\mathbf{x} + \\frac{1}{2} (\\mathbf{\\mu}_2^T \\Sigma^{-1} \\mathbf{\\mu}_2 - \\mathbf{\\mu}_1^T \\Sigma^{-1} \\mathbf{\\mu}_1) \\\\\n",
    "                   &= \\mathbf{w}^T \\mathbf{x} + b \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-selling",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log L(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b \\overset{C_1}{\\underset{C_2}{\\gtrless}} 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-paint",
   "metadata": {},
   "source": [
    "5.3.12. Generative and discriminative approach<br>\n",
    "Generative approach use Bayes's theorem like likelihood and posterior probability, so that we can make __decision boundary__ because we have to concentrate on distribution of classes. However, Discriminative approach just focus on classification of classes so that we have to concentrate on differences of classes.\n",
    "\n",
    "- Generative approach \n",
    "\n",
    "Generative model $ p(t, x) = p(x | t) p(t) $<br>\n",
    "$ p(t|x) = \\frac{p(x | t)p(t)}{p(x)} \\quad \\text{where} \\,\\ p(x) = \\int p(x | t) p(t) dt  \\quad (\\because \\,\\ Bayes's theorem)$ <br>\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_10.png\" width=\"400\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.10\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "Discriminative model $ p(t | x) directly $ <br>\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_11.png\" width=\"400\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.11\n",
    "</div>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-regulation",
   "metadata": {},
   "source": [
    "5.3.13. Probabilistic Generative Models for two classes classification<br>\n",
    "- Posterior probability in binary classification (Bayes rule)\n",
    "\n",
    "$$\n",
    "p(C_1 | \\mathbf{x}) = \\frac{p(\\mathbf{x} | C_1) p(C_1)}{\\sum_{i = 1}^{2} p(\\mathbf{x} | C_i) p(C_i)} = \\frac{1}{1 + \\exp(-a)} = \\sigma(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where} \\,\\ a = \\ln \\frac{p(\\mathbf{x}|C_1)p(C_1)}{\\mathbf{x}|C_2)p(C_2)} = \\ln \\frac{p(\\mathbf{x}, C_1)}{p(\\mathbf{x}, C_2)} \\quad (a \\,\\ \\text{takes simply a linear form of } \\mathbf{x}, \\,\\ a = \\mathbf{w}^T \\mathbf{x} + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-proposal",
   "metadata": {},
   "source": [
    "<strong>Proof.</strong><br>\n",
    "Let $ \\alpha = p(\\mathbf{x} | C_1) p(C_1) $ and $ \\beta = p(\\mathbf{x} | C_2) p(C_2) $, <br>\n",
    "\n",
    "$$\n",
    "\\frac{\\alpha}{\\alpha + \\beta} = \\frac{1}{\\frac{\\alpha + \\beta}{\\alpha}} = \\frac{1}{1 + \\frac{\\beta}{\\alpha}} = \\frac{1}{1 + \\exp(- \\ln \\frac{\\alpha}{\\beta})} \\qquad \\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-february",
   "metadata": {},
   "source": [
    "5.3.8. Bayes Gaussian Classifier<br>\n",
    "5.3.9. Generative and discriminative approach<br>\n",
    "5.3.10. Probabilistic generative models in two classes<br>\n",
    "5.3.11. Probabilistic generative models in multiple classes<br>\n",
    "5.3.12. Probabilistic discriminative models<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
