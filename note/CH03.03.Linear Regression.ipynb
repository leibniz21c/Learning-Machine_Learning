{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.6. Recursive least squares<br>\n",
    "The idea of recursive least squares is that \"What if additional training examples are available?\". The RLS aims to solve the least squares problem recursively. That is, it finds the new regression model. \n",
    "\n",
    "$$ \n",
    "\\mathbf{w}(n) = \\arg\\min_{\\mathbf{w}} \n",
    "\\left|\\left|\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{y}(1) \\\\\n",
    "        \\mathbf{y}(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{y}(n) \\\\\n",
    "    \\end{bmatrix}\n",
    "    -\n",
    "    \\begin{bmatrix}\n",
    "        X(1) \\\\\n",
    "        X(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        X(n) \\\\\n",
    "    \\end{bmatrix} \\mathbf{w}\n",
    "\\right|\\right| ^2\n",
    "$$\n",
    "\n",
    "in terms of new training sample $ (X(n), \\,\\ \\mathbf{y}(n)) $ and previous model<br>\n",
    "\n",
    "$$ \n",
    "\\mathbf{w}(n-1) = \\arg\\min_{\\mathbf{w}} \n",
    "\\left| \\left|\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{y}(1) \\\\\n",
    "        \\mathbf{y}(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{y}(n-1) \\\\\n",
    "    \\end{bmatrix}\n",
    "    -\n",
    "    \\begin{bmatrix}\n",
    "        X(1) \\\\\n",
    "        X(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        X(n-1) \\\\\n",
    "    \\end{bmatrix} \\mathbf{w}\n",
    "\\right| \\right| ^2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose $ P(i) = \\begin{bmatrix} X(1) \\\\ \\vdots \\\\ X(i) \\end{bmatrix}^T \\begin{bmatrix} X(1) \\\\ \\vdots \\\\ X(i) \\end{bmatrix}. $<br>\n",
    "$ ^\\forall i \\in \\mathbb{N}, \\quad \\mathbf{w}(i) = \\mathbf{w}(i-1) - P(i) X^T(i) ( \\mathbf{y}(i) - X(i) \\mathbf{w}(i-1) ) $<br>\n",
    "$ \\qquad \\text{where} \\qquad P(i) = P(i-1) - P(i-1) X^T(i) (I + X(i)P(i-1)X^T(i))^{-1} X(i)P(i-1) $<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF file (Too long)](./res/ch03/note_recursive_linear_regression.pdf)  $\\blacksquare$<br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.7. Regularized least squares<br>\n",
    "\n",
    "Consider the following error function :<br>\n",
    "$$ J(\\mathbf{w}) = E(\\mathbf{w}) + \\lambda R(\\mathbf{w}) $$\n",
    "- $E$ is called error term, $R$ is called regularization term.\n",
    "- $\\lambda$ is called the regularization coefficient.\n",
    "\n",
    "In context, regularized least squares is\n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2}|| \\mathbf{y} - X \\mathbf{w} ||^2 + \\frac{\\lambda}{2} R(\\mathbf{w}) \\} $$\n",
    "\n",
    "- Regularization helps the model less overfit.\n",
    "- In general, the norm of the weight vector $\\mathbf{w}$ is usually regularized \n",
    "$$ R(\\mathbf{w}) = || \\mathbf{w} ||_{q}^{q} $$\n",
    "\n",
    "The cost function with a more general regularizer is \n",
    "$$ J(\\mathbf{w}) = \\frac{1}{2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}^2) + \\frac{\\lambda}{2} \\sum_{i = 1}^{N} |w_i|^q $$\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_4.png\" width=\"600\" height=\"200\"><br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  Figure.3.2.2\n",
    "</div>\n",
    "\n",
    "\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In above picture, we can know that Lasso tends to generate sparser(most of $w_i$'s are nearly zero) solutions than a quadratic regularizer. We can consider above problem as optimization problem with constraint(Lagrange dual problem).\n",
    "<br><br>\n",
    "\n",
    "Quadratic RLS : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X\\mathbf{w} ||^2 + \\frac{\\lambda}{2} || \\mathbf{w} ||^2 \\} $$\n",
    "\n",
    "RLS solution : \n",
    "$$ \\mathbf{w} = (X^TX + \\lambda I )^{-1} X^T \\mathbf{y} $$\n",
    "\n",
    "- $ X^T X + \\lambda I $ is always invertible even if $X^T X$ is not invertible. (Trivial)\n",
    "- Regularized LS can be used regardless of $N$ and $m$.\n",
    "<br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "Trivial. $\\blacksquare$<br><br>\n",
    "\n",
    "$q$-norm RLS : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X\\mathbf{w} ||^2 + \\frac{\\lambda}{2} || \\mathbf{w} ||_q^q \\} $$\n",
    "\n",
    "RLS solution : \n",
    "$$ \\mathbf{w} = (X^TX + \\lambda I )^{-1} X^T \\mathbf{y} $$\n",
    "\n",
    "- The above optimization problem is convex.\n",
    "- It can be solved efficiently via the <strong>convex optimization techniques,</strong> e.g. interior point method at the computational complexity of $O((\\max\\{N, m\\})^3)$ : [Interior point method](https://en.wikipedia.org/wiki/Interior-point_method)\n",
    "- Also, the gradient descent based methods can be used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.8. Comparisons<br>\n",
    "Let $L$ is number of iteraions in gradient descent based method, and $N_{max} = \\max_i N_i, \\,\\ X(i) : N_i \\times m$\n",
    "\n",
    "| Batch gradient descent | Stochastic gradient descent |     Least squares     |  Recursive least squares  | \n",
    "|------------------------|-----------------------------|-----------------------|---------------------------|\n",
    "| $O(LNm)$               | $O(LN)$                     | $O((\\max\\{N, m\\})^3)$ |$O((\\max\\{N_{max}, m\\})^3)$|\n",
    "<br>\n",
    "\n",
    "- All the methods yields the same optimal performance\n",
    "- According to the situation, the complexity may be different\n",
    "- If the initial point can be chosen to be very close to the optimal point, then the gradient descent methods are the most efficient\n",
    "- Otherwise, the least squares may be more efficient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.9. Linear regression with basis functions<br>\n",
    "$$ \\hat{y_i} = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_i}) \\quad where \\quad \\mathbf{\\phi} = [\\phi_1, \\cdots, \\phi_m]^T $$\n",
    "- $\\mathbf{\\phi}(\\mathbf{x})$ is known as basis function.\n",
    "- Linear basis functions : $^\\forall i, \\,\\ \\phi_i(\\mathbf{x}) = x_i$\n",
    "- Polynomial basis functions : $ \\phi_i(x) = x^i $<br>\n",
    "    These are global functions(a small change in $x$ affect all basis functions)\n",
    "- Gaussian basis functions : $ \\phi_i(x) = \\exp \\{ - \\frac{(x- \\mu_i)^2}{2s^2} \\} $<br>\n",
    "    These are local(a small change in $x$ only affect nearby basis functions)\n",
    "- Sigmoid basis functions : $ \\phi_i(x) = \\sigma(\\frac{x-\\mu_i}{s}) $<br>\n",
    "    These are local. $\\mu_i$ and $s$ control location and scale(slope)\n",
    "\n",
    "<br><br>\n",
    "Cost function is set to be the sum of squares error \n",
    "$$ E(\\mathbf{w}) = \\frac{1}{2} \\sum_{i = 1}^{N} \\{ y_i - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i) \\}^2 $$\n",
    "\n",
    "Solution : \n",
    "$$ \n",
    "\\mathbf{w}_{\\text{LS}} = \\Phi^+ \\mathbf{y} = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{y}\n",
    "\\qquad \\text{where} \\qquad \n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "\\phi_1(\\mathbf{x}_1) & \\phi_2(\\mathbf{x}_1) & \\cdots & \\phi_m(\\mathbf{x}_1) \\\\\n",
    "\\phi_1(\\mathbf{x}_2) & \\phi_2(\\mathbf{x}_2) & \\cdots & \\phi_m(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_1(\\mathbf{x}_N) & \\phi_2(\\mathbf{x}_N) & \\cdots & \\phi_m(\\mathbf{x}_N) \\\\\n",
    "\\end{bmatrix}_{N \\times m}\n",
    "$$\n",
    "- In parctice, $N > M$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.10. Proper step size<br>\n",
    "When the number of training samples is very large$(N \\rightarrow \\infty)$, the least squares solution becomes\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}