{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.1. Regressive and approximated models\n",
    "3.1.1. General regressive model\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_1.png\" width=\"700\" height=\"200\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.1.1\n",
    "</div>\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.1.2. Approximated model\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_2.png\" width=\"700\" height=\"110\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.1.2\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "I want to find a function $\\hat{f}(\\mathbf{x}; D)$ that approximates the true function $f(\\mathbf{x})$ as well as possible in terms of the mean square error (MSE) between them, by means of some learning algorithm based on a training dataset (sample)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.2. Linear Regression\n",
    "3.2.1. Linearly approximated model<br>\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_3.png\" width=\"850\" height=\"130\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.1.3\n",
    "</div>\n",
    "<br>\n",
    "Linear regression techniques aim to find a linear function $\\hat{f} = w^T x$ that approximates the true function $f(x)$ as well as possible in terms of the mean square error (MSE) between them, based on a training dataset (sample)\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.2. Hypothesis<br>\n",
    "$$ \\hat{y} = \\sum_j w_i x_i + b = \\mathbf{w}^T \\mathbf{x} \\quad \\text{where} \\quad \\mathbf{w} = [w_1, \\,\\ \\cdots, \\,\\ w_{m-1}, \\,\\ b]^T, \\,\\ \\mathbf{x} = [x_1, \\,\\ \\cdots, \\,\\ x_{m-1}, \\,\\ 1]^T $$\n",
    "- $y$ : Target(of label)\n",
    "- $\\hat{y}$ : Output of model\n",
    "- $w_i$ : Weights\n",
    "- $b$ : Bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.3. Linear regression problem<br>\n",
    "Given the training set, to optimize the parameters $\\mathbf{w}$ to minimize the least squares error :\n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\} $$\n",
    "$J(\\mathbf{w})$ is convex and quadratic function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.4. Learning algorithm : A numerical approach<br>\n",
    "1) Gradient descent algorithm\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} $$\n",
    "$\\alpha$ is Learning rate.<br>\n",
    "\n",
    "2) Gradient calculation\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= \\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{2} \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i)^2 \\\\\n",
    "                                                   &= \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\\\\n",
    "                                                   &= \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i \\\\\n",
    "\\end{align*}                                          \n",
    "$$<br>\n",
    "\n",
    "3) Batch learning algorithm<br>\n",
    "Perform gradient descent step over the whole training set.<br>\n",
    "<strong>Repeat until convergence : </strong>\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i $$\n",
    "\n",
    "4) Online learning algorithm<br>\n",
    "Perform gradient descent step over a single training example.<br>\n",
    "<strong>Repeat until convergence : </strong>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{For } \\,\\ i &= 1 \\,\\ \\text{ to } \\,\\  N :  \\\\\n",
    "& \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "All of these algorithms converge to the global optimal point!(convex and quadratic!)<br>\n",
    "Update depends on the error, small(or large) update when the error is small(or large).<br>\n",
    "It is called Widrow-Hoff(or LMS) learning rule.<br><br>\n",
    "\n",
    "In big learning late, these are unstable(zigzaging).<br>\n",
    "In small learning late, these converge slowly.<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.5. Learning algorithm : Least squares(One-shot learning approach)<br>\n",
    "Let's rewrite the cost function in a compact form as\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\\\\n",
    "              &= \\frac{1}{2} \n",
    "\\begin{bmatrix}\n",
    "y_1 - \\mathbf{x}_1^T \\mathbf{w} & y_2 - \\mathbf{x}_2^T \\mathbf{w} & \\cdots & y_N - \\mathbf{x}_N^T \\mathbf{w}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\mathbf{x}_1^T \\mathbf{w} \\\\\n",
    "y_2 - \\mathbf{x}_2^T \\mathbf{w} \\\\\n",
    " \\vdots \\\\\n",
    "y_N - \\mathbf{x}_N^T \\mathbf{w} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "             &= \\frac{1}{2} (\\mathbf{y} - X \\mathbf{w})^T (\\mathbf{y} - X \\mathbf{w}) \\\\\n",
    "             &= \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{y} = [y_1, \\,\\ \\cdots, \\,\\ y_N]_{N \\times 1}^T $\n",
    "- $ X = [\\mathbf{x}_1, \\,\\ \\cdots, \\,\\ \\mathbf{x}_N]_{N \\times m}^T $\n",
    "- $m$ : Order of model\n",
    "- $N$ : Number of training examples\n",
    "<br>\n",
    "\n",
    "Thus, this problem can be considered as the ordinary least squares problem : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2 \\} $$\n",
    "\n",
    "<strong>Case 1 : Exact and unique solution when </strong> $ N = m = \\text{Rank}(X) $<br>\n",
    "$$ \\mathbf{w} = X^{-1} y $$\n",
    "When $N = m$ and $X$ is full-rank, the zero error can be achieved. Using a simple linear model and a set of training samples, it is able to perfectly estimate an unknown function. However, this ideal case of $N = m$ rarely occurs in practice(Not practical case).\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 2 : Over-determined case when </strong> $ N > m = \\text{Rank}(X) $<br>\n",
    "General case(Most practical case). But no unique solution to the equation $\\mathbf{y} = X\\mathbf{w}$.<br>\n",
    "Instead, try to minimize the error:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2 \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{y} - X \\mathbf{w})^T (\\mathbf{y} - X \\mathbf{w}) \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{y}^T X \\mathbf{w} + \\mathbf{w}^T X^T X \\mathbf{w}) \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} J(\\mathbf{w}) &= \\frac{1}{2} \\frac{\\partial}{\\partial \\mathbf{w}} ( \\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{y}^T X \\mathbf{w} + \\mathbf{w}^T X^T X \\mathbf{w} ) \\\\\n",
    "              &= X^T X \\mathbf{w} - X^T \\mathbf{y} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{w}} J(\\mathbf{w}) = \\mathbf{0} \\quad \\Longleftrightarrow \\quad X^T X \\mathbf{w} = X^T \\mathbf{y} \\quad \\text{(Normal equation)}$$\n",
    "LS solution : $\\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y}$\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 3 : Under-determined case when </strong> $ m > N = \\text{Rank}(X) $<br>\n",
    "There are infinitely many solutions to the equation $\\mathbf{y} = X \\mathbf{w}$, each of which yields the zero error. Try to obtain a particular solution with the minimum norm :\n",
    "$$ \\min_{\\mathbf{w}} \\frac{1}{2} || \\mathbf{w} ||^2 \\qquad s.t. \\qquad \\mathbf{y} = X \\mathbf{w} $$\n",
    "\n",
    "__CHECK_POINT__ : 라그랑주 승수법 ㄱㄱ\n",
    "\n",
    "\n",
    "case4 -> XX^T or X^TX 비가역적. -> 무어펜로즈 의사 역행렬\n",
    "\n",
    "\n",
    "LS의 기하학적 해석"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.3. Bayesian Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.4. Logistic and Softmax Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.5. $k$-Nearest Neighbors($k$-NN) Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}