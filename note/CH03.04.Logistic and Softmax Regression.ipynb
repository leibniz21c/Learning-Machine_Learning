{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.4. Logistic and softmax regression\n",
    "3.4.1. Logistic regression : hypothesis<br>\n",
    "To estimate the probability of Bernoulli Random Variable(or a binary labels), $y \\in \\{0, 1\\}$<br>\n",
    "Let \n",
    "$$ \\widehat{\\Pr}(y = 1) = f(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) \\quad \\text{where} \\,\\ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$\n",
    "\n",
    "- $w_j$ & $b$ : weights & bias\n",
    "- $\\mathbf{w} = [b, \\,\\ w_1, \\,\\ w_2, \\,\\ \\cdots]^T $ \n",
    "- $\\mathbf{x} = [1, \\,\\ x_1, \\,\\ x_2, \\,\\ \\cdots]^T $\n",
    "\n",
    "Probability density function is \n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y = 1 | \\mathbf{x} ; \\mathbf{w}) &= \\sigma(\\mathbf{w}^T \\mathbf{x}) \\\\\n",
    "p(y = 0 | \\mathbf{x} ; \\mathbf{w}) &= 1 - \\sigma(\\mathbf{w}^T \\mathbf{x}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ p(y | \\mathbf{x} ; \\mathbf{w}) = [\\sigma(\\mathbf{w}^T \\mathbf{x})]^y [1 - \\sigma(\\mathbf{w}^T \\mathbf{x})]^{1-y} $$\n",
    "\n",
    "If we assume that the training examples are i.i.d. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\mathbf{w}) &= \\log p(\\mathbf{y} | X; \\mathbf{w}) = \\log \\prod_i p(y_i | \\mathbf{x}_i ; \\mathbf{w}) \\\\\n",
    "              &= \\log \\prod_i [\\sigma(\\mathbf{w}^T \\mathbf{x}_i)]^{y_i} [1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)]^{1-y_i} \\\\\n",
    "              &= \\sum_i y_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Given the training set, learning the parameters to maximize the log-likelihood function:\n",
    "$$ \\max_\\mathbf{w} L(\\mathbf{w}) \\quad  \\text{which is concave.}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.2. Logistic regression : learning based on gradient ascent algorithm<br>\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} $$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} &= (y \\frac{1}{\\sigma(\\mathbf{w}^T \\mathbf{x})} - (1-y) \\frac{1}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x})}) \\frac{\\partial}{\\partial \\mathbf{w}} \\sigma(\\mathbf{w}^T \\mathbf{x}) \\qquad (\\because \\,\\ \\text{Chain rule}) \\\\\n",
    "                                                   &= (y \\frac{1}{\\sigma(\\mathbf{w}^T \\mathbf{x})} - (1-y) \\frac{1}{1 - \\sigma(\\mathbf{w}^T \\mathbf{x})}) \\sigma(\\mathbf{w}^T \\mathbf{x}) (1 - \\sigma(\\mathbf{w}^T \\mathbf{x})) \\frac{\\partial}{\\partial \\mathbf{w}} \\mathbf{w}^T \\mathbf{x} \\qquad (\\because \\,\\ \\text{Chain rule}) \\\\\n",
    "                                                   &= (y(1 - \\sigma(\\mathbf{w}^T \\mathbf{x})) - (1-y) \\sigma(\\mathbf{w}^T \\mathbf{x}))\\mathbf{x} \\\\\n",
    "                                                   &= (y - \\sigma(\\mathbf{w}^T \\mathbf{x}))\\mathbf{x} \\qquad (\\text{LMS learning rule})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, batch learning update form is \n",
    "$$ \\mathbf{w} \\leftarrow \\alpha \\sum_i (y_i - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) \\mathbf{x}_i $$\n",
    "\n",
    "Also, online learning update form is \n",
    "$$ \\mathbf{w} \\leftarrow \\alpha (y_i - \\sigma(\\mathbf{w}^T \\mathbf{x}_i)) \\mathbf{x}_i $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.3. Logistic regression : learning via Iterative Reweighted Least Squares(IRLS) based on Newton-Rapson method<br>\n",
    "Newton-Rapson method is in ___Calculus Early Transcendentals 8th Ed p. 345___. <br>\n",
    "Newton-Rapson method is an iterative algorithm to seek a solution $f(x) = 0$. <br>\n",
    "In the same context, we can find a solution $\\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{0} $.\n",
    "\n",
    "$$ \\mathbf{y} = [\\nabla \\mathbf{f}(\\mathbf{w})]^T (\\mathbf{w} - \\mathbf{w}_0) + \\mathbf{f}(\\mathbf{w}_0) $$\n",
    "\n",
    "$$ \\Rightarrow \\quad \\mathbf{w}_1 = \\mathbf{w}_0 - ([\\nabla \\mathbf{f}(\\mathbf{w})]^T)^{-1} \\mathbf{f}(\\mathbf{w}_0) $$\n",
    "\n",
    "Let $ \\mathbf{f}(\\mathbf{w}) = \\nabla L(\\mathbf{w}) $.\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{H}(\\mathbf{w}_0) (\\mathbf{w} - \\mathbf{w}_0) + \\nabla L(\\mathbf{w}_0) = \\mathbf{0} $$ \n",
    "\n",
    "$$ \\therefore \\quad \\mathbf{w}_1 = \\mathbf{w}_0 - \\mathbf{H}(\\mathbf{w}_0)^{-1} \\nabla L(\\mathbf{w}_0) $$\n",
    "\n",
    "It also can expand based on the Taylor series. <br>\n",
    "A definition of taylor series of a differentiable function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R} $ is  \n",
    "\n",
    "$$ T_f(x_1, x_2, \\cdots, x_d) = \\sum_{n_1 = 0}^{\\infty} \\sum_{n_2 = 0}^{\\infty} \\cdots \\sum_{n_d = 0}^{\\infty} \\frac{(x_1 - a_1)^{n_1} (x_2 - a_2)^{n_2} \\cdots (x_d - a_d)^{n_d}}{n_1 ! n_2 ! \\cdots n_d !} (\\frac{\\partial^{n_1 + n_2 + \\cdots + n_d} f}{\\partial x_1^{n_1} \\partial x_2^{n_2} \\cdots \\partial x_d^{n_d}}) (a_1, a_2, \\cdots, a_d) $$\n",
    "\n",
    "In above context, we can write second-degree taylor expansion.\n",
    "$$ L(\\mathbf{w} + \\Delta \\mathbf{w}) \\approx L(\\mathbf{w}) + [\\nabla L(\\mathbf{w})]^T \\Delta \\mathbf{w} + \\frac{1}{2} \\Delta \\mathbf{w}^T \\mathbf{H}(\\mathbf{w}) \\Delta\\mathbf{w} $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\Delta \\mathbf{w}}L(\\mathbf{w} + \\Delta \\mathbf{w}) \\approx \\nabla L(\\mathbf{w}) + \\mathbf{H}(\\mathbf{w}) \\Delta \\mathbf{w} = \\mathbf{0} $$\n",
    "\n",
    "$$ \\Delta \\mathbf{w} = \\mathbf{H}(\\mathbf{w})^{-1} \\nabla L (\\mathbf{w}) $$\n",
    "\n",
    "$$ \\therefore \\quad \\mathbf{w} \\leftarrow \\mathbf{w} - \\mathbf{H}(\\mathbf{w})^{-1} \\nabla L(\\mathbf{w}) $$\n",
    "\n",
    "- Generally speaking, Newton's method converges quickly asymptotically and does not exhibit the zigzagging behavior that sometimes characterizes the method of the gradient descent.\n",
    "- However, for Newton's method to work, the Hessian $\\mathbf{H}(\\mathbf{w})$ has to be a __positive definite matrix__ for all \\mathbf{w}.\n",
    "- Unfortunately, in general, there is no guarantee that $\\mathbf{H}(\\mathbf{w})$ is positive definite at every iteration of the algorithm.\n",
    "- If the Hessian $\\mathbf{H}(\\mathbf{w})$ is not positive definite, modification of Newton's method is necessary. (Bertsekas, 1995)\n",
    "- In any event, a major limitation of Newtonâ€™s method is its computational complexity.\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\mathbf{H}(\\mathbf{w})^{-1} \\nabla L(\\mathbf{w}) = \\mathbf{w} + (X^T S X)^{-1} X^T (\\mathbf{y} - \\mathbf{z}) $$\n",
    "\n",
    "$ \\text{where} \\quad \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ v_N \\end{bmatrix}, \\,\\ \\mathbf{z} = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_N \\end{bmatrix}, \\,\\ z_i = \\sigma(\\mathbf{w}^T \\mathbf{x}_i), \\,\\ X = \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} = \\begin{bmatrix}\n",
    " 1 & x_1(1) & x_1(2) & \\cdots \\\\\n",
    " 1 & x_2(1) & x_2(2) & \\cdots \\\\ \n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " 1 & x_N(1) & x_N(2) & \\cdots \\\\\n",
    " \\end{bmatrix}, \\,\\ \\mathbf{x}_i = \\begin{bmatrix} x_i(1) \\\\ x_i(2) \\\\ \\vdots \\end{bmatrix} $<br>\n",
    "$ S = diag(z_1(1-z_1), \\cdots z_N(1-z_N)) = \n",
    "\\begin{bmatrix}\n",
    "z_1(1 - z_1) & 0 & \\cdots & 0 \\\\\n",
    " 0 & z_2(1 - z_2) & \\cdots & 0 \\\\ \n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " 0 & 0 & \\cdots & z_N(1 - z_N) \\\\\n",
    " \\end{bmatrix} $<br><br>\n",
    "\n",
    " <strong>Proof.</strong><br>\n",
    " [PDF File (too long)](./res/ch03/note_IRLS.pdf)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_{new} &= \\mathbf{w}_{old} - (X^T S X)^{-1} X^T (\\mathbf{y} - \\mathbf{z}) \\\\\n",
    "                 &= (X^T S X)^{-1} \\{X^T S X \\mathbf{w}_{old} - X^T (\\mathbf{y} - \\mathbf{z}) \\} \\\\\n",
    "                 &= (X^T S X)^{-1} X^T S \\{X \\mathbf{w}_{old} - S^{-1} X^T (\\mathbf{y} - \\mathbf{z}) \\} \\\\\n",
    "                 &= (X^T S X)^{-1} X^T S \\mathbf{b} \\quad \\text{where} \\quad \\mathbf{b} = X \\mathbf{w}_{old} - S^{-1} X^T (\\mathbf{y} - \\mathbf{z})\n",
    "\\end{align*}  \n",
    "$$\n",
    "\n",
    "Above form is generalized version of the least squares solution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.4. Logistic regression : Binary classification<br>\n",
    "\n",
    "- Classification rule : \n",
    "$$ f(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) \\underset{y = 0}{\\overset{y = 1}\\lessgtr} 0.5 $$\n",
    "\n",
    "Logistic classification can be considered as a linear classification\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) \\underset{y = 0}{\\overset{y = 1} \\lessgtr} 0.5 \\quad \\rightleftharpoons \\quad g(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} \\underset{y = 0}{\\overset{y = 1} \\lessgtr} 0 $$\n",
    "\n",
    "$$ g(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} = \\log \\frac{p(y = 1 | \\mathbf{x}, \\mathbf{w})}{p(y = 0 | \\mathbf{x}, \\mathbf{w})} \\underset{y = 0}{\\overset{y = 1} \\lessgtr} 0 $$\n",
    "\n",
    "If the data is generated from the logistic model, then the logistic classification has the optimality in the maximum likelihood (ML) sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.5. Softmax regression : Overview<br>\n",
    "Generalization when $K > 2$ \n",
    "$$ p(C_k | \\mathbf{x}) = \\frac{p(\\mathbf{x} | C_k)p(C_k)}{\\sum_j p(\\mathbf{x} | C_j)p(C_j)} = \\frac{\\exp(a_k)}{\\sum_j \\exp(a_j)} \\quad \\text{where} \\quad a_k = \\ln(p(\\mathbf{x}|C_k)p(C_k))  $$\n",
    "\n",
    "When $K = 2$, the softmax function becomes the logistic function\n",
    "$$ \\frac{\\exp(a_1)}{\\exp(a_1) + \\exp(a_2)} = \\frac{1}{1 + \\exp(a_2 - a_1)} = \\frac{1}{1 + \\exp(a)} = \\sigma(a) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.6. Softmax regression : Hypothesis<br>\n",
    "To estimate probabilityes of $K$ labels, $y \\in \\{1, \\cdots, K\\}$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\begin{bmatrix} p(y = 1| \\mathbf{x}, W) \\\\ p(y = 2| \\mathbf{x}, W) \\\\ \\vdots \\\\ p(y = K| \\mathbf{x}, W) \\end{bmatrix} = \\frac{1}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x})} \\begin{bmatrix} \\exp(\\mathbf{w}_1^T \\mathbf{x}) \\\\ \\exp(\\mathbf{w}_2^T \\mathbf{x}) \\\\ \\vdots \\\\  \\exp(\\mathbf{w}_K^T \\mathbf{x}) \\end{bmatrix}$$\n",
    "\n",
    "Log-likelihood function \n",
    "\n",
    "$$ L(\\mathbf{w}) = \\sum_i \\sum_{k = 1}^{K} \\mathbf{1}(y_i = k) \\log \\frac{\\exp(\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)} \\qquad \\text{(Cross-entropy loss)}$$\n",
    "\n",
    "Given the training set, learning the parameters to maximize the log-likelihood function:\n",
    "$$ \\max_W L(\\mathbf{w}) \\qquad \\text{(Concave)} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.7. Softmax regression : Derivative of softmax function<br>\n",
    "$$ \\pi(z_k) = \\frac{e^{z_k}}{\\sum_{k^\\prime = 1}^{K} e^{z_k^\\prime}} $$\n",
    "\n",
    "$ \\text{If} \\,\\ k = j : $<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\pi(z_k)}{\\partial z_j} &= \\frac{\\partial}{\\partial z_j} \\frac{e^{z_k}}{\\sum_{k^\\prime = 1}^{K} e^{z^{k^\\prime}}} \\\\\n",
    "                                       &= \\frac{e^{z_k}(\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}} - e^{z_j})}{(\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}})^2} \\\\\n",
    "                                       &= \\frac{e^{z_k}}{\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}}} \\frac{\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}} - e^{z_j}}{\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}}} \\\\\n",
    "                                       &= \\pi(z_k) (1 - \\pi(z_j)) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$ \\text{If} \\,\\ k \\neq j : $<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\pi(z_k)}{\\partial z_j} &= \\frac{\\partial}{\\partial z_j} \\frac{e^{z_k}}{\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}}} \\\\\n",
    "                                       &= \\frac{0 - e^{z_k} e^{z_j}}{(\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}})^2} \\\\\n",
    "                                       &= - \\frac{e^{z_k}}{\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}}} \\frac{e^{z_j}}{\\sum_{k^\\prime = 1}^{K} e^{z_{k^\\prime}}} \\\\\n",
    "                                       &= - \\pi(z_k) \\pi(z_j)\n",
    "\\end{align*}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.8. Softmax regression : learning based on gradient ascent algorithm<br>\n",
    "$$ \\mathbf{w}_k \\leftarrow \\mathbf{w}_k + \\alpha \\frac{\\partial L(W)}{\\partial \\mathbf{w}_k}, \\quad k = 1,2,\\cdots, K $$\n",
    "$$ \\text{where} \\quad \\frac{\\partial L(W)}{\\partial \\mathbf{w}_k} = \\sum_i \\mathbf{x}_i [\\mathbf{1}(y_i = k) - \\frac{\\exp(\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)}] \\quad \\text{(LMS learning rule)}$$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(W)}{\\partial \\mathbf{w}_l} &= \\sum_i \\sum_{k = 1}^{K} \\mathbf{1}(y_i = k) \\frac{\\partial}{\\partial \\mathbf{w}_l} \\log \\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)} \\\\\n",
    "                                            &= \\sum_i \\sum_{k = 1}^{K} \\mathbf{1}(y_i = k) \\frac{1}{ \\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp (\\mathbf{w}_j^T \\mathbf{x}_i)}} \\frac{\\partial}{\\partial \\mathbf{w}_l} \\frac{\\exp(\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w_j^T \\mathbf{x}_i})} \\\\\n",
    "                                            &= \\sum_i \\sum_{k = 1}^{K} \\mathbf{1}(y_i = k) \\frac{1}{ \\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp (\\mathbf{w}_j^T \\mathbf{x}_i)}} \\frac{\\exp(\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w_j^T \\mathbf{x}_i})} (\\mathbf{1}_{k = l} - \\frac{\\exp (\\mathbf{w}_l^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)}) \\frac{\\partial(\\mathbf{w}_l^T \\mathbf{x}_i)}{\\partial \\mathbf{w}_l} \\\\\n",
    "                                            &= \\sum_i \\sum_{k = 1}^{K} \\mathbf{1}(y_i = k) (\\mathbf{1}_{k = l} - \\frac{\\exp (\\mathbf{w}_l^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)}) \\mathbf{x}_i \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\therefore \\quad \\frac{\\partial L(W)}{\\partial \\mathbf{w}_l} = \n",
    "\\begin{cases}\n",
    "\\sum_{i} \\mathbf{x}_i \\left[ \\mathbf{1}(y_i = k) - \\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)} \\right] & \\text{if} \\,\\ l = k \\\\\n",
    "0 & \\text{if} \\,\\ l \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Batch learning :\n",
    "\n",
    "$$ \\mathbf{w}_k \\leftarrow \\mathbf{w}_k + \\alpha \\sum_{i} \\mathbf{x}_i \\left[ \\mathbf{1}(y_i = k) - \\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)} \\right] $$\n",
    "\n",
    "Online learning :\n",
    "\n",
    "$$ \\mathbf{w}_k \\leftarrow \\mathbf{w}_k + \\alpha \\mathbf{x}_i \\left[ \\mathbf{1}(y_i = k) - \\frac{\\exp (\\mathbf{w}_k^T \\mathbf{x}_i)}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x}_i)} \\right] $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.9. Softmax regression : learning via Iterative Reweighted Least Squares(IRLS) based on Newton-Rapson method<br>\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\mathbf{H}^{-1} \\nabla_\\mathbf{w} L(W) $$\n",
    "\n",
    "$ \\text{where} $ $ \\mathbf{H} = \\begin{bmatrix}\n",
    "\\mathbf{H}_{11} & \\mathbf{H}_{12} & \\cdots  & \\mathbf{H}_{1K} \\\\\n",
    "\\mathbf{H}_{21} & \\mathbf{H}_{22} & \\cdots  & \\mathbf{H}_{2K} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{H}_{K1} & \\mathbf{H}_{K2} & \\cdots  & \\mathbf{H}_{KK} \\\\\n",
    "\\end{bmatrix}, $ $ \\,\\ \\mathbf{H}_{kj} \\triangleq \\frac{\\partial^2 L(W)}{\\partial \\mathbf{w}_k \\partial \\mathbf{w}_j} = - \\sum_i z_{ki} (\\mathbf{1}_{k = j} - z_{ji}) \\mathbf{x}_i \\mathbf{x}^T, $\n",
    "\n",
    "$$ \n",
    "\\nabla_\\mathbf{w} L(W) = \\left[ \\frac{\\partial L(W)}{\\partial \\mathbf{w}_1} \\,\\ \\frac{\\partial L(W)}{\\partial \\mathbf{w}_2} \\,\\ \\cdots \\,\\ \\frac{\\partial L(W)}{\\partial \\mathbf{w}_K}  \\right]^T,  \\,\\\n",
    "\\frac{\\partial L(W)}{\\partial \\mathbf{w}_k} = \\sum_i \\mathbf{x}_i [\\mathbf{1}(y_i = k) - z_{ki}] = X^T (\\mathbf{y} - \\mathbf{z}_k) \\\\\n",
    "$$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "On the same way in 3.4.3, it can solved. $\\blacksquare$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.4.10. Softmax regression : Multi-Class classification via softmax regression<br>\n",
    "Classification rule :\n",
    "\n",
    "$$ y = \\underset{_{k \\in \\{1, \\cdots , K\\}}}{\\arg \\max} \\left\\{ f(\\mathbf{x}) = \\frac{1}{\\sum_{j = 1}^{K} \\exp(\\mathbf{w}_j^T \\mathbf{x})} \\begin{bmatrix} \\exp(\\mathbf{w}_1^T \\mathbf{x}) \\\\ \\exp(\\mathbf{w}_2^T \\mathbf{x}) \\\\ \\vdots \\\\ \\exp(\\mathbf{w}_K^T \\mathbf{x}) \\end{bmatrix} \\right\\} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}