{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.4. Logistic and softmax regression\n",
    "3.4.1. Logistic regression : hypothesis<br>\n",
    "To estimate the probability of Bernoulli Random Variable(or a binary labels), $y \\in \\{0, 1\\}$<br>\n",
    "Let \n",
    "$$ \\widehat{\\Pr}(y = 1) = f(\\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x}) \\quad \\text{where} \\,\\ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$\n",
    "\n",
    "- $w_j$ & $b$ : weights & bias\n",
    "- $\\mathbf{w} = [b, \\,\\ w_1, \\,\\ w_2, \\,\\ \\cdots]^T $ \n",
    "- $\\mathbf{x} = [1, \\,\\ x_1, \\,\\ x_2, \\,\\ \\cdots]^T $\n",
    "\n",
    "Probability density function is \n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y = 1 | \\mathbf{x} ; \\mathbf{w}) &= \\sigma(\\mathbf{w}^T \\mathbf{x}) \\\\\n",
    "p(y = 0 | \\mathbf{x} ; \\mathbf{w}) &= 1 - \\sigma(\\mathbf{w}^T \\mathbf{x}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ p(y | \\mathbf{x} ; \\mathbf{w}) = [\\sigma(\\mathbf{w}^T \\mathbf{x})]^y [1 - \\sigma(\\mathbf{w}^T \\mathbf{x})]^{1-y} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}