{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Appendix.03 Probability Theory\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.1 Probability\n",
    "1. Subjective(Empirical) Definition :<br>\n",
    "> Our measure of how much we believe something to be true.<br><br>\n",
    "\n",
    "2. Relative Frequency Definition : <br>\n",
    "> Repeating the physical process an extreamly large number of times(trials) and then to look at the fraction of times that the outcome of interest occurs.<br><br>\n",
    "\n",
    "3. Mathematical(Classical) Definition :<br>\n",
    "> The ratio of a number of interested outcomes to the of number of all possible outcomes, presuming they all equally likely.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Axiom.A.3.1 Nonnegativity\n",
    "$$ P(A) \\ge 0 \\quad \\forall \\,\\ \\text{event} \\,\\ A $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Axiom.A.3.2 Additivity\n",
    "$$ P(A \\cup B) = P(A) + P(B) \\quad if \\quad A \\cap B = \\emptyset $$\n",
    "$$ P(\\bigcup_{n=1}^{\\infty} A_n) = \\sum_{n=1}^{\\infty} P(A_n) \\quad if \\quad A_i \\cap A_j \\neq \\emptyset, \\,\\ \\forall i \\neq j$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Axiom.A.3.3 Normalization\n",
    "$$ P(\\Omega) = 1$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.2 Conditional Probability of A given B\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{for} \\quad P(B) > 0 $$\n",
    "$$ \\Rightarrow P(A \\cap B) = P(A|B) P(B) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Thorem.A.3.1 Total Probability Theorem\n",
    "$$ \\text{If} \\quad A_i \\cap A_j \\neq \\emptyset \\quad \\text{for} \\,\\ 1 \\le i \\le n, \\,\\ 1 \\le j \\le n, \\,\\ i \\neq j, \\,\\ P(A_i) > 0, \\,\\ \\forall i, \\,\\ where \\,\\ \\bigcup_{i = 1}^{n} A_i = \\Omega, $$\n",
    "$$ \\text{then} \\quad P(B) = P(A_1 \\cap B) + \\cdots + P(A_n \\cap B) = P(B|A_1)P(A_1) + \\cdots + P(B|A_n)P(A_n) = P(B) = \\sum_i P(B|A_i)P(A_i) $$\n",
    "<strong>Proof)</strong><br>\n",
    "Trivial $ \\blacksquare $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.3.2 Bayes Rule\n",
    "$$ \\text{If} \\quad A_i \\cap A_j = \\emptyset \\,\\ for \\,\\ 1 \\le i \\le n, \\,\\ 1 \\le j \\le n, \\,\\ i \\neq j, \\,\\ P(A_i) > 0, \\,\\ \\forall i, \\,\\ \\bigcup_{i=1}^{n} A_i = \\Omega, $$\n",
    "$$ \\text{then} \\quad P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{P(B)} = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{n} P(B|A_j)P(A_j)} \\quad for \\,\\ P(B) > 0 $$\n",
    "<strong>Proof)</strong><br>\n",
    "$$\n",
    "\\begin{matrix}\n",
    "P(A_i \\cap B) &=& P(A_i|B)P(B) \\qquad \\cdots \\quad (1) \\\\\n",
    "              &=& P(B|A_i)P(A_i) \\qquad \\cdots \\quad (2) \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "$$\n",
    "\\begin{matrix}\n",
    "P(A_i | B) &=& \\frac{P(B|A_i)P(A_i)}{P(B)} \\qquad (\\because \\,\\ by \\,\\ (1), \\,\\ (2) \\,\\ ) \\\\\n",
    "           &=& \\frac{P(B|A_i)P(A_i)}{ \\sum_{j=1}^{n} P(B|A_j)P(A_j) } \\qquad (\\because Theorem.A.3.1 ) \\quad \\blacksquare\\\\\n",
    "\\end{matrix} \n",
    "$$\n",
    "We can express the posterior probability in terms of the prior probability and likelihood."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.3 Independence\n",
    "$A$ and $B$ are independence, if and only if, \n",
    "$$ P(A \\cap B) = P(A)P(B) $$\n",
    "$$\n",
    "\\begin{align*}\n",
    " \\Rightarrow \\quad &P(A|B) = P(A) \\quad \\text{for} \\,\\ P(B) > 0 \\\\\n",
    "                   &P(B|A) = P(B) \\quad \\text{for} \\,\\ P(A) > 0 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.3.3 Independence and disjoint\n",
    "$$ \\text{Independent} \\quad ^\\rightarrow_\\nleftarrow \\quad \\text{Disjoint} $$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$i)$ $A, A^c$ : disjoint, but dependent.<br>\n",
    "$ii)$ \n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A \\cup B) &= P(A) + P(B) - P(A \\cap B) \\\\\n",
    "            &= P(A) + P(B) \\quad (\\because \\,\\ P(A \\cap B) = 0)\n",
    "\\end{align*} \\qquad \\blacksquare\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.4 Discrete random variables\n",
    "If the random variable $X \\in S $\n",
    "\n",
    "$$ s.t. \\quad S = \\{ x_1, x_2, \\cdots \\} \\quad S \\,\\ \\text{is a countable set, } $$\n",
    "\n",
    "then $X$ is called discrete random variable. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.5 Probability mass functions(PMFs)\n",
    "In a discrete random variable $X$, \n",
    "$$ P_X(x) = P(\\{X = x\\}) $$ is called probability mass function of random variable $X$.<br>\n",
    "It has <br>\n",
    "$$ \\sum_x P_X(x) = 1, \\qquad P(X \\in S) = \\sum_{x \\in S} P_X(x) = 1 $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.6 Expectation\n",
    "Suppose $X$ is a discrete random variable. <br>\n",
    "Expectation of $X$ is \n",
    "$$ \\mathbb{E}[X] = \\sum_x x p_X(x).$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.3.4 Expected value rule\n",
    "$$ \\mathbb{E}[g(X)] = \\sum_x g(x) p_X(x) $$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "Let $Y = g(X)$<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[g(X)] &= E[Y] \\\\\n",
    "                 &= \\sum_y y P_Y(y) \\\\\n",
    "                 &= \\sum_y \\sum_{\\{x : g(x) = y\\}} y p_X(x) \\\\\n",
    "                 &= \\sum_y \\sum_{\\{x : g(x) = y\\}} g(x) p_X(x) \\\\\n",
    "                 &= \\sum_x g(x) p_X(x) \\qquad \\blacksquare\n",
    "\\end{align*}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.7 Variance, standard deviation and $n$th moment\n",
    "Suppose $X$ is a discrete random variable. <br>\n",
    "Variance of $X$ is \n",
    "$$ Var(X) = \\mathbb{E}[(X - E[X])^2]. $$\n",
    "Standard deviation of $X$ is\n",
    "$$ \\sigma_X = \\sqrt{Var(X)}. $$\n",
    "$n$th moment of $X$ is \n",
    "$$ \\mathbb{E}[X^n] = \\sum_x x^n p_X(x). $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.3.5 Variance rule\n",
    "$$ Var(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 $$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "Trivial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.3.6 Properties of mean and variance\n",
    "Let $Y = aX + b$<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[Y] &= \\sum_x (ax + b)p_X(x) \\\\\n",
    "     &= a \\sum_x xp_X(x) + b\\sum_x p_X(x) \\\\\n",
    "     &= a \\mathbb{E}[X] + b \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Var[Y] &= \\sum_x (ax + b - \\mathbb{E}[aX + b])^2p_X(x) \\\\\n",
    "       &= \\sum_x (ax + b - a \\mathbb{E}[X] - b)^2p_X(x) \\\\\n",
    "       &= a^2 \\sum_x (x - \\mathbb{E}[X])^2 p_X(x) \\\\ \n",
    "       &= a^2 Var(X) \\qquad \\text{which is called shift-invariance.} \\qquad \\blacksquare\n",
    "\\end{align*}\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.8 Joint PMFs of multiple random variables\n",
    "Suppose $X$ and $Y$ are discrete random variables. <br>\n",
    "\n",
    "$$ P_{X, Y} (x, y) = P(X = x, Y = y) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.3.7 Marginal PMF of multiple random variables\n",
    "Suppose $X$ and $Y$ are discrete random variables. <br>\n",
    "\n",
    "$$ p_X(x) = \\sum_y p_{X, Y}(x, y), \\quad p_Y(y) = \\sum_x p_{X, Y}(x, y) $$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$$ p_X(x) = P(\\{X = x \\cap Y = y_1\\} \\cup \\{X = x \\cap Y = y_2\\} \\cup \\cdots \\{X = x \\cap Y = y_k\\}) $$\n",
    "\n",
    "$$ p_Y(y) = P(\\{X = x_1 \\cap Y = y\\} \\cup \\{X = x_2 \\cap Y = y\\} \\cup \\cdots \\{X = x_j \\cap Y = y\\}) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.3.9 Conditional PMF given an event and random variable\n",
    "With given event $A$, \n",
    "$$ p_{X|A}(x) = P(X = x | A) = \\frac{P(\\{X = x\\} \\cap A)}{P(A)}.  $$\n",
    "With given a random variable $Y$,\n",
    "$$ p_{X | Y}(x | y) = P(X = x | Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)} = \\frac{p_{X, Y}(x, y)}{P_Y(y)} $$\n",
    "\n",
    "- Normalization : \n",
    "$$ \\sum_x p_{X|Y}(x|y) = \\frac{1}{P_Y(y)} \\sum_x P_{X, Y}(x, y) = \\frac{P_Y(y)}{P_Y(y)} = 1 $$\n",
    "- Joint PMF :\n",
    "$$ P_{X, Y} (x, y) = P_{X|Y}(x|y)p_Y(y) = p_{Y|X}(y|x)p_X(x) $$\n",
    "\n",
    "$$ p_{X, Y, Z}(x, y, z) = p_X(x) p_{Y|X}(y|x) p_{Z|X,Y} (z | x, y)$$\n",
    "- Marginal PMF\n",
    "$$ p_x(x) = \\sum_y p_{X|Y}(x|y) p_Y(y) $$\n",
    "\n",
    "$$ p_Y(y) = \\sum_x p_{Y|X}(y|x) p_X(x) $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}