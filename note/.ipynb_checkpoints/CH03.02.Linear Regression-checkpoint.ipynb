{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Linear Regression\n",
    "3.2.1. Linearly approximated model<br>\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_3.png\" width=\"850\" height=\"130\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.2.1\n",
    "</div>\n",
    "<br>\n",
    "Linear regression techniques aim to find a linear function $\\hat{f} = w^T x$ that approximates the true function $f(x)$ as well as possible in terms of the mean square error (MSE) between them, based on a training dataset (sample)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2. Hypothesis<br>\n",
    "$$ \\hat{y} = \\sum_j w_i x_i + b = \\mathbf{w}^T \\mathbf{x} \\quad \\text{where} \\quad \\mathbf{w} = [w_1, \\,\\ \\cdots, \\,\\ w_{m-1}, \\,\\ b]^T, \\,\\ \\mathbf{x} = [x_1, \\,\\ \\cdots, \\,\\ x_{m-1}, \\,\\ 1]^T $$\n",
    "- $y$ : Target(of label)\n",
    "- $\\hat{y}$ : Output of model\n",
    "- $w_i$ : Weights\n",
    "- $b$ : Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3. Linear regression problem<br>\n",
    "Given the training set, to optimize the parameters $\\mathbf{w}$ to minimize the least squares error :\n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\} $$\n",
    "$J(\\mathbf{w})$ is convex and quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.4. Learning algorithm : A numerical approach<br>\n",
    "1) Gradient descent algorithm\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} $$\n",
    "$\\alpha$ is Learning rate.<br>\n",
    "\n",
    "2) Gradient calculation\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= \\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{2} \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i)^2 \\\\\n",
    "                                                   &= \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\\\\n",
    "                                                   &= \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i \\\\\n",
    "\\end{align*}                                          \n",
    "$$<br>\n",
    "\n",
    "3) Batch learning algorithm<br>\n",
    "Perform gradient descent step over the whole training set.<br>\n",
    "<strong>Repeat until convergence : </strong>\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i $$\n",
    "\n",
    "4) Online learning algorithm<br>\n",
    "Perform gradient descent step over a single training example.<br>\n",
    "<strong>Repeat until convergence : </strong>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{For } \\,\\ i &= 1 \\,\\ \\text{ to } \\,\\  N :  \\\\\n",
    "& \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "All of these algorithms converge to the global optimal point!(convex and quadratic!)<br>\n",
    "Update depends on the error, small(or large) update when the error is small(or large).<br>\n",
    "It is called Widrow-Hoff(or LMS) learning rule.<br><br>\n",
    "\n",
    "In big learning late, these are unstable(zigzaging).<br>\n",
    "In small learning late, these converge slowly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.5. Learning algorithm : Least squares(One-shot learning approach)<br>\n",
    "Let's rewrite the cost function in a compact form as\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\\\\n",
    "              &= \\frac{1}{2} \n",
    "\\begin{bmatrix}\n",
    "y_1 - \\mathbf{x}_1^T \\mathbf{w} & y_2 - \\mathbf{x}_2^T \\mathbf{w} & \\cdots & y_N - \\mathbf{x}_N^T \\mathbf{w}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\mathbf{x}_1^T \\mathbf{w} \\\\\n",
    "y_2 - \\mathbf{x}_2^T \\mathbf{w} \\\\\n",
    " \\vdots \\\\\n",
    "y_N - \\mathbf{x}_N^T \\mathbf{w} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "             &= \\frac{1}{2} (\\mathbf{y} - X \\mathbf{w})^T (\\mathbf{y} - X \\mathbf{w}) \\\\\n",
    "             &= \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{y} = [y_1, \\,\\ \\cdots, \\,\\ y_N]_{N \\times 1}^T $\n",
    "- $ X = [\\mathbf{x}_1, \\,\\ \\cdots, \\,\\ \\mathbf{x}_N]_{N \\times m}^T $\n",
    "- $m$ : Order of model\n",
    "- $N$ : Number of training examples\n",
    "<br>\n",
    "\n",
    "Thus, this problem can be considered as the ordinary least squares problem : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2 \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<strong>Case 1 : Exact and unique solution when </strong> $ N = m = \\text{Rank}(X) $<br>\n",
    "$$ \\mathbf{w} = X^{-1} y $$\n",
    "When $N = m$ and $X$ is full-rank, the zero error can be achieved. Using a simple linear model and a set of training samples, it is able to perfectly estimate an unknown function. However, this ideal case of $N = m$ rarely occurs in practice(Not practical case).\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 2 : Over-determined case when </strong> $ N > m = \\text{Rank}(X) $<br>\n",
    "General case(Most practical case). But no unique solution to the equation $\\mathbf{y} = X\\mathbf{w}$.<br>\n",
    "Instead, try to minimize the error:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2 \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{y} - X \\mathbf{w})^T (\\mathbf{y} - X \\mathbf{w}) \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{y}^T X \\mathbf{w} + \\mathbf{w}^T X^T X \\mathbf{w}) \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} J(\\mathbf{w}) &= \\frac{1}{2} \\frac{\\partial}{\\partial \\mathbf{w}} ( \\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{y}^T X \\mathbf{w} + \\mathbf{w}^T X^T X \\mathbf{w} ) \\\\\n",
    "              &= X^T X \\mathbf{w} - X^T \\mathbf{y} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{w}} J(\\mathbf{w}) = \\mathbf{0} \\quad \\Longleftrightarrow \\quad X^T X \\mathbf{w} = X^T \\mathbf{y} \\quad \\text{(Normal equation)}$$\n",
    "LS solution : $\\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y}$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Case 3 : Under-determined case when </strong> $ m > N = \\text{Rank}(X) $<br>\n",
    "There are infinitely many solutions to the equation $\\mathbf{y} = X \\mathbf{w}$, each of which yields the zero error. Try to obtain a particular solution with the minimum norm :\n",
    "$$ \\min_{\\mathbf{w}} \\frac{1}{2} || \\mathbf{w} ||^2 \\qquad s.t. \\qquad \\mathbf{y} = X \\mathbf{w} $$\n",
    "\n",
    "Suppose $ \\mathcal{L}(\\mathbf{w}, \\mathbf{\\mu}) = \\frac{1}{2} || \\mathbf{w} ||^2 + \\mathbf{\\mu}^T (\\mathbf{y} - X\\mathbf{w}) $.\n",
    "$$\n",
    "\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}, \\mathbf{\\mu}) = \\mathbf{w} - X^T \\mathbf{\\mu}\n",
    "$$\n",
    "\n",
    "$$ \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}, \\mathbf{\\mu}) = \\mathbf{0} \\implies \\mathbf{w} = X^T \\mathbf{\\mu} $$\n",
    "\n",
    "$$ X \\mathbf{w} = XX^T \\mathbf{\\mu} = \\mathbf{y} \\implies \\mathbf{\\mu} = (XX^T)^{-1} \\mathbf{y} $$\n",
    "\n",
    "Therefore, LS solution is $ \\mathbf{w} = X^T (XX^T)^{-1} \\mathbf{y} $.\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 4 : Rank-deficient under-determined case and rank-deficient over-determined case when </strong> $ m > N > \\text{Rank}(X) $ and $ N > m > \\text{Rank}(X) $<br>\n",
    "There are infinitely many solutions satisfying the normal equation. Therefore, the LS solution is not unique. Try to obtain minimum norm least squares solution\n",
    "\n",
    "$$ \\mathbf{w} = X^+ \\mathbf{y} = V_1 \\Sigma_1^{-1} U_1^H \\mathbf{y} $$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.6. Recursive least squares<br>\n",
    "The idea of recursive least squares is that \"What if additional training examples are available?\". The RLS aims to solve the least squares problem recursively. That is, it finds the new regression model. \n",
    "\n",
    "$$ \n",
    "\\mathbf{w}(n) = \\arg\\min_{\\mathbf{w}} \n",
    "\\left|\\left|\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{y}(1) \\\\\n",
    "        \\mathbf{y}(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{y}(n) \\\\\n",
    "    \\end{bmatrix}\n",
    "    -\n",
    "    \\begin{bmatrix}\n",
    "        X(1) \\\\\n",
    "        X(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        X(n) \\\\\n",
    "    \\end{bmatrix} \\mathbf{w}\n",
    "\\right|\\right| ^2\n",
    "$$\n",
    "\n",
    "in terms of new training sample $ (X(n), \\,\\ \\mathbf{y}(n)) $ and previous model<br>\n",
    "\n",
    "$$ \n",
    "\\mathbf{w}(n-1) = \\arg\\min_{\\mathbf{w}} \n",
    "\\left| \\left|\n",
    "    \\begin{bmatrix}\n",
    "        \\mathbf{y}(1) \\\\\n",
    "        \\mathbf{y}(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{y}(n-1) \\\\\n",
    "    \\end{bmatrix}\n",
    "    -\n",
    "    \\begin{bmatrix}\n",
    "        X(1) \\\\\n",
    "        X(2) \\\\\n",
    "        \\vdots \\\\\n",
    "        X(n-1) \\\\\n",
    "    \\end{bmatrix} \\mathbf{w}\n",
    "\\right| \\right| ^2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose $ P(i) = \\begin{bmatrix} X(1) \\\\ \\vdots \\\\ X(i) \\end{bmatrix}^T \\begin{bmatrix} X(1) \\\\ \\vdots \\\\ X(i) \\end{bmatrix}. $<br>\n",
    "$ ^\\forall i \\in \\mathbb{N}, \\quad \\mathbf{w}(i) = \\mathbf{w}(i-1) - P(i) X^T(i) ( \\mathbf{y}(i) - X(i) \\mathbf{w}(i-1) ) $<br>\n",
    "$ \\qquad \\text{where} \\qquad P(i) = P(i-1) - P(i-1) X^T(i) (I + X(i)P(i-1)X^T(i))^{-1} X(i)P(i-1) $<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF file (Too long)](./res/ch03/note_recursive_linear_regression.pdf)  $ \\blacksquare $ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.7. Regularized least squares<br>\n",
    "Consider the following error function :<br>\n",
    "$$ J(\\mathbf{w}) = E(\\mathbf{w}) + \\lambda R(\\mathbf{w}) $$\n",
    "- $E$ is called error term, $R$ is called regularization term.\n",
    "- $\\lambda$ is called the regularization coefficient.\n",
    "\n",
    "In context, regularized least squares is\n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2}|| \\mathbf{y} - X \\mathbf{w} ||^2 + \\frac{\\lambda}{2} R(\\mathbf{w}) \\} $$\n",
    "\n",
    "- Regularization helps the model less overfit.\n",
    "- In general, the norm of the weight vector $\\mathbf{w}$ is usually regularized \n",
    "$$ R(\\mathbf{w}) = || \\mathbf{w} ||_{q}^{q} $$\n",
    "\n",
    "The cost function with a more general regularizer is \n",
    "$$ J(\\mathbf{w}) = \\frac{1}{2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}^2) + \\frac{\\lambda}{2} \\sum_{i = 1}^{N} |w_i|^q $$\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_4.png\" width=\"600\" height=\"200\"><br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  Figure.3.2.2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above picture, we can know that Lasso tends to generate sparser(most of $w_i$'s are nearly zero) solutions than a quadratic regularizer. We can consider above problem as optimization problem with constraint(Lagrange dual problem).\n",
    "<br><br>\n",
    "\n",
    "Quadratic RLS : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X\\mathbf{w} ||^2 + \\frac{\\lambda}{2} || \\mathbf{w} ||^2 \\} $$\n",
    "\n",
    "RLS solution : \n",
    "$$ \\mathbf{w} = (X^TX + \\lambda I )^{-1} X^T \\mathbf{y} $$\n",
    "\n",
    "- $ X^T X + \\lambda I $ is always invertible even if $X^T X$ is not invertible. (Trivial)\n",
    "- Regularized LS can be used regardless of $N$ and $m$.\n",
    "<br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "Trivial. $\\blacksquare$<br><br>\n",
    "\n",
    "$q$-norm RLS : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X\\mathbf{w} ||^2 + \\frac{\\lambda}{2} || \\mathbf{w} ||_q^q \\} $$\n",
    "\n",
    "RLS solution : \n",
    "$$ \\mathbf{w} = (X^TX + \\lambda I )^{-1} X^T \\mathbf{y} $$\n",
    "\n",
    "- The above optimization problem is convex.\n",
    "- It can be solved efficiently via the <strong>convex optimization techniques,</strong> e.g. interior point method at the computational complexity of $O((\\max\\{N, m\\})^3)$ : [Interior point method](https://en.wikipedia.org/wiki/Interior-point_method)\n",
    "- Also, the gradient descent based methods can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.8. Comparisons<br>\n",
    "Let $L$ is number of iteraions in gradient descent based method, and $N_{max} = \\max_i N_i, \\,\\ X(i) : N_i \\times m$\n",
    "\n",
    "| Batch gradient descent | Stochastic gradient descent |     Least squares     |  Recursive least squares  | \n",
    "|------------------------|-----------------------------|-----------------------|---------------------------|\n",
    "| $O(LNm)$               | $O(LN)$                     | $O((\\max\\{N, m\\})^3)$ |$O((\\max\\{N_{max}, m\\})^3)$|\n",
    "<br>\n",
    "\n",
    "- All the methods yields the same optimal performance\n",
    "- According to the situation, the complexity may be different\n",
    "- If the initial point can be chosen to be very close to the optimal point, then the gradient descent methods are the most efficient\n",
    "- Otherwise, the least squares may be more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.9. Linear regression with basis functions<br>\n",
    "$$ \\hat{y_i} = \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_i}) \\quad where \\quad \\mathbf{\\phi} = [\\phi_1, \\cdots, \\phi_m]^T $$\n",
    "- $\\mathbf{\\phi}(\\mathbf{x})$ is known as basis function.\n",
    "- Linear basis functions : $^\\forall i, \\,\\ \\phi_i(\\mathbf{x}) = x_i$\n",
    "- Polynomial basis functions : $ \\phi_i(x) = x^i $<br>\n",
    "    These are global functions(a small change in $x$ affect all basis functions)\n",
    "- Gaussian basis functions : $ \\phi_i(x) = \\exp \\{ - \\frac{(x- \\mu_i)^2}{2s^2} \\} $<br>\n",
    "    These are local(a small change in $x$ only affect nearby basis functions)\n",
    "- Sigmoid basis functions : $ \\phi_i(x) = \\sigma(\\frac{x-\\mu_i}{s}) $<br>\n",
    "    These are local. $\\mu_i$ and $s$ control location and scale(slope)\n",
    "\n",
    "<br><br>\n",
    "Cost function is set to be the sum of squares error \n",
    "$$ E(\\mathbf{w}) = \\frac{1}{2} \\sum_{i = 1}^{N} \\{ y_i - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_i) \\}^2 $$\n",
    "\n",
    "Solution : \n",
    "$$ \n",
    "\\mathbf{w}_{\\text{LS}} = \\Phi^+ \\mathbf{y} = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{y}\n",
    "\\qquad \\text{where} \\qquad \n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "\\phi_1(\\mathbf{x}_1) & \\phi_2(\\mathbf{x}_1) & \\cdots & \\phi_m(\\mathbf{x}_1) \\\\\n",
    "\\phi_1(\\mathbf{x}_2) & \\phi_2(\\mathbf{x}_2) & \\cdots & \\phi_m(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_1(\\mathbf{x}_N) & \\phi_2(\\mathbf{x}_N) & \\cdots & \\phi_m(\\mathbf{x}_N) \\\\\n",
    "\\end{bmatrix}_{N \\times m}\n",
    "$$\n",
    "- In parctice, $N > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.10. Proper step size<br>\n",
    "In least squares, \n",
    "$$ \\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y} $$\n",
    "\n",
    "Let $ \\frac{1}{N} X^T X = \\frac{1}{N} \\sum_{i = 1}^{N} \\mathbf{x}_i \\mathbf{x}_i^T = \\mathbb{E}[\\mathbf{x} \\mathbf{x}^T] \\triangleq R_x $ <br>\n",
    "and $ \\frac{1}{N} X^T \\mathbf{y} = \\frac{1}{N} \\sum_{i = 1}^{N} \\mathbf{x}_i y_i = \\mathbb{E}[\\mathbf{x} y] \\triangleq \\mathbf{r}_{xy}  $ <br>\n",
    "\n",
    "Thus, \n",
    "$$ \\mathbf{w}^* = (\\frac{1}{N} X^T X)^{-1} (\\frac{1}{N} X^T \\mathbf{y}) = R_x^{-1} \\mathbf{r}_{xy} $$\n",
    "\n",
    "The batch(resp. online) learning algorithm converges to $\\mathbf{w}^*$ in the sense that $N \\rightarrow \\infty$(resp. in the mean) if\n",
    "$$ 0 < \\alpha < \\frac{2}{\\max_{i} \\lambda_i} \\qquad \\text{where} \\,\\ \\lambda_i \\,\\ \\text{are eigenvalue of} \\,\\ R_x $$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF File (Too long)](./res/ch03/note_proper_step_size.pdf)  $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.11. Good training samples<br>\n",
    "So, what training samples are good for learning? We conduct analysis for a linearly regressive model : \n",
    "$$ \\mathbf{y} = X \\mathbf{w} + \\boldsymbol{\\epsilon}, \\quad \\text{where} \\,\\ \\boldsymbol{\\epsilon} \\,\\ \\text{is an error vector with} \\,\\ \\mathbb{E}[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2 I $$\n",
    "\n",
    "Since the LS solution into the MSE, we have\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\epsilon &= \\mathbb{E}[|| \\mathbf{w} - (X^T X)^{-1} X^T \\mathbf{y} ||^2] = \\mathbb{E}[|| (X^T X)^{-1} X^T \\boldsymbol{\\epsilon} ||^2] \\\\\n",
    "         &= \\mathbb{E}[Tr\\{ (X^T X)^{-1} X^T \\boldsymbol{\\epsilon} \\boldsymbol{\\epsilon}^T X (X^T X)^{-1} \\}] \\\\\n",
    "         &= \\sigma^2 Tr[(X^T X)^{-1} X^T X (X^T X)^{-1}] \\\\\n",
    "         &= \\sigma^2 Tr[(X^T X)^{-1}] \\quad (N \\ge m)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, the optimal training samples can be obtained by solving\n",
    "$$ \\min_X \\{ \\epsilon = \\sigma^2 Tr[(X^T X)^{-1}] \\} $$\n",
    "\n",
    "The MSE is minimized when "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{1}{N} X^T X = \\frac{1}{N} \\sum_{n = 1}^{N} \\mathbf{x}_n \\mathbf{x}_n^T \\approx \\mathbb[\\mathbf{x} \\mathbf{x}^T] = \\alpha I_m \\quad \\text{for some} \\,\\ \\alpha > 0\n",
    "$$\n",
    "\n",
    "- The optimal training samples(in terms of the MSE) should have independent features.\n",
    "- Also, the features should have the same statistics (or be identically distributed), i.e., an equal variance.\n",
    "- It must be $N \\ge m$ to ensure $X^T X \\propto I_m $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
