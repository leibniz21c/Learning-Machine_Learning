{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix.01 Review of Linear Algebra\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.1 Frobenius norm of the matrix\n",
    "$$ \\left \\| \\boldsymbol{\\mathbf{X}} \\right \\|_F = \\sqrt{Tr(X^TX)} = \\sqrt{Tr(XX^T)} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.2 Inner product of vectors\n",
    "Let vectors $\\mathbf{w}$ and $\\mathbf{x}$ be $n\\times 1$ vector.<br>\n",
    "$\\mathbf{w}^T\\mathbf{x}$ is called inner product of vectors.<br>\n",
    "\n",
    "$ \\mathbf{w}^T\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "w_1 & w_2 & \\cdots & w_n \n",
    "\\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} w_ix_i $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.3 Outter product of vectors\n",
    "Let vectors $\\mathbf{w}$ and $\\mathbf{x}$ be $n \\times 1$ vectors.<br>\n",
    "$\\mathbf{x}\\mathbf{w}^T$ is called outter product of vectors.<br>\n",
    "\n",
    "$ \\mathbf{x}\\mathbf{w}^T = $ $ \\begin{bmatrix} \n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix} = $ $ \\begin{bmatrix} \n",
    "x_1\\mathbf{w}^T \\\\\n",
    "x_2\\mathbf{w}^T \\\\\n",
    "\\vdots \\\\\n",
    "x_n\\mathbf{w}^T \\\\\n",
    "\\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    " x_1w_1 & x_1w_2 & \\cdots & x_1w_n \\\\\n",
    " x_2w_1 & x_2w_2 & \\cdots & x_2w_n \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " x_nw_1 & x_nw_2 & \\cdots & x_nw_n \\\\\n",
    " \\end{bmatrix} $ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.4 Matrix-vector multiplication\n",
    "Let $W \\,\\ : \\,\\ m \\times n$ and $\\mathbf{x} \\,\\ : \\,\\ n \\times 1$<br><br>\n",
    "\n",
    "(1)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix} \\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_n \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} \\mathbf{w}_i\\mathbf{x}_i $ <br><br>\n",
    "\n",
    "(2)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T \\\\\n",
    "\\bar{\\mathbf{w}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T \\\\\n",
    "\\end{bmatrix} $ $ \\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T\\mathbf{x} \\\\\n",
    "\\bar{\\mathbf{w}_2}^T\\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\mathbf{x} \\\\\n",
    "\\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\sum_{i=1}^{n} w_{1i}x_i \\\\\n",
    "\\sum_{i=1}^{n} w_{2i}x_i \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{n} w_{mi}x_i \\\\\n",
    "\\end{bmatrix} $ where $ \\bar{\\mathbf{w}_i} = $ (transpose of i-th row vector of $W$)<br><br>\n",
    "\n",
    "(3)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix} W_1 & W_2 & \\cdots & W_N \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\\n",
    "\\cdots \\\\\n",
    "\\mathbf{x}_N \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{N} W_i\\mathbf{x}_i $ where $ \\sum_{i=1}^{N} n_i = n $<br><br>\n",
    "\n",
    "(4)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{W_1}^T \\\\\n",
    "\\bar{W_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W_M}^T \\\\\n",
    "\\end{bmatrix} $ $ \\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{W_1}^T\\mathbf{x} \\\\\n",
    "\\bar{W_2}^T\\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W_M}^T\\mathbf{x} \\\\\n",
    "\\end{bmatrix} $ where $ \\sum_{i=1}^{M} m_i = m $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> < Note > </strong><br>\n",
    "(1) : W를 행벡터로 보는 시각 -> 내적의 확장<br>\n",
    "    , 각 bold(x) 인덱스의 w_i 만큼 스칼라배 한다는 관점.<br>\n",
    "(2) : W를 열벡터로 보는 시각 -> 외적의 확장<br>\n",
    "    , 이후 각 원소가 bold(x)만큼의 스칼라배 한다는 관점, 이후 내적으로 변환<br>\n",
    "(3) : 행렬을 서브메트릭스로 나누는 관점 + 벡터를 서브 벡터로 나누는 관점<br>\n",
    "(4) : 파티션하여 다시 벡터를 스칼라로 보고 스칼라배하는 관점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.5 Matrix-matrix multiplication\n",
    "Let $ W \\,\\ : \\,\\ m \\times n$ and $ X \\,\\ : \\,\\ n \\times l $.<br><br>\n",
    "\n",
    "(1)<br>\n",
    "$ WX = $ $ \\begin{bmatrix} \\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_n \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{x}_1}^T \\\\\n",
    "\\bar{\\mathbf{x}_2}^T \\\\\n",
    "\\cdots \\\\\n",
    "\\bar{\\mathbf{x}_n}^T \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} \\mathbf{w}_i\\bar{\\mathbf{x}_i}^T $ <br><br>\n",
    "\n",
    "(2)<br>\n",
    "$ WX = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T \\\\\n",
    "\\bar{\\mathbf{w}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_l \\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_1}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_1}^T\\mathbf{x}_l \\\\\n",
    "\\bar{\\mathbf{w}_2}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_2}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_2}^T\\mathbf{x}_l \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_m}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_m}^T\\mathbf{x}_l \\\\\n",
    "\\end{bmatrix} $<br><br>\n",
    "\n",
    "(3)<br>\n",
    "$ WX = $ $ \\begin{bmatrix} W_1 & W_2 & \\cdots & W_N \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\bar{X}_1 \\\\\n",
    "\\bar{X}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{X}_N \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{N} W_i\\bar{X_i} $ where $W_i \\,\\ : \\,\\ m \\times n_i \\,\\ , \\,\\ \\bar{X}_i \\,\\ : \\,\\ n_i \\times l \\,\\ , \\,\\ n = \\sum_{i=1}^{N} n_i $ <br><br>\n",
    "\n",
    "(4)<br>\n",
    "$ WX = $ $ \\begin{bmatrix}\n",
    "\\bar{W}_1^T \\\\\n",
    "\\bar{W}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W}_M^T \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} X_1 & X_2 & \\cdots & X_L \\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\bar{W}_1^TX_1 & \\bar{W}_1^TX_2 & \\cdots & \\bar{W}_1^TX_L \\\\\n",
    "\\bar{W}_2^TX_1 & \\bar{W}_2^TX_2 & \\cdots & \\bar{W}_2^TX_L \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bar{W}_M^TX_1 & \\bar{W}_M^TX_2 & \\cdots & \\bar{W}_M^TX_L \\\\\n",
    "\\end{bmatrix} $ where $ \\bar{W}_i \\,\\ : \\,\\ m_i \\times n \\,\\ , \\,\\ m = \\sum_{i=1}^{M} m_i \\,\\ , \\,\\ X_i \\,\\ : \\,\\ n \\times l_i \\,\\ , \\,\\ l = \\sum_{i=1}^{l} l_i $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Note > \n",
    "(1) : 메트릭스를 벡터로 보는 관점 그냥 내적의 일반화\n",
    "(4) : -> Linear Regression Least Square or PCA\n",
    "\n",
    "    ==> 이떄까지는 1차연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.6 Quadratic Forms\n",
    "In vectors-matrix, let $ w \\,\\ : \\,\\ n \\times 1 \\,\\ , \\,\\ R \\,\\ : \\,\\ n \\times n $<br>\n",
    "\n",
    "$ \\begin{matrix}\n",
    "\\mathbf{w}^TR\\mathbf{w} &=& \\mathbf{w}^T \\sum_{j=1}^{n} \\mathbf{r}_j \\mathbf{w}_j \\\\\n",
    "                        &=& \\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix} \\begin{bmatrix} \\sum_{j=1}^{n} r_{1j}w_j \\\\\n",
    "                        \\sum_{j=1}^{n} r_{2j}w_j \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\sum_{j=1}^{n} r_{nj}w_j \\\\ \\end{bmatrix} \\\\\n",
    "                        &=& \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_ir_{ij}w_j\n",
    "\\end{matrix} $ <br><br>\n",
    "\n",
    "In matrix-matrix, <br>\n",
    "$ W^TRW = $ $ \\sum_{i=1}^{n} \\sum_{j=1}^{n} W_iR_{ij}W_j $ where $ W = \\begin{bmatrix}\n",
    "W_1 \\\\\n",
    "W_2 \\\\\n",
    "\\vdots \\\\\n",
    "W_n \\\\\n",
    "\\end{bmatrix} $ , $ R = \\begin{bmatrix}\n",
    "R_{11} & R_{12} & \\cdots & R_{1n} \\\\\n",
    "R_{21} & R_{22} & \\cdots & R_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "R_{n1} & R_{n2} & \\cdots & R_{nn} \\\\\n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.7 Unitary(Orthogonal) matrix\n",
    "A $ n \\times n $ matrix $Q$ is called unitary(orthogonal) matrix if <br>\n",
    "$$ Q^TQ = QQ^T = I \\,\\ where \\,\\ Q \\,\\ : \\,\\ n \\times n $$<br><br>\n",
    "\n",
    "$(i)$<br>\n",
    "Let $ Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $<br>\n",
    "$ \\begin{bmatrix}\n",
    "\\mathbf{q}_1^T \\\\\n",
    "\\mathbf{q}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{q}_n^T \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $ $ = \\begin{bmatrix}\n",
    "\\mathbf{q}_1^T\\mathbf{q}_1 & \\mathbf{q}_1^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_1^T\\mathbf{q}_n \\\\\n",
    "\\mathbf{q}_2^T\\mathbf{q}_1 & \\mathbf{q}_2^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_2^T\\mathbf{q}_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{q}_n^T\\mathbf{q}_1 & \\mathbf{q}_n^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_n^T\\mathbf{q}_n \\\\\n",
    "\\end{bmatrix}$ ,which is \n",
    "$ \\begin{cases}\n",
    "1, & j = i  \\\\\n",
    "0, & j \\neq i \\\\\n",
    "\\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.8 Eigenvalues and Eigenvectors\n",
    "$ \\begin{matrix} \n",
    "\\text{If} \\,\\  \\exists \\,\\ \\lambda_i \\in \\mathbb{R} \\,\\ s.t. \\,\\ R\\mathbf{q} = \\lambda \\mathbf{q} \\,\\ &\\Leftrightarrow&  \\,\\ (R - \\lambda I)\\mathbf{q} = \\mathbf{0} \\\\\n",
    "&\\Leftrightarrow& \\,\\ P(\\lambda) = det(R-\\lambda I) = 0\n",
    "\\end{matrix} $ <br>\n",
    "$ \\,\\ where \\,\\ R \\,\\ : \\,\\ n \\times n $<br><br>\n",
    "$ \\lambda $ is called eigenvalue and $ \\mathbf{q} $ is called eigenvector.<br>\n",
    "Eigenvectors are often normalized such that <br>\n",
    "$$ \\left \\| \\mathbf{q}_i \\right \\| = 1, \\,\\ i = 1,2,\\cdots,n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.9 Symmetric Matrix\n",
    "$$ R^T = R \\,\\ where \\,\\ R \\,\\ : \\,\\ n \\times n $$\n",
    ",which is called symmetric matrix.<br><br>\n",
    "\n",
    "$(i)$ The eigenvalues of a symmetric matrix are real-valued, not complex-valued.\n",
    "$$ \\lambda_i \\in \\mathbb{R} \\,\\ for \\,\\ i = 1,2,\\cdots,n $$\n",
    "$(ii)$ The eigenvectors of a symmetric matrix are orthonormal.\n",
    "$$\n",
    "\\mathbf{q}_i^T\\mathbf{q}_j = \\begin{cases}\n",
    "1, & \\,\\ \\mbox{if } \\,\\ i = j \\\\\n",
    "0, & \\,\\ \\mbox{if } \\,\\ i \\neq j \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.10 Positive definite matrix and positive semi-definite matrix\n",
    "A symmetric $n \\times n$ real matrix $R$ is said to be positive definite if the scalar $\\mathbf{z}^TR\\mathbf{z}$ is positive for every non-zero column vector $\\mathbf{z}$ of n real numbers. It can be written\n",
    "$$ R \\succ \\mathbf{0} $$\n",
    "<br>\n",
    "A symmetric $n \\times n$ real matrix $R$ is said to be semi-positive definite if the scalar $\\mathbf{z}^TR\\mathbf{z}$ isn't negative for every non-zero column vector $\\mathbf{z}$ of n real numbers. It can be written\n",
    "$$ R \\succeq \\mathbf{0} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem.A.1.1 Eigenvalue Decomposition(EVD) or Spectral Theoren\n",
    "Any symmetric matrix $ R $ can be decomposed as <br><br>\n",
    "$$ R = Q \\Lambda Q^T = \\sum_{i = 1}^{n} \\lambda_i \\mathbf{q}_i \\mathbf{q}_i^T \\,\\ $$<br>\n",
    "$ where \\,\\ Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} \\mbox{ is unitary matrix of eigenvectors and } $ $ \\Lambda = $ $ \\begin{bmatrix} \n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\lambda_n \\\\\n",
    "\\end{bmatrix} $ \n",
    "\n",
    "<strong>Proof)</strong>\n",
    "$$ R\\mathbf{q}_i = \\lambda_i \\mathbf{q}_i, \\,\\ i = 1,2, \\cdots, n $$\n",
    "$$ \\mbox{Let } Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $$\n",
    "$$\n",
    "\\begin{matrix}\n",
    "RQ &=& R \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} \\\\\n",
    "   &=& \\begin{bmatrix} \\lambda_1 \\mathbf{q}_1 & \\lambda_2 \\mathbf{q}_2 & \\cdots \\ \\lambda_n \\mathbf{q}_n \\end{bmatrix} \\\\\n",
    "   &=& \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n) \\\\\n",
    "   &=& Q \\Lambda \\,\\ \\,\\ \\mbox{where} \\,\\ \\,\\ \\Lambda = diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n)\n",
    "\\end{matrix}\n",
    "$$\n",
    "$$ R = Q \\Lambda Q^{-1} = Q \\Lambda Q^{T} \\,\\ \\,\\ (\\because \\,\\ Q^{-1} = Q^T ) \\,\\ \\,\\ \\,\\ \\blacksquare $$ "
   ]
  },
  {
   "source": [
    "기존 방식 역행렬 n*n -> O(n^3), 이거쓰면 O(n^2)까지 가능."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.1.2 Interpretation of EVD\n",
    "\n",
    "<strong>Proof)</strong>\n",
    "\n",
    "$ \\blacksquare $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.11 Trace\n",
    "\n",
    "\n",
    "\n",
    "<strong>Proof)</strong>\n",
    "\n",
    "$ \\blacksquare $\n"
   ]
  },
  {
   "source": [
    "#### Definition.A.1.12 Gradient of a scalar function with respect to a vector\n",
    "Let the function $ f \\,\\ : \\,\\ \\mathbb{R}^m \\rightarrow \\mathbb{R} $ be.<br>\n",
    "$ \\frac{\\partial f(\\mathbf{w})}{\\partial \\mathbf{w}} = \\bigtriangledown _\\mathbf{w} f(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\mathbf{w})}{\\partial w_1} \\\\\n",
    "\\frac{\\partial f(\\mathbf{w})}{\\partial w_2} \\\\\n",
    "\\cdots \\\\\n",
    "\\frac{\\partial f(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\end{bmatrix} $ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.13 Gradient of a vector function with respect to a vector\n",
    "Let the function $ \\mathbf{g} \\,\\ : \\,\\ \\mathbb{R}^m \\rightarrow \\mathbb{R}^n $ be.<br>\n",
    "If $ \\mathbf{g}(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "g_1(\\mathbf{w}) \\\\\n",
    "g_2(\\mathbf{w}) \\\\\n",
    "\\vdots \\\\\n",
    "g_n(\\mathbf{w}) \\\\\n",
    "\\end{bmatrix} $ . and $ w \\,\\ : \\,\\ m \\times 1 $,<br>\n",
    "\n",
    "$ \\frac{\\partial \\mathbf{g}(\\mathbf{w})}{\\partial \\mathbf{w}} = \\bigtriangledown _\\mathbf{w} \\mathbf{g}(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial g_1(\\mathbf{w})}{\\partial w_1} & \\frac{\\partial g_1(\\mathbf{w})}{\\partial w_2} & \\cdots & \\frac{\\partial g_1(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\frac{\\partial g_2(\\mathbf{w})}{\\partial w_1} & \\frac{\\partial g_2(\\mathbf{w})}{\\partial w_2} & \\cdots & \\frac{\\partial g_2(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial g_m(\\mathbf{w})}{\\partial w_1} & \\frac{\\partial g_m(\\mathbf{w})}{\\partial w_2} & \\cdots & \\frac{\\partial g_m(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\end{bmatrix} $ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.14 Hessian matrix of a scalar function with respect to a vector\n",
    "Let the function $ f \\,\\ : \\,\\ \\mathbb{R}^m \\rightarrow \\mathbb{R} $ and $ \\mathbf{w} \\,\\ : \\,\\ m \\times 1 $ be.<br>\n",
    "$ \\mathbf{H} = \\frac{\\partial }{\\partial \\mathbf{w}}\\bigtriangledown _\\mathbf{w}^2f(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_1^2} & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_1\\partial w_2} & \\cdots & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_1\\partial w_m} \\\\ \n",
    "\\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_2\\partial w_1} & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_2^2} & \\cdots & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_2\\partial w_m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_m\\partial w_1} & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_m\\partial w_2} & \\cdots & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_m^2} \\\\\n",
    "\\end{bmatrix} \n",
    "$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.15 Gradient of a scalar function with respect to a matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.1.3 Geometric solution of distance between vector and hyperplane\n",
    "\n",
    "<strong>Proof)</strong>\n",
    "\n",
    "$ \\blacksquare $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reference :</strong><br>\n",
    "Deep Learning - Yosha Benjio<br>\n",
    "https://proofwiki.org<br>\n",
    "https://wikipedia.org<br>\n",
    "https://ko.wikipedia.org/wiki/위키백과:TeX_문법<br>"
   ]
  }
 ]
}