{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix.01 Linear Algebra\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.1 Frobenius norm of the matrix\n",
    "$$ \\left \\| \\boldsymbol{\\mathbf{X}} \\right \\|_F = \\sqrt{Tr(X^TX)} = \\sqrt{Tr(XX^T)} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.2 Inner product of vectors\n",
    "Let vectors $\\mathbf{w}$ and $\\mathbf{x}$ be $n\\times 1$ vector.<br>\n",
    "$\\mathbf{w}^T\\mathbf{x}$ is called inner product of vectors.<br>\n",
    "\n",
    "$ \\mathbf{w}^T\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "w_1 & w_2 & \\cdots & w_n \n",
    "\\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} w_ix_i $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.3 Outter product of vectors\n",
    "Let vectors $\\mathbf{w}$ and $\\mathbf{x}$ be $n \\times 1$ vectors.<br>\n",
    "$\\mathbf{x}\\mathbf{w}^T$ is called outter product of vectors.<br>\n",
    "\n",
    "$ \\mathbf{x}\\mathbf{w}^T = $ $ \\begin{bmatrix} \n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix} = $ $ \\begin{bmatrix} \n",
    "x_1\\mathbf{w}^T \\\\\n",
    "x_2\\mathbf{w}^T \\\\\n",
    "\\vdots \\\\\n",
    "x_n\\mathbf{w}^T \\\\\n",
    "\\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    " x_1w_1 & x_1w_2 & \\cdots & x_1w_n \\\\\n",
    " x_2w_1 & x_2w_2 & \\cdots & x_2w_n \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " x_nw_1 & x_nw_2 & \\cdots & x_nw_n \\\\\n",
    " \\end{bmatrix} $ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.4 Matrix-vector multiplication\n",
    "Let $W \\,\\ : \\,\\ m \\times n$ and $\\mathbf{x} \\,\\ : \\,\\ n \\times 1$<br><br>\n",
    "\n",
    "(1)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix} \\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_n \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} \\mathbf{w}_i\\mathbf{x}_i $ <br><br>\n",
    "\n",
    "(2)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T \\\\\n",
    "\\bar{\\mathbf{w}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T \\\\\n",
    "\\end{bmatrix} $ $ \\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T\\mathbf{x} \\\\\n",
    "\\bar{\\mathbf{w}_2}^T\\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\mathbf{x} \\\\\n",
    "\\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\sum_{i=1}^{n} w_{1i}x_i \\\\\n",
    "\\sum_{i=1}^{n} w_{2i}x_i \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{n} w_{mi}x_i \\\\\n",
    "\\end{bmatrix} $ where $ \\bar{\\mathbf{w}_i} = $ (transpose of i-th row vector of $W$)<br><br>\n",
    "\n",
    "(3)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix} W_1 & W_2 & \\cdots & W_N \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\\n",
    "\\cdots \\\\\n",
    "\\mathbf{x}_N \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{N} W_i\\mathbf{x}_i $ where $ \\sum_{i=1}^{N} n_i = n $<br><br>\n",
    "\n",
    "(4)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{W_1}^T \\\\\n",
    "\\bar{W_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W_M}^T \\\\\n",
    "\\end{bmatrix} $ $ \\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{W_1}^T\\mathbf{x} \\\\\n",
    "\\bar{W_2}^T\\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W_M}^T\\mathbf{x} \\\\\n",
    "\\end{bmatrix} $ where $ \\sum_{i=1}^{M} m_i = m $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.5 Matrix-matrix multiplication\n",
    "Let $ W \\,\\ : \\,\\ m \\times n$ and $ X \\,\\ : \\,\\ n \\times l $.<br><br>\n",
    "\n",
    "(1)<br>\n",
    "$ WX = $ $ \\begin{bmatrix} \\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_n \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{x}_1}^T \\\\\n",
    "\\bar{\\mathbf{x}_2}^T \\\\\n",
    "\\cdots \\\\\n",
    "\\bar{\\mathbf{x}_n}^T \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} \\mathbf{w}_i\\bar{\\mathbf{x}_i}^T $ <br><br>\n",
    "\n",
    "(2)<br>\n",
    "$ WX = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T \\\\\n",
    "\\bar{\\mathbf{w}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_l \\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_1}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_1}^T\\mathbf{x}_l \\\\\n",
    "\\bar{\\mathbf{w}_2}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_2}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_2}^T\\mathbf{x}_l \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_m}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_m}^T\\mathbf{x}_l \\\\\n",
    "\\end{bmatrix} $<br><br>\n",
    "\n",
    "(3)<br>\n",
    "$ WX = $ $ \\begin{bmatrix} W_1 & W_2 & \\cdots & W_N \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\bar{X}_1 \\\\\n",
    "\\bar{X}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{X}_N \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{N} W_i\\bar{X_i} $ where $W_i \\,\\ : \\,\\ m \\times n_i \\,\\ , \\,\\ \\bar{X}_i \\,\\ : \\,\\ n_i \\times l \\,\\ , \\,\\ n = \\sum_{i=1}^{N} n_i $ <br><br>\n",
    "\n",
    "(4)<br>\n",
    "$ WX = $ $ \\begin{bmatrix}\n",
    "\\bar{W}_1^T \\\\\n",
    "\\bar{W}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W}_M^T \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} X_1 & X_2 & \\cdots & X_L \\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\bar{W}_1^TX_1 & \\bar{W}_1^TX_2 & \\cdots & \\bar{W}_1^TX_L \\\\\n",
    "\\bar{W}_2^TX_1 & \\bar{W}_2^TX_2 & \\cdots & \\bar{W}_2^TX_L \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bar{W}_M^TX_1 & \\bar{W}_M^TX_2 & \\cdots & \\bar{W}_M^TX_L \\\\\n",
    "\\end{bmatrix} $ where $ \\bar{W}_i \\,\\ : \\,\\ m_i \\times n \\,\\ , \\,\\ m = \\sum_{i=1}^{M} m_i \\,\\ , \\,\\ X_i \\,\\ : \\,\\ n \\times l_i \\,\\ , \\,\\ l = \\sum_{i=1}^{l} l_i $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.6 Quadratic Forms\n",
    "In vectors-matrix, let $ w \\,\\ : \\,\\ n \\times 1 \\,\\ , \\,\\ R \\,\\ : \\,\\ n \\times n $<br>\n",
    "\n",
    "$ \\begin{matrix}\n",
    "\\mathbf{w}^TR\\mathbf{w} &=& \\mathbf{w}^T \\sum_{j=1}^{n} \\mathbf{r}_j \\mathbf{w}_j \\\\\n",
    "                        &=& \\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix} \\begin{bmatrix} \\sum_{j=1}^{n} r_{1j}w_j \\\\\n",
    "                        \\sum_{j=1}^{n} r_{2j}w_j \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\sum_{j=1}^{n} r_{nj}w_j \\\\ \\end{bmatrix} \\\\\n",
    "                        &=& \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_ir_{ij}w_j\n",
    "\\end{matrix} $ <br><br>\n",
    "\n",
    "In matrix-matrix, <br>\n",
    "$ W^TRW = $ $ \\sum_{i=1}^{n} \\sum_{j=1}^{n} W_iR_{ij}W_j $ where $ W = \\begin{bmatrix}\n",
    "W_1 \\\\\n",
    "W_2 \\\\\n",
    "\\vdots \\\\\n",
    "W_n \\\\\n",
    "\\end{bmatrix} $ , $ R = \\begin{bmatrix}\n",
    "R_{11} & R_{12} & \\cdots & R_{1n} \\\\\n",
    "R_{21} & R_{22} & \\cdots & R_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "R_{n1} & R_{n2} & \\cdots & R_{nn} \\\\\n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.7 Unitary(Orthogonal) matrix\n",
    "A $ n \\times n $ matrix $Q$ is called unitary(orthogonal) matrix if <br>\n",
    "$$ Q^TQ = QQ^T = I \\,\\ where \\,\\ Q \\,\\ : \\,\\ n \\times n $$<br><br>\n",
    "\n",
    "$(i)$<br>\n",
    "Let $ Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $<br>\n",
    "$ \\begin{bmatrix}\n",
    "\\mathbf{q}_1^T \\\\\n",
    "\\mathbf{q}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{q}_n^T \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $ $ = \\begin{bmatrix}\n",
    "\\mathbf{q}_1^T\\mathbf{q}_1 & \\mathbf{q}_1^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_1^T\\mathbf{q}_n \\\\\n",
    "\\mathbf{q}_2^T\\mathbf{q}_1 & \\mathbf{q}_2^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_2^T\\mathbf{q}_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{q}_n^T\\mathbf{q}_1 & \\mathbf{q}_n^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_n^T\\mathbf{q}_n \\\\\n",
    "\\end{bmatrix}$ ,which is \n",
    "$ \\begin{cases}\n",
    "1, & j = i  \\\\\n",
    "0, & j \\neq i \\\\\n",
    "\\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.8 Eigenvalues and Eigenvectors\n",
    "$ \\begin{matrix} \n",
    "\\text{If} \\,\\  \\exists \\,\\ \\lambda_i \\in \\mathbb{R} \\,\\ s.t. \\,\\ R\\mathbf{q} = \\lambda \\mathbf{q} \\,\\ &\\Leftrightarrow&  \\,\\ (R - \\lambda I)\\mathbf{q} = \\mathbf{0} \\\\\n",
    "&\\Leftrightarrow& \\,\\ P(\\lambda) = det(R-\\lambda I) = 0\n",
    "\\end{matrix} $ <br>\n",
    "$ \\,\\ where \\,\\ R \\,\\ : \\,\\ n \\times n $<br><br>\n",
    "$ \\lambda $ is called eigenvalue and $ \\mathbf{q} $ is called eigenvector.<br>\n",
    "Eigenvectors are often normalized such that <br>\n",
    "$$ \\left \\| \\mathbf{q}_i \\right \\| = 1, \\,\\ i = 1,2,\\cdots,n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.9 Symmetric Matrix\n",
    "$$ R^T = R \\,\\ where \\,\\ R \\,\\ : \\,\\ n \\times n $$\n",
    ",which is called symmetric matrix.<br><br>\n",
    "\n",
    "$(i)$ The eigenvalues of a symmetric matrix are real-valued, not complex-valued.\n",
    "$$ \\lambda_i \\in \\mathbb{R} \\,\\ for \\,\\ i = 1,2,\\cdots,n $$\n",
    "$(ii)$ The eigenvectors of a symmetric matrix are orthonormal.\n",
    "$$\n",
    "\\mathbf{q}_i^T\\mathbf{q}_j = \\begin{cases}\n",
    "1, & \\,\\ \\mbox{if } \\,\\ i = j \\\\\n",
    "0, & \\,\\ \\mbox{if } \\,\\ i \\neq j \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.10 Positive definite matrix and positive semi-definite matrix\n",
    "A symmetric $n \\times n$ real matrix $R$ is said to be positive definite if the scalar $\\mathbf{z}^TR\\mathbf{z}$ is positive for every non-zero column vector $\\mathbf{z}$ of n real numbers. It can be written\n",
    "$$ R \\succ \\mathbf{0} $$\n",
    "<br>\n",
    "A symmetric $n \\times n$ real matrix $R$ is said to be semi-positive definite if the scalar $\\mathbf{z}^TR\\mathbf{z}$ isn't negative for every non-zero column vector $\\mathbf{z}$ of n real numbers. It can be written\n",
    "$$ R \\succeq \\mathbf{0} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem.A.1.1 Eigenvalue Decomposition(EVD) and singular value decomposition(SVD)\n",
    "Any symmetric matrix $ R $ can be decomposed as <br><br>\n",
    "$$ R = Q \\Lambda Q^T = \\sum_{i = 1}^{n} \\lambda_i \\mathbf{q}_i \\mathbf{q}_i^T $$ <br>\n",
    "$ where \\,\\ Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} \\mbox{ is unitary matrix of eigenvectors and } $ $ \\Lambda = $ $ \\begin{bmatrix} \n",
    "\\lambda_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\lambda_n \\\\\n",
    "\\end{bmatrix} $ \n",
    "\n",
    "<strong>Proof.</strong>\n",
    "$$ R\\mathbf{q}_i = \\lambda_i \\mathbf{q}_i, \\,\\ i = 1,2, \\cdots, n $$\n",
    "$$ \\mbox{Let } Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $$\n",
    "$$\n",
    "\\begin{align*}\n",
    "RQ &= R \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} \\\\\n",
    "   &= \\begin{bmatrix} \\lambda_1 \\mathbf{q}_1 & \\lambda_2 \\mathbf{q}_2 & \\cdots \\ \\lambda_n \\mathbf{q}_n \\end{bmatrix} \\\\\n",
    "   &= \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n) \\\\\n",
    "   &= Q \\Lambda \\,\\ \\,\\ \\mbox{where} \\,\\ \\,\\ \\Lambda = diag(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "$$ R = Q \\Lambda Q^{-1} = Q \\Lambda Q^{T} \\,\\ \\,\\ (\\because \\,\\ Q^{-1} = Q^T ) \\,\\ \\,\\ \\,\\ \\blacksquare $$ \n",
    "\n",
    "Singular value decomposition(SVD)\n",
    "Any square matrix $ A $ can be decomposed as <br><br>\n",
    "$$ A = V \\Sigma U^T $$ <br>\n",
    "$ \\text{where} \\,\\ V \\,\\ \\text{is an unitary matrix of eigenvectors of} \\,\\ A^TA $ <br>\n",
    "$ \\,\\ \\text{,} \\,\\ U^T \\,\\ \\text{is an unitary matrix of eigenvectors of} \\,\\ AA^T $."
   ]
  },
  {
   "source": [
    "#### Theorem.A.1.2 Interpretation of EVD\n",
    "Let $\\mathbf{x} \\,\\ : \\,\\ n \\times 1$ and $R \\,\\ : \\,\\ n \\times n$.<br>\n",
    "$ \\mathbf{x}^T R \\mathbf{x} = 1 \\,\\ for \\,\\ A \\succ 0 $ <br>\n",
    "that is an ellipse in n-dimensional space.<br><br>\n",
    "\n",
    "Axes : eigenvectors $\\{ \\mathbf{q}_i\\}_{i=1}^{n}$<br>\n",
    "Half-Length of each axis : $\\frac{1}{\\sqrt{\\lambda_i}}$<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$\\mathbf{x}^TA\\mathbf{x} = \\mathbf{x}^T V \\Lambda V^T \\mathbf{x} = 1 $<br>\n",
    "$ \\mathbf{y}^T \\Lambda \\mathbf{y} = \\sum_{j=1}^{n} \\lambda_j y_j^2 = 1 \\,\\ where \\,\\ \\mathbf{y} = V^T \\mathbf{x} $<br>\n",
    "$ \\sum_{j=1}^{n} \\lambda_j y_j^2 $ is a equation for the ellipse in n-dimension. $ \\blacksquare $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1.11 Trace\n",
    "Let $A$ be a matrix.\n",
    "$$ Tr(A) = \\sum_{i=1}^{n} a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn} $$\n",
    "<br>\n",
    "$ (i) \\,\\ Tr(c) = c \\,\\ for \\,\\ some \\,\\ scalar \\,\\ c$<br>\n",
    "$ (ii) \\,\\ Tr(AB) = Tr(BA) $<br>\n",
    "$ (iii) \\,\\ Tr(\\mathbf{xx}^T) = Tr({\\mathbf{x^Tx}}) = \\mathbf{x^Tx} = \\left \\| x \\right \\|^2 \\,\\ for \\,\\ some \\,\\ vector \\,\\ \\mathbf{x} $<br>\n",
    "$ (vi) \\,\\ Tr(A) = \\sum_{i=1}^{n} a_{ii} = \\sum_{i=1}^{n} \\lambda_i \\,\\ where \\,\\ A \\,\\ : \\,\\ n \\times n $<br>\n",
    "\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$ (i) \\,\\ Definition $<br>\n",
    "$ (ii) \\,\\ Trivial $ $ \\blacksquare $<br>\n",
    "$ (iii) \\,\\ Trivial $ $ \\blacksquare $<br>\n",
    "$ \\begin{align*}\n",
    "(vi) \\,\\ Tr(A) &= Tr(Q \\Lambda Q^T) \\\\\n",
    "      &= Tr(Q^T Q \\Lambda) \\,\\ (\\,\\ \\because \\,\\ (ii) \\,\\ ) \\\\\n",
    "      &= Tr(\\Lambda) \\,\\ (\\,\\ \\because \\,\\ Q \\,\\ is \\,\\ unitary \\,\\ ) \\\\\n",
    "      &= \\sum_{i=1}^{n} \\lambda_i \\,\\ \\,\\ \\blacksquare \n",
    "\\end{align*} $ \n"
   ]
  },
  {
   "source": [
    "#### Definition.A.1.12 Gradient of a scalar function with respect to a vector\n",
    "Let the function $ f \\,\\ : \\,\\ \\mathbb{R}^m \\rightarrow \\mathbb{R} $ be.<br>\n",
    "$ \\frac{\\partial f(\\mathbf{w})}{\\partial \\mathbf{w}} = \\bigtriangledown _\\mathbf{w} f(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\mathbf{w})}{\\partial w_1} \\\\\n",
    "\\frac{\\partial f(\\mathbf{w})}{\\partial w_2} \\\\\n",
    "\\cdots \\\\\n",
    "\\frac{\\partial f(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\end{bmatrix} $ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.13 Gradient of a vector function with respect to a vector\n",
    "Let the function $ \\mathbf{g} \\,\\ : \\,\\ \\mathbb{R}^m \\rightarrow \\mathbb{R}^n $ be.<br>\n",
    "If $ \\mathbf{g}(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "g_1(\\mathbf{w}) \\\\\n",
    "g_2(\\mathbf{w}) \\\\\n",
    "\\vdots \\\\\n",
    "g_n(\\mathbf{w}) \\\\\n",
    "\\end{bmatrix} $ . and $ w \\,\\ : \\,\\ m \\times 1 $,<br>\n",
    "\n",
    "$ \\frac{\\partial \\mathbf{g}(\\mathbf{w})}{\\partial \\mathbf{w}} = \\bigtriangledown _\\mathbf{w} \\mathbf{g}(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial g_1(\\mathbf{w})}{\\partial w_1} & \\frac{\\partial g_1(\\mathbf{w})}{\\partial w_2} & \\cdots & \\frac{\\partial g_1(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\frac{\\partial g_2(\\mathbf{w})}{\\partial w_1} & \\frac{\\partial g_2(\\mathbf{w})}{\\partial w_2} & \\cdots & \\frac{\\partial g_2(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial g_m(\\mathbf{w})}{\\partial w_1} & \\frac{\\partial g_m(\\mathbf{w})}{\\partial w_2} & \\cdots & \\frac{\\partial g_m(\\mathbf{w})}{\\partial w_m} \\\\\n",
    "\\end{bmatrix} $ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.14 Hessian matrix of a scalar function with respect to a vector\n",
    "Let the function $ f \\,\\ : \\,\\ \\mathbb{R}^m \\rightarrow \\mathbb{R} $ and $ \\mathbf{w} \\,\\ : \\,\\ m \\times 1 $ be.<br>\n",
    "$ \\mathbf{H} = \\frac{\\partial }{\\partial \\mathbf{w}}\\bigtriangledown _\\mathbf{w}^2f(\\mathbf{w}) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_1^2} & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_1\\partial w_2} & \\cdots & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_1\\partial w_m} \\\\ \n",
    "\\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_2\\partial w_1} & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_2^2} & \\cdots & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_2\\partial w_m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_m\\partial w_1} & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_m\\partial w_2} & \\cdots & \\frac{\\partial ^2f(\\mathbf{w})}{\\partial w_m^2} \\\\\n",
    "\\end{bmatrix} \n",
    "$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.15 Gradient of a scalar function with respect to a matrix\n",
    "Let the function $ f \\,\\ : \\,\\ \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R} $ be a function.<br>\n",
    "\n",
    "$ \\frac{\\partial f(W)}{\\partial W} = \\bigtriangledown_W f(W) = $ $ \\begin{bmatrix}\n",
    "\\frac{\\partial f(W)}{\\partial w_{11}} & \\frac{\\partial f(W)}{\\partial w_{12}} & \\cdots & \\frac{\\partial f(W)}{\\partial w_{1n}} \\\\\n",
    "\\frac{\\partial f(W)}{\\partial w_{21}} & \\frac{\\partial f(W)}{\\partial w_{22}} & \\cdots & \\frac{\\partial f(W)}{\\partial w_{2n}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f(W)}{\\partial w_{m1}} & \\frac{\\partial f(W)}{\\partial w_{m2}} & \\cdots & \\frac{\\partial f(W)}{\\partial w_{mn}} \\\\\n",
    "\\end{bmatrix} $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.1.3 Geometric solution of distance between vector and hyperplane\n",
    "Let a vector $\\mathbf{x}_0$ and a hyperplane $\\mathbf{w}^T \\mathbf{x} + b = 0$ be.\n",
    "The distance between a vector $\\mathbf{x}_0$ and a hyperplane $\\mathbf{w}^T \\mathbf{x} + b = 0$ $r$ is\n",
    "$$ r = \\frac{\\mathbf{w}^T \\mathbf{x}_0 + b}{ \\left \\| \\mathbf{w} \\right \\| } \\,\\ where \\,\\ r \\lessgtr 0 \\,\\ if \\,\\ \\mathbf{w}^T \\mathbf{x}_0 + b \\lessgtr 0. $$\n",
    "<strong>Proof.</strong><br>\n",
    "Let $z \\,\\ s.t. \\,\\ \\mathbf{w}^T \\mathbf{z} + b = 0 $ (i.e. a vector on the hyperplane.)<br>\n",
    "$$ r = (\\mathbf{x}_0 - \\mathbf{z})^T \\frac{ \\mathbf{w} }{\\left \\| \\mathbf{w} \\right \\|} = \\frac{\\mathbf{w}^T \\mathbf{x}_0 - \\mathbf{w}^T \\mathbf{z}}{\\left \\| \\mathbf{w} \\right \\|} = \\frac{\\mathbf{w}^T \\mathbf{x}_0 + b}{ \\left \\| \\mathbf{w} \\right \\| } \\,\\ ( \\,\\  \\because \\,\\ \\mathbf{w}^T \\mathbf{z} = -b \\,\\ ) \\,\\ \\blacksquare $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Definition.A.1.16 Moore-penrose pseudoinverse matrix\n",
    "Let $A$ be $m \\times n$ matrix.<br>\n",
    "\n",
    "$$ A^+ = \\lim_{\\alpha \\rightarrow 0} ( A^T A + \\alpha I )^{-1} A^T $$\n",
    "$A^+$ is called psedoinverse matrix of $A$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Theorem.A.1.4 Singular value decomposition with pseduinverse matrix\n",
    "Suppose $A$ be $ m \\times n $ matrix.\n",
    "$$ A^+ = V D^+ U^T $$\n",
    "Columns of $U_{m \\times m}$ are left-singular vector of $A$. (Eigenvectors of $AA^T$)<br>\n",
    "Columns of $V_{n \\times n}$ are right-singular vector of $A$. (Eigenvectors of $A^TA$)<br>\n",
    "<br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^+ &= \\lim_{\\alpha \\rightarrow 0} (A^T A + \\alpha I)^{-1}A^T \\\\\n",
    "    &= \\lim_{\\alpha \\rightarrow 0} (V \\Sigma^T U^T U \\Sigma V^T + \\alpha I)^{-1} V \\Sigma^T U^T \\quad (\\because \\,\\ \\text{SVD of} \\,\\ A) \\\\\n",
    "    &= \\lim_{\\alpha \\rightarrow 0} (V \\Sigma^T \\Sigma V^T + \\alpha I )^{-1} V \\Sigma^T U^T \\\\\n",
    "    &= (V^T)^{-1}(\\Sigma^T \\Sigma)^{-1} V^{-1} V \\Sigma^T U^T \\\\\n",
    "    &= V (\\Sigma^T \\Sigma)^{-1} \\Sigma^T U^T \\\\\n",
    "    &= V \\lim_{\\alpha \\rightarrow 0} (\\Sigma^T \\Sigma + \\alpha I)^{-1} \\Sigma^T U^T \\\\\n",
    "    &= V \\Sigma^+ U^T \\qquad \\blacksquare\n",
    "\\end{align*}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Lemma.A.1.1 Woodbury formula\n",
    "Let $A \\in \\mathbb{R}^{n \\times n}$ and $C \\in \\mathbb{R}^{k \\times k}$ are invertible and $U, V \\in \\mathbb{R}^{n \\times k}$. <br>\n",
    "$A + UCV^T$ is invertible, if and only if, $C^{-1} + V^T A^{-1} U$ is invertible.<br>\n",
    "In addition, \n",
    "$$ (A + UCV^T)^{-1} = A^{-1} - A^{-1}U(C^{-1} + V^TA^{-1}U)^{-1}V^TA^{-1} $$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "$ \\det\\begin{pmatrix} A & U \\\\ V^T & -C^{-1} \\end{pmatrix} \\neq 0 \\quad (\\because \\,\\ A \\,\\ \\text{and} \\,\\ -C^{-1}) $ <br>\n",
    "Therefore, there is an unique solution about following equation.<br>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "A & U \\\\\n",
    "V^T & -C^{-1} \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "X \\\\\n",
    "Y \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "I \\\\\n",
    "O \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\quad\n",
    "\n",
    "\\text{where} \\,\\ X \\in \\mathbb{R}^{n \\times n}, \\,\\ Y \\in \\mathbb{R}^{k \\times n}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "AX + UY = I \\quad \\cdots \\,\\ (1) \\\\\n",
    "V^TX - C^{-1}Y = O \\quad \\cdots \\,\\ (2) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "$$ Y = CV^T X \\quad (\\because \\,\\ (2)) \\quad \\cdots \\,\\ (3) $$\n",
    "\n",
    "$$ AX + UCV^T X = (A + UCV^T )X = I \\quad (\\because \\,\\ (1), \\,\\ (3)) $$\n",
    "\n",
    "$$ (A + UCV^T )^{-1} = X \\quad (\\because \\,\\ \\text{definition of inverse matrix}) $$\n",
    "\n",
    "$$ X = A^{-1}(I - UY) \\quad (\\because \\,\\ (1)) \\quad \\cdots \\,\\ (4) $$\n",
    "\n",
    "$$ V^T A^{-1} (I - UY) - C^{-1} Y = 0 \\quad (\\because \\,\\ (2), \\,\\ (4)) $$\n",
    "\n",
    "$$ Y = (C^{-1} + V^T A^{-1} U)^{-1} V^T A^{-1} \\quad \\cdots \\,\\ (5) $$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X &= A^{-1}(I - U(C^{-1} + V^TA^{-1}U)^{-1} V^T A^{-1}) \\quad (\\because \\,\\ (4), \\,\\ (5)) \\\\\n",
    "  &= A^{-1} - A^{-1} U (C^{-1} + V^TA^{-1}U)^{-1} V^T A^{-1} \\qquad \\blacksquare\n",
    "\\end{align*}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reference :</strong><br>\n",
    "Deep Learning - Yosha Benjio<br>\n",
    "대학수학2 - 박찬녕<br>\n",
    "https://proofwiki.org<br>\n",
    "https://wikipedia.org<br>\n",
    "https://ko.wikipedia.org/wiki/위키백과:TeX_문법<br>"
   ]
  }
 ]
}