{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix. Review of Linear Algebra\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.1 Frobenius norm of the matrix\n",
    "$$ \\left \\| \\boldsymbol{\\mathbf{X}} \\right \\|_F = \\sqrt{Tr(X^TX)} = \\sqrt{Tr(XX^T)} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.2 Inner product of vectors\n",
    "Let vectors $\\mathbf{w}$ and $\\mathbf{x}$ be $n\\times 1$ vector.<br>\n",
    "$\\mathbf{w}^T\\mathbf{x}$ is called inner product of vectors.<br>\n",
    "\n",
    "$ \\mathbf{w}^T\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "w_1 & w_2 & \\cdots & w_n \n",
    "\\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} w_ix_i $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.3 Outter product of vectors\n",
    "Let vectors $\\mathbf{w}$ and $\\mathbf{x}$ be $n \\times 1$ vectors.<br>\n",
    "$\\mathbf{x}\\mathbf{w}^T$ is called outter product of vectors.<br>\n",
    "\n",
    "$ \\mathbf{x}\\mathbf{w}^T = $ $ \\begin{bmatrix} \n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix} = $ $ \\begin{bmatrix} \n",
    "x_1\\mathbf{w}^T \\\\\n",
    "x_2\\mathbf{w}^T \\\\\n",
    "\\vdots \\\\\n",
    "x_n\\mathbf{w}^T \\\\\n",
    "\\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    " x_1w_1 & x_1w_2 & \\cdots & x_1w_n \\\\\n",
    " x_2w_1 & x_2w_2 & \\cdots & x_2w_n \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " x_nw_1 & x_nw_2 & \\cdots & x_nw_n \\\\\n",
    " \\end{bmatrix} $ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.4 Matrix-vector multiplication\n",
    "Let $W \\,\\ : \\,\\ m \\times n$ and $\\mathbf{x} \\,\\ : \\,\\ n \\times 1$<br><br>\n",
    "\n",
    "(1)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix} \\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_n \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_n \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} \\mathbf{w}_i\\mathbf{x}_i $ <br><br>\n",
    "\n",
    "(2)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T \\\\\n",
    "\\bar{\\mathbf{w}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T \\\\\n",
    "\\end{bmatrix} $ $ \\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T\\mathbf{x} \\\\\n",
    "\\bar{\\mathbf{w}_2}^T\\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\mathbf{x} \\\\\n",
    "\\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\sum_{i=1}^{n} w_{1i}x_i \\\\\n",
    "\\sum_{i=1}^{n} w_{2i}x_i \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{n} w_{mi}x_i \\\\\n",
    "\\end{bmatrix} $ where $ \\bar{\\mathbf{w}_i} = $ (transpose of i-th row vector of $W$)<br><br>\n",
    "\n",
    "(3)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix} W_1 & W_2 & \\cdots & W_N \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\mathbf{x}_1 \\\\\n",
    "\\mathbf{x}_2 \\\\\n",
    "\\cdots \\\\\n",
    "\\mathbf{x}_N \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{N} W_i\\mathbf{x}_i $ where $ \\sum_{i=1}^{N} n_i = n $<br><br>\n",
    "\n",
    "(4)<br>\n",
    "$ W\\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{W_1}^T \\\\\n",
    "\\bar{W_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W_M}^T \\\\\n",
    "\\end{bmatrix} $ $ \\mathbf{x} = $ $ \\begin{bmatrix}\n",
    "\\bar{W_1}^T\\mathbf{x} \\\\\n",
    "\\bar{W_2}^T\\mathbf{x} \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W_M}^T\\mathbf{x} \\\\\n",
    "\\end{bmatrix} $ where $ \\sum_{i=1}^{M} m_i = m $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> < Note > </strong><br>\n",
    "(1) : W를 행벡터로 보는 시각 -> 내적의 확장<br>\n",
    "    , 각 bold(x) 인덱스의 w_i 만큼 스칼라배 한다는 관점.<br>\n",
    "(2) : W를 열벡터로 보는 시각 -> 외적의 확장<br>\n",
    "    , 이후 각 원소가 bold(x)만큼의 스칼라배 한다는 관점, 이후 내적으로 변환<br>\n",
    "(3) : 행렬을 서브메트릭스로 나누는 관점 + 벡터를 서브 벡터로 나누는 관점<br>\n",
    "(4) : 파티션하여 다시 벡터를 스칼라로 보고 스칼라배하는 관점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.5 Matrix-matrix multiplication\n",
    "Let $ W \\,\\ : \\,\\ m \\times n$ and $ X \\,\\ : \\,\\ n \\times l $.<br><br>\n",
    "\n",
    "(1)<br>\n",
    "$ WX = $ $ \\begin{bmatrix} \\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_n \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{x}_1}^T \\\\\n",
    "\\bar{\\mathbf{x}_2}^T \\\\\n",
    "\\cdots \\\\\n",
    "\\bar{\\mathbf{x}_n}^T \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{n} \\mathbf{w}_i\\bar{\\mathbf{x}_i}^T $ <br><br>\n",
    "\n",
    "(2)<br>\n",
    "$ WX = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T \\\\\n",
    "\\bar{\\mathbf{w}_2}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_l \\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\bar{\\mathbf{w}_1}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_1}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_1}^T\\mathbf{x}_l \\\\\n",
    "\\bar{\\mathbf{w}_2}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_2}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_2}^T\\mathbf{x}_l \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bar{\\mathbf{w}_m}^T\\mathbf{x}_1 & \\bar{\\mathbf{w}_m}^T\\mathbf{x}_2 & \\cdots & \\bar{\\mathbf{w}_m}^T\\mathbf{x}_l \\\\\n",
    "\\end{bmatrix} $<br><br>\n",
    "\n",
    "(3)<br>\n",
    "$ WX = $ $ \\begin{bmatrix} W_1 & W_2 & \\cdots & W_N \\end{bmatrix} $ $ \\begin{bmatrix}\n",
    "\\bar{X}_1 \\\\\n",
    "\\bar{X}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{X}_N \\\\\n",
    "\\end{bmatrix} = $ $ \\sum_{i=1}^{N} W_i\\bar{X_i} $ where $W_i \\,\\ : \\,\\ m \\times n_i \\,\\ , \\,\\ \\bar{X}_i \\,\\ : \\,\\ n_i \\times l \\,\\ , \\,\\ n = \\sum_{i=1}^{N} n_i $ <br><br>\n",
    "\n",
    "(4)<br>\n",
    "$ WX = $ $ \\begin{bmatrix}\n",
    "\\bar{W}_1^T \\\\\n",
    "\\bar{W}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\bar{W}_M^T \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} X_1 & X_2 & \\cdots & X_L \\end{bmatrix} = $ $ \\begin{bmatrix}\n",
    "\\bar{W}_1^TX_1 & \\bar{W}_1^TX_2 & \\cdots & \\bar{W}_1^TX_L \\\\\n",
    "\\bar{W}_2^TX_1 & \\bar{W}_2^TX_2 & \\cdots & \\bar{W}_2^TX_L \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bar{W}_M^TX_1 & \\bar{W}_M^TX_2 & \\cdots & \\bar{W}_M^TX_L \\\\\n",
    "\\end{bmatrix} $ where $ \\bar{W}_i \\,\\ : \\,\\ m_i \\times n \\,\\ , \\,\\ m = \\sum_{i=1}^{M} m_i \\,\\ , \\,\\ X_i \\,\\ : \\,\\ n \\times l_i \\,\\ , \\,\\ l = \\sum_{i=1}^{l} l_i $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Note > \n",
    "(1) : 메트릭스를 벡터로 보는 관점 그냥 내적의 일반화\n",
    "(4) : -> Linear Regression Least Square or PCA\n",
    "\n",
    "    ==> 이떄까지는 1차연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.6 Quadratic Forms\n",
    "In vectors-matrix, let $ w \\,\\ : \\,\\ n \\times 1 \\,\\ , \\,\\ R \\,\\ : \\,\\ n \\times n $<br>\n",
    "\n",
    "$ \\begin{matrix}\n",
    "\\mathbf{w}^TR\\mathbf{w} &=& \\mathbf{w}^T \\sum_{j=1}^{n} \\mathbf{r}_j \\mathbf{w}_j \\\\\n",
    "                        &=& \\begin{bmatrix} w_1 & w_2 & \\cdots & w_n \\end{bmatrix} \\begin{bmatrix} \\sum_{j=1}^{n} r_{1j}w_j \\\\\n",
    "                        \\sum_{j=1}^{n} r_{2j}w_j \\\\\n",
    "                        \\vdots \\\\\n",
    "                        \\sum_{j=1}^{n} r_{nj}w_j \\\\ \\end{bmatrix} \\\\\n",
    "                        &=& \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_ir_{ij}w_j\n",
    "\\end{matrix} $ <br><br>\n",
    "\n",
    "In matrix-matrix, <br>\n",
    "$ W^TRW = $ $ \\sum_{i=1}^{n} \\sum_{j=1}^{n} W_iR_{ij}W_j $ where $ W = \\begin{bmatrix}\n",
    "W_1 \\\\\n",
    "W_2 \\\\\n",
    "\\vdots \\\\\n",
    "W_n \\\\\n",
    "\\end{bmatrix} $ , $ R = \\begin{bmatrix}\n",
    "R_{11} & R_{12} & \\cdots & R_{1n} \\\\\n",
    "R_{21} & R_{22} & \\cdots & R_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "R_{n1} & R_{n2} & \\cdots & R_{nn} \\\\\n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.7 Unitary(Orthogonal) matrix\n",
    "A $ n \\times n $ matrix $Q$ is called unitary(orthogonal) matrix if <br>\n",
    "$$ Q^TQ = QQ^T = I \\,\\ where \\,\\ Q \\,\\ : \\,\\ n \\times n $$<br><br>\n",
    "\n",
    "$(i)$<br>\n",
    "Let $ Q = \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $<br>\n",
    "$ \\begin{bmatrix}\n",
    "\\mathbf{q}_1^T \\\\\n",
    "\\mathbf{q}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{q}_n^T \\\\\n",
    "\\end{bmatrix} $ $ \\begin{bmatrix} \\mathbf{q}_1 & \\mathbf{q}_2 & \\cdots & \\mathbf{q}_n \\end{bmatrix} $ $ = \\begin{bmatrix}\n",
    "\\mathbf{q}_1^T\\mathbf{q}_1 & \\mathbf{q}_1^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_1^T\\mathbf{q}_n \\\\\n",
    "\\mathbf{q}_2^T\\mathbf{q}_1 & \\mathbf{q}_2^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_2^T\\mathbf{q}_n \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{q}_n^T\\mathbf{q}_1 & \\mathbf{q}_n^T\\mathbf{q}_2 & \\cdots & \\mathbf{q}_n^T\\mathbf{q}_n \\\\\n",
    "\\end{bmatrix}$ ,which is \n",
    "$ \\begin{cases}\n",
    "1, & j = i  \\\\\n",
    "0, & j \\neq i \\\\\n",
    "\\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.8 Eigenvalues and Eigenvectors\n",
    "$  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.9 Symmetric Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.10 Positive semi-definite matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem.A.1 Eigenvalue Decomposition(EVD) or Spectral Theoren\n",
    "\n",
    "<strong>Proof)</strong>\n",
    "\n",
    "$ \\blacksquare $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem.A.2 Interpretation of EVD\n",
    "\n",
    "<strong>Proof)</strong>\n",
    "\n",
    "$ \\blacksquare $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition.A.11 Trace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reference :</strong><br>\n",
    "Deep Learning - Yosha Benjio<br>\n",
    "https://proofwiki.org<br>\n",
    "https://wikipedia.org<br>\n",
    "https://ko.wikipedia.org/wiki/위키백과:TeX_문법<br>"
   ]
  }
 ]
}