{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historic-exclusive",
   "metadata": {},
   "source": [
    "# Chapter.04 Statistical Learning\n",
    "---\n",
    "All of the regression analysis techniques seen in CH03 depend on the quality and quantity of data. If there is no data and only statistical information, then is it possible to learn? For this, consider applying the Linear Regression technique.<br><br>\n",
    "\n",
    "As we know, I'll use linearly regressive and approximated models in CH03.2.1 with Expectational error; $ \\mathbb{E}[\\epsilon] = 0, \\,\\ \\mathbb{E}[\\epsilon^2] = \\sigma^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-danger",
   "metadata": {},
   "source": [
    "### 4.1. Wiener filter(Optimal linear MMSE filter)\n",
    "4.1.1. Overview<br>\n",
    "Let's consider the linear regression problem\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}} \\left\\{ J(\\mathbf{w}) = \\frac{1}{2}|| \\mathbf{y} - X\\mathbf{w} ||^2 \\right\\} \\quad \n",
    "\\text{where} \\,\\ \\mathbf{w} = [b, w_1, w_2, \\cdots]^T, \\,\\ \\mathbf{x} = [1, x_1, x_2, \\cdots]^T, \\,\\ \\mathbf{y} = [y_1, \\cdots, y_N]^T, \\,\\ X = [\\mathbf{x}_1, \\cdots, \\,\\ \\mathbf{x}_N]^T\n",
    "$$\n",
    "\n",
    "The LS solution of above problem is \n",
    "$$ \\mathbf{w} = (X^TX)^{-1}X^T \\mathbf{y} $$\n",
    "Therefore, we can see that the solution depends on the training data $(x, y)$.<br>\n",
    "Let's see what happens if the number of training samples approaches infinity (i.e., $N \\rightarrow \\infty$)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w} &= (\\frac{1}{N}X^TX)^{-1}(\\frac{1}{N}X^T\\mathbf{y}) \\\\\n",
    "           &\\rightarrow \\mathbf{w} = R^{-1}_x \\mathbf{r}_{xy} \\\\\n",
    "\\end{align*}   \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{where} \\,\\ &\\frac{1}{N} X^TX = \\frac{1}{N} \\sum_{i = 1}^{N} \\mathbf{x}_i \\mathbf{x}_i^T = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^T] = R_x \\quad \\text{(covariance of input)}, \\\\\n",
    "                 &\\frac{1}{N}X^T\\mathbf{y} = \\frac{1}{N} \\sum_{i = 1}^{N} \\mathbf{x}_iy_i = \\mathbb{E}[\\mathbf{x}y] = \\mathbf{r}_{xy} \\quad \\text{(cross correlation)} \\\\\n",
    "\\end{align*}            \n",
    "$$\n",
    "\n",
    "Now, the solution depends only on the second-order statistics of the training data $ (x, y) $, not the training data itself. If there are so many data so that we cannot learn the system, we can use __Wiener filter__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-certification",
   "metadata": {},
   "source": [
    "4.1.2. Optimal linaer filtering problem<br>\n",
    "This is good for __audio data__. The limiting form of the linear regression problem with infinitely many training samples; \n",
    "\n",
    "$$ \\min_{\\mathbf{w}} \\left\\{ J(\\mathbf{w}) = \\frac{1}{2} \\mathbb{E}\\left[ (y - \\mathbf{w}^T \\mathbf{x})^2 \\right] \\right\\} $$\n",
    "\n",
    ", that is convex and quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-shopper",
   "metadata": {},
   "source": [
    "4.1.3. Wiener filter(Limiting form of the LS solution)<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w}_{WF} &= \\arg\\min_{\\mathbf{w}} \\mathbb{E}[(y - \\mathbf{w}^T \\mathbf{x})^2] \\\\\n",
    "                &= R_{x}^{-1} \\mathbf{r}_{xy} \\quad \\text{(optimal linear filtering in the MMSE sense)} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF File too long](./res/ch04/Ch04.1.3.pdf)  $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-knowing",
   "metadata": {},
   "source": [
    "### 4.2. Steepest Gradient Descent Method and Least Mean Square Algorithm\n",
    "4.2.1. Gradient descent algorithm<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} \\quad  \\text{in linear regression} \\,\\ \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= \\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{2} \\mathbb{E} \\left[(y - \\mathbf{w}^T \\mathbf{x})^2\\right] \\\\\n",
    "                                                   &= \\mathbb{E} \\left[ (\\mathbf{w}^T \\mathbf{x} - y) \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{x} - y)\\right] \\,\\ (\\because \\,\\ \\text{chain rule})\\\\\n",
    "                                                   &= \\mathbb{E} \\left[ (\\mathbf{w}^T \\mathbf{x} - y) \\mathbf{x} \\right] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Above term is case that $ N \\rightarrow \\infty $ of $ \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i)\\mathbf{x}_i $. It is iteratively updating the parameters according to __error__ $ (y - \\mathbf{w}^T \\mathbf{x}) $, that is called steepest gradient descent method. <br><br>\n",
    "\n",
    "Wiener filter have an advantage that we don't need to get many data because we can use statistical information. However, how can we get the statistical information like mean, variance and so on?<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-being",
   "metadata": {},
   "source": [
    "4.2.2. Two approaches for gradient descent<br>\n",
    "- Steepest Gradient Descent : perform the steepest gradient descent step until convergence\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Repeat} &\\,\\ \\text{until convergence:} \\\\\n",
    "              &\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\mathbb{E} \\left[ (y - \\mathbf{w}^T \\mathbf{x})\\mathbf{x}\\right] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Least Mean Square (LMS) Algorithm(SGD) : approximate the expectation by the one-point sample mean\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Repeat} &\\,\\ \\text{until convergence:} \\\\\n",
    "              &\\text{For} \\,\\ i = 1, 2, \\cdots, \\\\ \n",
    "              &\\quad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha (y - \\mathbf{w}^T \\mathbf{x}_i) \\mathbf{x}_i \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "First, we have to think about computational efficiency.<br>\n",
    "Steepest gradient descent requires the expectation, which is in general unknown. Therefore, computation is very heavy and inefficient.<br>\n",
    "Stochastic gradient descent(LMS Algorithm) requires no statistical knowledge. Therefore, computation is very efficient and simple.<br><br>\n",
    "\n",
    "Second, we have to confirm about convergence of both algorithms.<br>\n",
    "<strong>Proof.</strong><br>\n",
    "[PDF File too long](./res/ch04/Ch04.2.2.pdf) $ \\blacksquare $ <br><br>\n",
    "For two algorithms, the convergence is guaranteed if the step size satisfies the following condition:\n",
    "$$ 0 < \\alpha < \\frac{2}{\\lambda_{\\text{max}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-rochester",
   "metadata": {},
   "source": [
    "<img src=\"./res/ch04/fig_2_2.png\" width=\"600\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.4.2.2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-possession",
   "metadata": {},
   "source": [
    "### 4.3. Minimum Mean Square Error(MMSE) Estimator\n",
    "The minimum mean square error(MMSE) estimator is\n",
    "$$\n",
    "\\hat{f}_{\\text{MMSE}}(\\mathbf{x}) = \\arg\\min_f \\mathbb{E} \\left[ \\frac{1}{2}(y - f(\\mathbf{x}))^2 \\right] = \\mathbb{E}[y | \\mathbf{x}]\n",
    "$$\n",
    "- Conditional mean estimator is optimal in the MMSE sense.\n",
    "- No training data is needed; only the statistics (i.e., conditional mean) is required to know.\n",
    "- In baysian rule context, if $ \\mathbf{x} $ and $ y $ are jointly gaussian, then that is $ \\mathbf{w}_{\\text{MAP}} $\n",
    "\n",
    "So, if we want to accurate regression analysis, we just have to get conditional mean, that is very difficult so that we usually use NN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-caution",
   "metadata": {},
   "source": [
    "### 4.4. Review\n",
    "If we want to use just statistical knowledge, it is better to assume the gaussian distribution except special circumstances. Just make sure we can use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-diana",
   "metadata": {},
   "source": [
    "<strong>Reference.</strong><br>\n",
    "https://www.google.com/search?client=safari&rls=en&q=wiener+filter&ie=UTF-8&oe=UTF-8<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
