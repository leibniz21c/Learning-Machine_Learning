{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "buried-thesaurus",
   "metadata": {},
   "source": [
    "# Chapter.05 Classification\n",
    "---\n",
    "### 5.3. Probabilistic Approaches for Classification\n",
    "5.3.1. Statistics vs Bayesian Classification\n",
    "\n",
    "- Statistical classification\n",
    "    - Based on the Neyman-Pearson criterion\n",
    "    - Typically used in sonar and rader systems (with unknown priors)\n",
    "    - ML estimator\n",
    "- Bayesian Classification\n",
    "    - Based on minimization of the Bayes risk(by cost function)\n",
    "    - Typically used in communications and pattern recognition systems\n",
    "    - MAP estimator (suppose we know prior e.g., gaussian)\n",
    "\n",
    "<br>\n",
    "Both are based on the Likelihood Ratio Test (LRT), just comparing the ratio of likelihoods, but to different thresholds\n",
    "\n",
    "$$\n",
    "L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\xi\n",
    "$$\n",
    "\n",
    "$ L(\\mathbf{x}) $ is a likelihood ratio and $ \\xi $ is a decision threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-economy",
   "metadata": {},
   "source": [
    "5.3.2. Probabilities in classification<br>\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_1.png\" width=\"800\" height=\"600\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.1\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\text{mistake}) &= p(\\mathbf{x} \\in \\mathcal{R}_1, \\mathcal{C}_2) + p(\\mathbf{x} \\in \\mathcal{R}_2, \\mathcal{C}_1) \\\\\n",
    "                  &= \\int_{\\mathcal{R}_1} p(\\mathbf{x}, C_1) d\\mathbf{x} + \\int_{\\mathcal{R}_2} p(\\mathbf{x}, C_2) d\\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(correct) &= \\int_{\\mathcal{R}_1} p(\\mathbf{x}, C_1) d\\mathbf{x} + \\int_{\\mathcal{R}_2} p(\\mathbf{x}, C_2) d\\mathbf{x} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-noise",
   "metadata": {},
   "source": [
    "5.3.3. A simple binary classification\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_2.png\" width=\"500\" height=\"400\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.2\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\text{Type-1 error : } \\int_{\\mathcal{R}_2} p(x | C_1) dx = \\int_{\\frac{1}{2}}^{\\infty} = Q(0.5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Type-2 error : } \\int_{\\mathcal{R}_1} p(x | C_2) dx = \\int_{-\\infty}^{\\frac{1}{2}} = Q(0.5)\n",
    "$$\n",
    "\n",
    "What if the threshold changes?\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_3.png\" width=\"550\" height=\"430\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.3\n",
    "</div>\n",
    "\n",
    "It isn't possible to reduce both error probabilities at the same time. So there is a criterion suggested by Neyman-Pearson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-ghost",
   "metadata": {},
   "source": [
    "5.3.4. Statistical classification : Neyman-Pearson criterion<br>\n",
    "To degign the optimal(binary) classifier, one possible choice is to minimize the __Type-Ⅱ error__(false negative), or equivalently, to maximize the __Power of test__ by constraining __Type-Ⅰ error__(false positive) below a threshold $ \\alpha $:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathcal{R}_2} \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_2) d\\mathbf{x} \\quad s.t. \\quad \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_1) d\\mathbf{x} \\le \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-lyric",
   "metadata": {},
   "source": [
    "<strong>Theorem.5.3.4.1. Neyman-Pearson theorem</strong><br>\n",
    "The solution to \n",
    "\n",
    "$$\n",
    "\\max_{\\mathcal{R}_2} \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_2) d\\mathbf{x} \\quad s.t. \\quad \\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_1) d\\mathbf{x} \\le \\alpha\n",
    "$$\n",
    "\n",
    "is given by \n",
    "\n",
    "$$\n",
    "R_2^* = \\left\\{ \\mathbf{x} : L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} > \\gamma \\right\\} \\quad \\text{where the threshold} \\,\\ \\gamma \\,\\ \\text{is found such that} \\,\\ \\int_{\\mathcal{R_2}} p(\\mathbf{x} | C_1) d\\mathbf{x} = \\alpha \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-laugh",
   "metadata": {},
   "source": [
    "Neyman-Pearson(Binary) Classification Rule : \n",
    "- Decide a decision threshold $ \\gamma $ from \n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal{R}_2} p(\\mathbf{x} | C_1) d\\mathbf{x} = \\alpha\n",
    "$$\n",
    "\n",
    "- Perform classification according to the likelihood ratio test : \n",
    "\n",
    "$$\n",
    "L(\\mathbf{x}) = \\frac{p(\\mathbf{x} | C_2)}{p(\\mathbf{x} | C_1)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\gamma\n",
    "$$\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "It can be proved by constraint optimization problem. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-roman",
   "metadata": {},
   "source": [
    "Let's solve the problem in 5.3.3.\n",
    "\n",
    "$$\n",
    "Pr\\{\\text{False positive(Type 1 error)}\\} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{p(x|C_2)}{p(x|C_1)} &= \\frac{\\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2}(x - 1)^2)}{\\frac{1}{\\sqrt{2 \\pi}} \\exp(- \\frac{1}{2} x^2)} \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\gamma \\\\\n",
    "                          &= \\exp\\left(- \\frac{1}{2} (x^2 - 2x + 1 - x^2) \\right) = \\exp \\left( x - \\frac{1}{2} \\right) \\overset{C_2}{\\underset{C_1}{\\gtrless}} \\gamma \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr\\{\\text{False positive(Type 1 error)}\\} &= Pr\\left\\{ \\exp \\left( x - \\frac{1}{2} \\right) > \\gamma | C_1 \\right\\} \\\\\n",
    "                                          &= Pr\\left\\{ x - \\frac{1}{2} > \\ln \\gamma | C_1 \\right\\} \\\\\n",
    "                                          &= \\int_{\\ln \\gamma + \\frac{1}{2}}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{1}{2} x^2) dx = 0.5 \\quad \\Rightarrow \\quad \\ln \\gamma + \\frac{1}{2} = 0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr\\{\\text{True positive}\\} &= Pr\\left\\{ x > 0 \\right\\} \\\\\n",
    "                           &= \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp[- \\frac{1}{2}(x - 1)^2] dx = 0.84 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So are there some limitations? Although an NP criterion can be formulated for multiple classes, it seems to seldom be used in practice. More commonly the minimum $P_e$ criterion or its generalization, the Bayes risk criterion, is employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-bookmark",
   "metadata": {},
   "source": [
    "5.3.4. Receiver Operating Characteristics (ROC)<br> \n",
    "ROC curve : $Pr\\{\\text{True positive}\\} \\,\\ \\text{vs} \\,\\ Pr\\{\\text{False positive}\\} $\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_4.png\" width=\"550\" height=\"430\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.4\n",
    "</div>\n",
    "\n",
    "This is alternative way of summarizing the performance of a classifier and very useful to compare different classifiers and to decide which one performs best.\n",
    "\n",
    "<img src=\"./res/ch05/fig_3_5.png\" width=\"650\" height=\"530\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.5.3.5\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-syndicate",
   "metadata": {},
   "source": [
    "5.3.5. Bayesian classification : Minimum Bayes Risk Classifier in two classes<br> \n",
    "Bayes risk $ \\mathcal{R} $ is given like following\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R} &= c_{11}Pr(\\text{True Positive}) + c_{22}Pr(\\text{True Negative}) + c_{21}Pr(\\text{False Negative}) + c_{12}Pr(\\text{False Positive}) \\\\\n",
    "            &= c_{11} \\pi_1 \\int_{R_1} p(\\mathbf{x} | C_1) d\\mathbf{x} + c_{22} \\pi_2 \\int_{R_2} p(\\mathbf{x} | C_2) d\\mathbf{x} + c_{21} \\pi_1 \\int_{R_2} p(\\mathbf{x} | C_1) d\\mathbf{x} + c_{12} \\pi_2 \\int_{R_1} p(\\mathbf{x} | C_2) d\\mathbf{x} \\\\\n",
    "            &\\quad \\text{where} \\,\\ \\pi_i = P(C_i), \\,\\ i = 1, 2, \\,\\ R_i \\,\\ : \\,\\ \\text{decision region in which} \\,\\ \\mathbf{x} \\in C_i , \\,\\ c_{ij} \\,\\ : \\,\\ \\text{cost if we choose} \\,\\ C_i \\,\\ \\text{but} \\,\\ C_j \\,\\ \\text{is true} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-kuwait",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Since} \\,\\ \\int_{R_1} p(\\mathbf{x} | C_i) d\\mathbf{x} + \\int_{R_2} p(\\mathbf{x} | C_i) d\\mathbf{x} = 1, \\,\\ i = 1, 2, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{We have} \\,\\ \\mathcal{R} = &c_{11} \\pi_1 \\int_{R_1} p(\\mathbf{x} | C_1) d\\mathbf{x} + c_{22} \\pi_2 \\left( 1 - \\int_{R_1} p(\\mathbf{x} | C_2) d\\mathbf{x} \\right) \\\\\n",
    "                                 &+ c_{21} \\pi_1 \\left( 1 - \\int_{R_1} p(\\mathbf{x} | C_1) d\\mathbf{x} \\right) + c_{12} \\pi_2 \\int_{R_1} p(\\mathbf{x} | C_2) d\\mathbf{x} \\\\                             \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R} = &c_{21} \\pi_1 + c_{22} \\pi_2 \\\\\n",
    "              &+ \\int_{R_1} \\left[ \\pi_2 (c_{12} - c_{22}) p(\\mathbf{x} | C_2) - \\pi_1 (c_{21} - c_{11}) p(\\mathbf{x} | C_1) \\right] d\\mathbf{x} \\quad \\text{where} \\,\\ c_{12} - c_{22} > 0 \\,\\ \\text{and} \\,\\ c_{21} - c_{11} > 0 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-remedy",
   "metadata": {},
   "source": [
    "$ c_{11} $ and $ c_{22} $ are negative to maximize TP and TN.<br>\n",
    "$ c_{12} $ and $ c_{21} $ are positive to minimize FP and FN.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-fiber",
   "metadata": {},
   "source": [
    "5.3.6. Bayesian classification : Minimum Bayes Risk Classifier in multiple classes<br>\n",
    "5.3.7. Naive Bayes Classifier<br>\n",
    "5.3.8. Bayes Gaussian Classifier<br>\n",
    "5.3.9. Generative and discriminative approach<br>\n",
    "5.3.10. Probabilistic generative models in two classes<br>\n",
    "5.3.11. Probabilistic generative models in multiple classes<br>\n",
    "5.3.12. Probabilistic discriminative models<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-account",
   "metadata": {},
   "source": [
    "<strong>Reference.</strong><br>\n",
    "http://sas.uwaterloo.ca/~aghodsib/courses/f07stat841/notes/lecture6.pdf<br>\n",
    "https://web.stanford.edu/~boyd/papers/pdf/robust_FDA.pdf<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
