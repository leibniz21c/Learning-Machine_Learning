{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.07 Multilayer perceptron\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Multilayer perceptron\n",
    "7.1.1. Limitation of Rosenblatt's Perceptron<br>\n",
    "- Working only for binary classes with linearly separable patterns\n",
    "- Inability to solve nonlinear classification problem(e.g., XOR problem)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.2. Model<br>\n",
    "\n",
    "<img src=\"./res/ch07/fig_1_1.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.7.1.1\n",
    "</div>\n",
    "\n",
    "Above figure consists of multiple hidden layers for feature extraction. It can do nonlinear transformation with nonlinear activation functions. \n",
    "\n",
    "$$\n",
    "\\varphi_i(\\sum w_i x_i + b)\n",
    "$$\n",
    "\n",
    "Also, it have high(or full) connectivity. It can be used in regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.3. Cost functions<br>\n",
    "\n",
    "There are training samples, that is,\n",
    "$$\n",
    "\\mathfrak{I} = \\{ \\mathbf{x}(n), \\,\\ \\mathbf{d}(n)\\}_{n - 1}^{N}\n",
    "$$\n",
    "\n",
    "Let $|C|$ be number of output nodes.\n",
    "\n",
    "- MSE cost functions\n",
    "    - Instantaneous MSE(Online learning)\n",
    "$$\n",
    "E(n) = \\frac{1}{2} \\sum_{j \\in C} e_j^2(n)\n",
    "$$\n",
    "    - Average MSE(batch learning)\n",
    "$$\n",
    "E_{av}(n) = \\frac{1}{N} \\sum_{n = 1}^N E(n) = \\frac{1}{2N} \\sum_{n = 1}^{N} \\sum_{j \\in C} e_j^2(n) \\quad \\text{where} \\,\\ e_j(n) = d_j(n) - o_j(n), \\,\\ o_j \\,\\ \\text{is a j-th output of neural network.}\n",
    "$$\n",
    "\n",
    "In regression process, we have to use instantaneous MSE because we have to reduce at least one summation.\n",
    "\n",
    "- Cross-Entropy cost functions\n",
    "    - Instantaneous cross-entropy\n",
    "$$\n",
    "E(n) = -\\frac{1}{2} \\sum_{j \\in C} d_j(n) \\log(o_j(n))\n",
    "$$\n",
    "    - Average cross-entropy\n",
    "$$\n",
    "E_{av}(n) = \\frac{1}{N} \\sum_{n = 1}^N E(n) = -\\frac{1}{2N} \\sum_{n = 1}^{N} \\sum_{j \\in C} d_j(n) \\log(o_j(n))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where} \\,\\ d_j(n) \\in \\{0,1\\}, \\,\\ o_j(n) \\in [0, 1] \\,\\ \\text{and} \\,\\ \\sum_jo_j(n) = 1\n",
    "$$\n",
    "\n",
    "In classification process, $d_j(n)$ is one-hot encoded label and $o_j(n)$ is probability value. In this context, this model should use softmax activation function so that $ \\sum_j o_j(n) = 1 \\quad (\\because \\,\\ \\text{Axiom.A.3.3})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.4. Batch and online learning<br>\n",
    "- Batch Learning\n",
    "    - Using all the training samples for weight updates\n",
    "    - Cost function : $ E_{av}(N) $\n",
    "    - Approaching to the standard gradient descent\n",
    "    - Large memory but stable behavior\n",
    "- Online Learning\n",
    "    - Weight updates are on an example-by-example basis\n",
    "    - Cost function : $ E(n) $\n",
    "    - Simple and effective to implement\n",
    "    - Much less storage but less stable behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.5. Back-Propagation algorithm : Overview<br>\n",
    "We will consider online learning method in this chapter. <br>\n",
    "Back-Propagation algorithm is the algorithm that can get gradient to training neural network. <br><br>\n",
    "\n",
    "Let $ n $ be an index of training sample and $ w_{ij} $ be edge(connection) from neuron $ i $ to neuron $ j $.<br>\n",
    "$ v_i $ is an induced local field of neuron $ i $ and $ y_i $ is output of neuron $ j $.<br>\n",
    "Neuron $j$ is a node in the $ l $th layer and neuron $i$ is a node in the $ (l-1) $th layer.<br>\n",
    "\n",
    "$$\n",
    "v_j(n) = \\sum_{i = 0}^m w_{ji}(n) y_i(n) \n",
    "$$\n",
    "\n",
    "$$\n",
    "y_j(n) = \\varphi(v_j(n))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In online learning,\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{ji}(n + 1) &= w_{ji}(n) - \\eta \\frac{\\partial E(n)}{\\partial w_{ji}(n)} \\\\\n",
    "              &= w_{ji}(n) + \\Delta w_{ji}(n) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E(n)}{\\partial w_{ji}(n)} = \\frac{\\partial E(n)}{\\partial v_j(n)} \\frac{\\partial v_j (n)}{\\partial w_{ji}(n)} \\quad (\\because \\,\\ \\text{Chain rule})\n",
    "$$\n",
    "<br>\n",
    "Let $  \\frac{\\partial E(n)}{\\partial v_j(n)} = - \\delta_j(n) $. <br>\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial E(n)}{\\partial w_{ji}(n)} = -\\delta_j(n) y_j(n) \\quad (\\because \\,\\ \\frac{\\partial v_j(n)}{\\partial w_{ji}(n)} = \\frac{\\partial}{\\partial w_{ji}}\\left( \\sum_{i = 0}^m w_{ji}(n) y_i(n)  \\right) = y_i(n))\n",
    "$$\n",
    "\n",
    "$ \\delta_j(n) $ is called __local gradient__ . In this time, the weight update is proportional to the local gradient and the input signal, that is, the error multiplied by the local gradient.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Delta w_{ij}(n) &= - \\eta \\frac{\\partial E(n)}{\\partial w_{ji} (n)} \\\\\n",
    "                 &= \\eta \\delta_j(n) y_i(n) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means, \n",
    "$$\n",
    "\\left(\\text{Weight correction} \\,\\ \\Delta w_{ji}(n)\\right) = \\left(\\text{learning-rate parameter} \\,\\ \\eta \\right) \\times \\left(\\text{local gradient} \\,\\ \\delta_j(n)\\right) \\times \\left(\\text{input signal of neuron } \\,\\ j, \\,\\ y_i(n)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.6. Back-Propagation algorithm : Local gradients at output nodes<br>\n",
    "Suppose neuron $j$ is an output node and cost function is MSE.\n",
    "\n",
    "<img src=\"./res/ch07/fig_1_2.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.7.1.2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\delta_j(n) &= -\\frac{\\partial E(n)}{\\partial v_j(n)} \\quad (\\because \\,\\ \\text{Definition}) \\\\\n",
    "            &= - \\frac{\\partial E(n)}{\\partial e_j(n)} \\frac{\\partial e_j(n)}{\\partial y_j(n)} \\frac{\\partial y_j (n)}{\\partial v_j(n)} \\quad (\\because \\,\\ \\text{Chain rule}) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E(n)}{\\partial e_j(n)} = e_j(n) \\quad \\left( \\because \\,\\ E(n) = \\frac{1}{2} \\sum_{j \\in C} e_j^2(n) \\right) \\quad \\cdots (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e_j(n)}{\\partial y_j(n)} = -1 \\quad \\left( \\because \\,\\ e_j(n) = d_j(n) - y_j(n) \\right) \\quad \\cdots (2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j(n)}{\\partial v_j(n)} = \\varphi_j^\\prime (v_j(n)) \\quad \\left( \\because \\,\\ y_j(n) = \\varphi_j(v_j(n)) \\right) \\quad \\cdots (3)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta_j(n) = \\varphi_j^\\prime (v_j(n)) e_j(n) \\qquad (\\because \\,\\ (1), \\,\\ (2), \\,\\ \\text{and} \\,\\ (3))\n",
    "$$\n",
    "\n",
    "In the linear model, $ \\delta_j(n) = e_j(n) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.7. Back-Propagation algorithm : Local gradients at hidden nodes<br>\n",
    "Suppose neuron $j$ is an hidden node at $(L - 1) $ layer and cost function is MSE.\n",
    "\n",
    "<img src=\"./res/ch07/fig_1_3.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.7.1.3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta_j(n) = - \\frac{\\partial E(n)}{\\partial v_j(n)} = - \\frac{\\partial E(n)}{\\partial y_j(n)} \\frac{\\partial y_j(n)}{\\partial v_j (n)}\n",
    "$$\n",
    "\n",
    "We already know that, \n",
    "$$\n",
    "\\frac{\\partial y_j(n)}{\\partial v_j(n)} = \\varphi_j^\\prime (v_j(n)) \\quad (\\because \\,\\ y_j(n) = \\varphi_j(v_j(n)))\n",
    "$$\n",
    "\n",
    "So, we have to focus on following term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(n)}{\\partial y_j(n)} &= \\sum_k e_k(n) \\frac{\\partial e_k(n)}{\\partial y_j(n)} \\quad (\\because \\,\\ E(n) = \\frac{1}{2} \\sum_k e_k^2(n) ) \\\\\n",
    "                                      &= \\sum_k e_k(n) \\frac{\\partial e_k(n)}{\\partial v_k(n)} \\frac{\\partial v_k(n)}{\\partial y_j(n)} \\quad (\\because \\,\\ \\text{Chain rule}) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial e_k(n)}{\\partial v_k(n)} = \\varphi_k^\\prime (v_k(n)) \\quad (\\because \\,\\ e_k(n) = d_k(n) - y_k(n) = d_k(n) - \\varphi_k(v_k(n))) \\quad \\cdots (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial v_k(n)}{\\partial y_j(n)} = w_{kj}(n) \\quad (\\because \\,\\ v_k(n) = \\sum_{j = 1}^{m}w_{kj}(n)y_j(n)) \\quad \\cdots (2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(n)}{\\partial y_j(n)} &= \\sum_k e_k(n) \\frac{\\partial e_k(n)}{\\partial v_k(n)} \\frac{\\partial v_k(n)}{\\partial y_j(n)} \\\\\n",
    "                                      &= - \\sum_k e_k(n) \\varphi_k^\\prime (v_k(n))w_{kj}(n) \\quad (\\because \\,\\ (1) \\,\\ \\text{and} \\,\\ (2))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\delta_j(n) &= - \\frac{\\partial E(n)}{\\partial v_j(n)} = - \\frac{\\partial E(n)}{\\partial y_j(n)} \\frac{\\partial y_j(n)}{\\partial v_j (n)} \\\\\n",
    "            &= \\varphi_j^\\prime (v_j(n)) \\cdot \\sum_k e_k(n) \\varphi_k^\\prime (v_k(n)) w_{kj}(n) \\\\\n",
    "            &= \\varphi_j^\\prime(v_j(n)) \\cdot \\sum_k \\delta_k(n) w_{kj}(n) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "In the above formula, we can see that the values of the next layer are multiplied by $ \\delta $. In order to generalize this, we have to check the $ (l-2) $ layer, and we can prove this by mathematical induction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose $ i $ is an hidden node at $(L - 2) $ layer and cost function is MSE. It means, <br>\n",
    "$ k $ : a neuron in the $L$th (i.e., output) layer<br>\n",
    "$ j $ : a neuron in the $(L-1)$th (i.e., output) layer<br>\n",
    "$ i $ : a neuron in the $(L-2)$th (i.e., output) layer<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta_i(n) = - \\frac{\\partial E(n)}{\\partial v_i(n)} = - \\frac{\\partial E(n)}{\\partial y_i(n)} \\frac{\\partial y_i(n)}{\\partial v_i (n)}\n",
    "$$\n",
    "\n",
    "We already know that, \n",
    "$$\n",
    "\\frac{\\partial y_i(n)}{\\partial v_i(n)} = \\varphi_i^\\prime (v_i(n)) \\quad (\\because \\,\\ y_i(n) = \\varphi_i(v_i(n)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(n)}{\\partial y_i(n)} &= \\sum_k e_k(n) \\frac{\\partial e_k(n)}{\\partial y_i(n)} \\quad (\\because \\,\\ E(n) = \\frac{1}{2} \\sum_k e_k^2(n)) \\\\\n",
    "                                      &= \\sum_k e_k(n) \\frac{\\partial e_k(n)}{\\partial v_k(n)} \\frac{\\partial v_k(n)}{\\partial y_i(n)} \\\\\n",
    "                                      &= \\sum_k e_k(n) \\left( - \\varphi_k^\\prime (v_k(n)) \\right) \\frac{\\partial v_k(n)}{\\partial y_i(n)} \\quad (\\because \\,\\ e_k(n) = d_k(n) - \\varphi_k(v_k(n)))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this time, the local induced field of output layer is,\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_k(n) &= \\sum_{j = 1}^m w_{kj}(n)y_j(n) = \\sum_{j = 1}^m w_{kj}(n) \\varphi_j (v_j(n)) \\\\\n",
    "       &= \\sum_{j = 1}^m w_{kj}(n) \\varphi_j \\left( \\sum_{i = 1}^l w_{ji}(n) y_i(n) \\right) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial v_k(n)}{\\partial y_i(n)} &= \\sum_{j = 1}^m w_{kj}(n) \\varphi_j^\\prime \\left( \\sum_{i = 1}^l w_{ji}(n) y_i(n) \\right) w_{ji}(n) \\\\\n",
    "                                        &= \\sum_{j = 1}^m w_{kj}(n) \\varphi_j^\\prime \\left( v_j(n) \\right) w_{ji}(n) \\quad \\cdots (1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(n)}{\\partial y_i(n)} &= - \\sum_k e_k(n) \\varphi_k^\\prime (v_k(n)) \\frac{\\partial v_k(n)}{\\partial y_i(n)} \\\\\n",
    "                                      &= - \\sum_k e_k(n) \\varphi_k^\\prime (v_k(n)) \\sum_{j} w_{kj}(n) \\varphi_j^\\prime \\left( v_j(n) \\right) w_{ji}(n)  \\quad (\\because \\,\\ (1))\\\\\n",
    "                                      &= - \\sum_j \\left[\\sum_k \\delta_k(n) w_{kj}(n) \\varphi)j^\\prime (v_j(n)) \\right] w_{ji}(n) \\\\\n",
    "                                      &= - \\sum_j \\delta_j(n)w_{ji}(n)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\therefore \\quad \\delta_i(n) = - \\varphi_i^\\prime (v_i(n)) \\cdot \\sum_j \\delta_j(n) w_{ji}(n)\n",
    "$$\n",
    "\n",
    "This is the same as the $(L-1)$ layer. $ \\qquad \\blacksquare $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.8. Back-Propagation algorithm : Algorithm<br>\n",
    "\n",
    "1. __Initialization__  : Initialize weights and randomly shuffle training samples.\n",
    "2. __Forward Computation__  : \n",
    "$$\n",
    "v_j^{(l)}(n) = \\sum_i w_{ji}^{(l)}(n) y_i^{(l-1)}(n) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_j^{l}(n) = \n",
    "\\begin{cases}\n",
    "x_j(n), & \\text{for neuron} \\,\\ j \\,\\ \\text{in the input layer, i.e.,} \\,\\ l = 1 \\\\\n",
    "\\varphi_j(v_j(n)), & \\text{for neuron} \\,\\ j \\,\\ \\text{in the hidden layer} \\\\\n",
    "o_j(n), & \\text{for neuron} \\,\\ j \\,\\ \\text{in the output layer, i.e.,} \\,\\ l = L \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{And we can get} \\quad e_j(n) = d_j(n) - o_j(n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Backward Computation__  :\n",
    "$$\n",
    "\\delta_j^{(l)}(n) = \n",
    "\\begin{cases}\n",
    "e_j^{(L)}(n) \\varphi_j^\\prime \\left( v_j^{(L)}(n) \\right) , & \\text{for neuron} \\,\\ j \\,\\ \\text{in the output layer} \\,\\ L \\\\\n",
    "\\varphi_j^\\prime \\left( v_j^{(l)}(n) \\right) \\sum_k \\delta_k^{(l + 1)}(n) w_{kj}^{(l + 1)} (n), & \\text{for neuron} \\,\\ j \\,\\ \\text{in the hidden layer} \\,\\ l \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Weights update__  :\n",
    "\n",
    "$$\n",
    "w_{ji}^{(l)}(n + 1) = w_{ji}^{(l)}(n) + \\eta \\delta_j^{(l)}(n) y_i^{(l - 1)}(n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we update weights, we can consider momentum additionally. It goes like\n",
    "\n",
    "$$\n",
    "w_{ji}^{(l)}(n + 1) = w_{ji}^{(l)}(n) + \\eta \\delta_j^{(l)}(n) y_i^{(l - 1)}(n) + \\alpha \\Delta w_{ji}^{(l)}(n - 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1.9. XOR Problem<br>\n",
    "Let's solve __XOR Problem__  with above model and algorithm!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reference.</strong><br>\n",
    "Simon Haykin, Neural networks and learning machines<br>\n",
    "Yosha Benjio, Deep Learning<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
