{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.02 Rosenblatt's Perceptron\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.1 Rosenblatt's Perceptron Model\n",
    "2.1.1. Overview<br>\n",
    "\n",
    "1. The first model/algorithm for supervised learning, in <strong>1957</strong> \n",
    "2. Single-layer single-output neural network for binary classification of <strong>linearly separable</strong> patterns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2.1.2. Activation function : Sign function(threshold function)\n",
    "\n",
    "$$\n",
    "\\varphi(x) = \n",
    "\\begin{cases}\n",
    "+1, \\quad if \\,\\ x > 0 \\\\\n",
    "-1, \\quad if \\,\\ x < 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "2.1.3. Network Architecture<br>\n",
    "- Basic model :\n",
    "\n",
    "<img src=\"./res/ch02/fig_2_1.png\" width=\"550\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.2.1.1\n",
    "</div>\n",
    "\n",
    "- Compact model : \n",
    "\n",
    "$ Let \\,\\ \\mathbf{x} = [+1, \\,\\ x_1, \\,\\ x_2, \\,\\ \\cdots, \\,\\ x_m]^T \\,\\ and \\,\\ \n",
    "\\mathbf{w} = [b, \\,\\ w_1, \\,\\ w_2, \\,\\ \\cdots, \\,\\ w_m]^T$<br>\n",
    "$$ v = \\sum_{i = 0}^{m} w_i x_i = \\mathbf{w}^T \\mathbf{x}, \\quad y = sgn(v) = sgn(\\mathbf{w}^T \\mathbf{x}) $$\n",
    "\n",
    "<img src=\"./res/ch02/fig_2_2.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.2.1.2\n",
    "</div>\n",
    "\n",
    "2.1.4. Assumption <br>\n",
    " - This is binary classification.\n",
    " - Two classes are linearly separable.\n",
    " - Decision boundary(Hyper plane) : \n",
    " $$ \\mathbf{w}^T \\mathbf{x} = 0 $$\n",
    " - Decision rule for binary classification :\n",
    " $$ \\mathbf{x} \\in C_1 \\,\\ if \\,\\ y = +1 \\,\\ \\nLeftrightarrow \\,\\ \\mathbf{w}^T \\mathbf{x} > 0 $$\n",
    " $$ \\mathbf{x} \\in C_2 \\,\\ if \\,\\ y = -1 \\,\\ \\nLeftrightarrow \\,\\ \\mathbf{w}^T \\mathbf{x} < 0 $$\n",
    "\n",
    "2.1.5. Training Problem Definition<br>\n",
    "To find a weight vector $ \\mathbf{w} $ such that<br>\n",
    "$ \\mathbf{w}^T \\mathbf{x} > 0 $ for every input vector $ \\mathbf{x} $ belonging to class $ C_1 $<br>\n",
    "$ \\mathbf{w}^T \\mathbf{x} \\le 0 $ for every input vector $ \\mathbf{x} $ belonging to class $ C_2 $<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2.1.6. Training Algorithm for Perceptron<br>\n",
    "- Cost function : Use distance metric, not MSE. Total Distance between the classifier and misclassified samples.\n",
    "\n",
    "$$ J(\\mathbf{w}) = \\sum_{\\mathbf{x} \\in \\mathcal{H}} | \\mathbf{w}^T \\mathbf{x} |, \\quad where \\,\\ \\mathcal{H} \\text{ is set of misclassfied samples.} $$\n",
    "Therefore, \n",
    "$$ (\\mathbf{w}^T \\mathbf{x}) d > 0  \\quad (\\text{correctly classified}) $$\n",
    "$$ (\\mathbf{w}^T \\mathbf{x}) d \\le 0  \\quad (\\text{wrongly classified}) $$\n",
    "\n",
    "- Learning via gradient descent\n",
    "\n",
    "$$ \\nabla J(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} ( - \\sum_{\\mathbf{x} \\in \\mathcal{H}} d \\mathbf{w}^T \\mathbf{x} ) = - \\sum_{\\mathbf{x} \\in \\mathcal{H}} d \\mathbf{x} $$\n",
    "It follows <strong>Widrow-Hoff(or LMS)</strong> learning rule. <br>\n",
    "    - Correction depends on the <strong>error</strong>\n",
    "    - Small(or large) update when the error is small(or large)\n",
    "An equivalent update take the following form:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_{\\mathbf{x} \\in \\mathcal{H}} d \\mathbf{x} \\quad \\mathcal{H} : \\text{ set of misclassfied samples} \\\\\n",
    "= \\mathbf{w} + \\frac{\\eta}{2} \\sum_{\\mathbf{x} \\in \\mathcal{H}} (d - y) \\mathbf{x} \\qquad where \\,\\ (d-y)  \\text{ is error} \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Batch training :\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_{\\mathbf{x} \\in \\mathcal{H}} (d-y) \\mathbf{x} $$ \n",
    "\n",
    "Online Version : \n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (d-y) \\mathbf{x} $$ \n",
    "\n",
    "Both learning algorithms were shown to converge by NoviKov's Theorem.\n",
    "\n",
    "<img src=\"./res/ch02/alg_2_1.png\" width=\"700\" height=\"500\"><br>\n",
    "<div align=\"center\">\n",
    "  Algorithm.2.1.1\n",
    "</div>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2.1.7. Geometric Interpretation<br>\n",
    "$$ \\mathbf{w}_{new} \\leftarrow \\mathbf{w}_{old} + d \\mathbf{x} $$\n",
    "\n",
    "<img src=\"./res/ch02/fig_2_3.png\" width=\"550\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.2.1.3\n",
    "</div>\n",
    "\n",
    "It can work in linear separable sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2.1.8. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.2 Perceptron Convergence Theorem\n",
    "2.2.1. Perceptron Convergence Theorem<br>\n",
    "For the subsets of training vectors to be linearly separable(look at the assumption), the perceptron converges after some $ n_0 $ iterations, in the sense that\n",
    "\n",
    "$$ _{}^{\\forall}i, \\quad \\mathbf{w}(n_0) = \\mathbf{w}(n_0 + i) \\qquad where \\,\\ i \\in \\mathbb{N} $$\n",
    "\n",
    "$ \\mathbf{w(n_0)} $ is a solution vector for $ n_0 \\le n_{max} $<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "When correctly classified, $ \\nexists $ correction\n",
    "$$ \\mathbf{w}(n+1) = \\mathbf{w}(n)  \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) > 0, \\,\\ \\mathbf{x}(n) \\in C_1 $$\n",
    "$$ \\mathbf{w}(n+1) = \\mathbf{w}(n)  \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) \\le 0, \\,\\ \\mathbf{x}(n) \\in C_2 $$\n",
    "When wrongly classified, the weight vector is updated as\n",
    "$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) - \\eta \\mathbf{x}(n) \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) > 0, \\,\\ \\mathbf{x}(n) \\in C_2 $$\n",
    "$$ \\mathbf{w}(n+1) = \\mathbf{w}(n) + \\eta \\mathbf{x}(n) \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) \\le 0, \\,\\ \\mathbf{x}(n) \\in C_1 $$\n",
    "<br>\n",
    "Suppose the worst case\n",
    "$$ \\eta = 1, \\,\\ \\mathbf{w}(0) = 0, \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) < 0, \\,\\ \\mathbf{x}(n) \\in C_1 , \\,\\ _{}^{\\forall}n $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2.3 Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "https://www.ctan.org/pkg/algorithmicx<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}