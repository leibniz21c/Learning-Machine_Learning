{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter.03 Regression\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.1. Regressive and approximated models\n",
    "3.1.1. General regressive model\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_1.png\" width=\"700\" height=\"200\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.1.1\n",
    "</div>\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.1.2. Approximated model\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_2.png\" width=\"700\" height=\"110\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.1.2\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "I want to find a function $\\hat{f}(\\mathbf{x}; D)$ that approximates the true function $f(\\mathbf{x})$ as well as possible in terms of the mean square error (MSE) between them, by means of some learning algorithm based on a training dataset (sample)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.2. Linear Regression\n",
    "3.2.1. Linearly approximated model<br>\n",
    "\n",
    "<img src=\"./res/ch03/fig_3_3.png\" width=\"850\" height=\"130\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.3.1.3\n",
    "</div>\n",
    "<br>\n",
    "Linear regression techniques aim to find a linear function $\\hat{f} = w^T x$ that approximates the true function $f(x)$ as well as possible in terms of the mean square error (MSE) between them, based on a training dataset (sample)\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.2. Hypothesis<br>\n",
    "$$ \\hat{y} = \\sum_j w_i x_i + b = \\mathbf{w}^T \\mathbf{x} \\quad \\text{where} \\quad \\mathbf{w} = [w_1, \\,\\ \\cdots, \\,\\ w_{m-1}, \\,\\ b]^T, \\,\\ \\mathbf{x} = [x_1, \\,\\ \\cdots, \\,\\ x_{m-1}, \\,\\ 1]^T $$\n",
    "- $y$ : Target(of label)\n",
    "- $\\hat{y}$ : Output of model\n",
    "- $w_i$ : Weights\n",
    "- $b$ : Bias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.3. Linear regression problem<br>\n",
    "Given the training set, to optimize the parameters $\\mathbf{w}$ to minimize the least squares error :\n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} \\sum_i (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\} $$\n",
    "$J(\\mathbf{w})$ is convex and quadratic function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.4. Learning algorithm : A numerical approach<br>\n",
    "1) Gradient descent algorithm\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} $$\n",
    "$\\alpha$ is Learning rate.<br>\n",
    "\n",
    "2) Gradient calculation\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} &= \\frac{\\partial}{\\partial \\mathbf{w}} \\frac{1}{2} \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i)^2 \\\\\n",
    "                                                   &= \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\\\\n",
    "                                                   &= \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i \\\\\n",
    "\\end{align*}                                          \n",
    "$$<br>\n",
    "\n",
    "3) Batch learning algorithm<br>\n",
    "Perform gradient descent step over the whole training set.<br>\n",
    "<strong>Repeat until convergence : </strong>\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i $$\n",
    "\n",
    "4) Online learning algorithm<br>\n",
    "Perform gradient descent step over a single training example.<br>\n",
    "<strong>Repeat until convergence : </strong>\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{For } \\,\\ i &= 1 \\,\\ \\text{ to } \\,\\  N :  \\\\\n",
    "& \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\sum_i (\\mathbf{w}^T \\mathbf{x}_i - y_i) \\mathbf{x}_i \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "All of these algorithms converge to the global optimal point!(convex and quadratic!)<br>\n",
    "Update depends on the error, small(or large) update when the error is small(or large).<br>\n",
    "It is called Widrow-Hoff(or LMS) learning rule.<br><br>\n",
    "\n",
    "In big learning late, these are unstable(zigzaging).<br>\n",
    "In small learning late, these converge slowly.<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.5. Learning algorithm : Least squares(One-shot learning approach)<br>\n",
    "Let's rewrite the cost function in a compact form as\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} \\sum_{i = 1}^{N} (y_i - \\mathbf{w}^T \\mathbf{x}_i)^2 \\\\\n",
    "              &= \\frac{1}{2} \n",
    "\\begin{bmatrix}\n",
    "y_1 - \\mathbf{x}_1^T \\mathbf{w} & y_2 - \\mathbf{x}_2^T \\mathbf{w} & \\cdots & y_N - \\mathbf{x}_N^T \\mathbf{w}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 - \\mathbf{x}_1^T \\mathbf{w} \\\\\n",
    "y_2 - \\mathbf{x}_2^T \\mathbf{w} \\\\\n",
    " \\vdots \\\\\n",
    "y_N - \\mathbf{x}_N^T \\mathbf{w} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "             &= \\frac{1}{2} (\\mathbf{y} - X \\mathbf{w})^T (\\mathbf{y} - X \\mathbf{w}) \\\\\n",
    "             &= \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{y} = [y_1, \\,\\ \\cdots, \\,\\ y_N]_{N \\times 1}^T $\n",
    "- $ X = [\\mathbf{x}_1, \\,\\ \\cdots, \\,\\ \\mathbf{x}_N]_{N \\times m}^T $\n",
    "- $m$ : Order of model\n",
    "- $N$ : Number of training examples\n",
    "<br>\n",
    "\n",
    "Thus, this problem can be considered as the ordinary least squares problem : \n",
    "$$ \\min_{\\mathbf{w}} \\{ J(\\mathbf{w}) = \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2 \\} $$\n",
    "\n",
    "<strong>Case 1 : Exact and unique solution when </strong> $ N = m = \\text{Rank}(X) $<br>\n",
    "$$ \\mathbf{w} = X^{-1} y $$\n",
    "When $N = m$ and $X$ is full-rank, the zero error can be achieved. Using a simple linear model and a set of training samples, it is able to perfectly estimate an unknown function. However, this ideal case of $N = m$ rarely occurs in practice(Not practical case).\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 2 : Over-determined case when </strong> $ N > m = \\text{Rank}(X) $<br>\n",
    "General case(Most practical case). But no unique solution to the equation $\\mathbf{y} = X\\mathbf{w}$.<br>\n",
    "Instead, try to minimize the error:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\mathbf{w}) &= \\frac{1}{2} || \\mathbf{y} - X \\mathbf{w} ||^2 \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{y} - X \\mathbf{w})^T (\\mathbf{y} - X \\mathbf{w}) \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{y}^T X \\mathbf{w} + \\mathbf{w}^T X^T X \\mathbf{w}) \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}} J(\\mathbf{w}) &= \\frac{1}{2} \\frac{\\partial}{\\partial \\mathbf{w}} ( \\mathbf{y}^T \\mathbf{y} - 2 \\mathbf{y}^T X \\mathbf{w} + \\mathbf{w}^T X^T X \\mathbf{w} ) \\\\\n",
    "              &= X^T X \\mathbf{w} - X^T \\mathbf{y} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{w}} J(\\mathbf{w}) = \\mathbf{0} \\quad \\Longleftrightarrow \\quad X^T X \\mathbf{w} = X^T \\mathbf{y} \\quad \\text{(Normal equation)}$$\n",
    "LS solution : $\\mathbf{w} = (X^T X)^{-1} X^T \\mathbf{y}$\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 3 : Under-determined case when </strong> $ m > N = \\text{Rank}(X) $<br>\n",
    "There are infinitely many solutions to the equation $\\mathbf{y} = X \\mathbf{w}$, each of which yields the zero error. Try to obtain a particular solution with the minimum norm :\n",
    "$$ \\min_{\\mathbf{w}} \\frac{1}{2} || \\mathbf{w} ||^2 \\qquad s.t. \\qquad \\mathbf{y} = X \\mathbf{w} $$\n",
    "\n",
    "Suppose $ \\mathcal{L}(\\mathbf{w}, \\mathbf{\\mu}) = \\frac{1}{2} || \\mathbf{w} ||^2 + \\mathbf{\\mu}^T (\\mathbf{y} - X\\mathbf{w}) $.\n",
    "$$\n",
    "\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}, \\mathbf{\\mu}) = \\mathbf{w} - X^T \\mathbf{\\mu}\n",
    "$$\n",
    "\n",
    "$$ \\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}, \\mathbf{\\mu}) = \\mathbf{0} \\implies \\mathbf{w} = X^T \\mathbf{\\mu} $$\n",
    "\n",
    "$$ X \\mathbf{w} = XX^T \\mathbf{\\mu} = \\mathbf{y} \\implies \\mathbf{\\mu} = (XX^T)^{-1} \\mathbf{y} $$\n",
    "\n",
    "Therefore, LS solution is $ \\mathbf{w} = X^T (XX^T)^{-1} \\mathbf{y} $.\n",
    "<br><br>\n",
    "\n",
    "<strong>Case 4 : Rank-deficient under-determined case and rank-deficient over-determined case when </strong> $ m > N > \\text{Rank}(X) $ and $ N > m > \\text{Rank}(X) $<br>\n",
    "There are infinitely many solutions satisfying the normal equation. Therefore, the LS solution is not unique. Try to obtain minimum norm least squares solution\n",
    "\n",
    "$$ \\mathbf{w} = X^+ \\mathbf{y} = V_1 \\Sigma_1^{-1} U_1^H \\mathbf{y} $$\n",
    "<br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3.2.6. Recursive least squares<br>\n",
    "The idea of recursive least squares is that \"What if additional training examples are available?\". The RLS aims to solve the least squares problem recursively. That is, it finds the new regression model. \n",
    "\n",
    "$$ \n",
    "\\mathbf{w}(n) = \\arg\\min_{\\mathbf{w}} \n",
    "\\begin{Vmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{y}(1) \\\\\n",
    "\\mathbf{y}(2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{y}(n) \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "X(1) \\\\\n",
    "X(2) \\\\\n",
    "\\vdots \\\\\n",
    "X(n) \\\\\n",
    "\\end{bmatrix}\n",
    "\\mathbf{w}\n",
    "\\end{Vmatrix}^2\n",
    "$$\n",
    "in terms of new training sample $(X(n), \\,\\ \\mathbf{y}(n))$ and previous model\n",
    "$$ \n",
    "\\mathbf{w}(n-1) = \\arg\\min_{\\mathbf{w}} \n",
    "\\begin{Vmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{y}(1) \\\\\n",
    "\\mathbf{y}(2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{y}(n-1) \\\\\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "X(1) \\\\\n",
    "X(2) \\\\\n",
    "\\vdots \\\\\n",
    "X(n-1) \\\\\n",
    "\\end{bmatrix}\n",
    "\\mathbf{w}\n",
    "\\end{Vmatrix}^2\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.3. Bayesian Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.4. Logistic and Softmax Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.5. $k$-Nearest Neighbors($k$-NN) Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}