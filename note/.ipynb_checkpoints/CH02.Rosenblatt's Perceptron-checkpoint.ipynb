{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.02 Rosenblatt's Perceptron\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Rosenblatt's Perceptron Model\n",
    "2.1.1. Overview<br>\n",
    "\n",
    "1. The first model/algorithm for supervised learning, in <strong>1957</strong> \n",
    "2. Single-layer single-output neural network for binary classification of <strong>linearly separable</strong> patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2. Activation function : Sign function(threshold function)\n",
    "\n",
    "$$\n",
    "\\varphi(x) = \n",
    "\\begin{cases}\n",
    "+1, \\quad if \\,\\ x > 0 \\\\\n",
    "-1, \\quad if \\,\\ x < 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "2.1.3. Network Architecture<br>\n",
    "- Basic model :\n",
    "\n",
    "<img src=\"./res/ch02/fig_2_1.png\" width=\"550\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.2.1.1\n",
    "</div>\n",
    "\n",
    "- Compact model : \n",
    "\n",
    "$ Let \\,\\ \\mathbf{x} = [+1, \\,\\ x_1, \\,\\ x_2, \\,\\ \\cdots, \\,\\ x_m]^T \\,\\ and \\,\\ \n",
    "\\mathbf{w} = [b, \\,\\ w_1, \\,\\ w_2, \\,\\ \\cdots, \\,\\ w_m]^T$<br>\n",
    "$$ v = \\sum_{i = 0}^{m} w_i x_i = \\mathbf{w}^T \\mathbf{x}, \\quad y = sgn(v) = sgn(\\mathbf{w}^T \\mathbf{x}) $$\n",
    "\n",
    "<img src=\"./res/ch02/fig_2_2.png\" width=\"600\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.2.1.2\n",
    "</div>\n",
    "\n",
    "2.1.4. Assumption <br>\n",
    " - This is binary classification.\n",
    " - Two classes are linearly separable.\n",
    " - Decision boundary(Hyper plane) : \n",
    " $$ \\mathbf{w}^T \\mathbf{x} = 0 $$\n",
    " - Decision rule for binary classification :\n",
    " $$ \\mathbf{x} \\in C_1 \\,\\ if \\,\\ y = +1 \\,\\ \\nLeftrightarrow \\,\\ \\mathbf{w}^T \\mathbf{x} > 0 $$\n",
    " $$ \\mathbf{x} \\in C_2 \\,\\ if \\,\\ y = -1 \\,\\ \\nLeftrightarrow \\,\\ \\mathbf{w}^T \\mathbf{x} < 0 $$\n",
    "\n",
    "2.1.5. Training Problem Definition<br>\n",
    "To find a weight vector $ \\mathbf{w} $ such that<br>\n",
    "$ \\mathbf{w}^T \\mathbf{x} > 0 $ for every input vector $ \\mathbf{x} $ belonging to class $ C_1 $<br>\n",
    "$ \\mathbf{w}^T \\mathbf{x} \\le 0 $ for every input vector $ \\mathbf{x} $ belonging to class $ C_2 $<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.6. Training Algorithm for Perceptron<br>\n",
    "- Cost function : Use distance metric, not MSE. Total Distance between the classifier and misclassified samples.\n",
    "\n",
    "$$ J(\\mathbf{w}) = \\sum_{\\mathbf{x} \\in \\mathcal{H}} | \\mathbf{w}^T \\mathbf{x} |, \\quad where \\,\\ \\mathcal{H} \\text{ is set of misclassfied samples.} $$\n",
    "Therefore, \n",
    "$$ (\\mathbf{w}^T \\mathbf{x}) d > 0  \\quad (\\text{correctly classified}) $$\n",
    "$$ (\\mathbf{w}^T \\mathbf{x}) d \\le 0  \\quad (\\text{wrongly classified}) $$\n",
    "\n",
    "- Learning via gradient descent\n",
    "\n",
    "$$ \\nabla J(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} ( - \\sum_{\\mathbf{x} \\in \\mathcal{H}} d \\mathbf{w}^T \\mathbf{x} ) = - \\sum_{\\mathbf{x} \\in \\mathcal{H}} d \\mathbf{x} $$\n",
    "It follows <strong>Widrow-Hoff(or LMS)</strong> learning rule. <br>\n",
    "    - Correction depends on the <strong>error</strong>\n",
    "    - Small(or large) update when the error is small(or large)\n",
    "An equivalent update take the following form:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_{\\mathbf{x} \\in \\mathcal{H}} d \\mathbf{x} \\quad \\mathcal{H} : \\text{ set of misclassfied samples} \\\\\n",
    "= \\mathbf{w} + \\frac{\\eta}{2} \\sum_{\\mathbf{x} \\in \\mathcal{H}} (d - y) \\mathbf{x} \\qquad where \\,\\ (d-y)  \\text{ is error} \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Batch training :\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\sum_{\\mathbf{x} \\in \\mathcal{H}} (d-y) \\mathbf{x} $$ \n",
    "\n",
    "Online Version : \n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (d-y) \\mathbf{x} $$ \n",
    "\n",
    "Both learning algorithms were shown to converge by NoviKov's Theorem.\n",
    "\n",
    "<img src=\"./res/ch02/alg_2_1.png\" width=\"700\" height=\"500\"><br>\n",
    "<div align=\"center\">\n",
    "  Algorithm.2.1.1\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.7. Geometric Interpretation<br>\n",
    "$$ \\mathbf{w}_{new} \\leftarrow \\mathbf{w}_{old} + d \\mathbf{x} $$\n",
    "\n",
    "<img src=\"./res/ch02/fig_2_3.png\" width=\"550\" height=\"300\"><br>\n",
    "<div align=\"center\">\n",
    "  Figure.2.1.3\n",
    "</div>\n",
    "\n",
    "It can work in linear separable sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Perceptron Convergence Theorem\n",
    "2.2.1. Perceptron Convergence Theorem<br>\n",
    "For the subsets of training vectors to be linearly separable(look at the assumption), the perceptron converges after some $ n_0 $ iterations, in the sense that\n",
    "\n",
    "$$ _{}^{\\forall}i, \\quad \\mathbf{w}(n_0) = \\mathbf{w}(n_0 + i) \\qquad where \\,\\ i \\in \\mathbb{N} $$\n",
    "\n",
    "$ \\mathbf{w(n_0)} $ is a solution vector for $ n_0 \\le n_{max} $<br><br>\n",
    "\n",
    "<strong>Proof.</strong><br>\n",
    "When correctly classified, $ \\nexists $ correction\n",
    "$$ \n",
    "\\mathbf{w}(n+1) = \\mathbf{w}(n)  \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) > 0, \\,\\ \\mathbf{x}(n) \\in C_1 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}(n+1) = \\mathbf{w}(n)  \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) \\le 0, \\,\\ \\mathbf{x}(n) \\in C_2 \n",
    "$$\n",
    "\n",
    "When wrongly classified, the weight vector is updated as\n",
    "$$\n",
    "\\mathbf{w}(n+1) = \\mathbf{w}(n) - \\eta \\mathbf{x}(n) \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) > 0, \\,\\ \\mathbf{x}(n) \\in C_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\eta \\mathbf{x}(n) \\quad if \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) \\le 0, \\,\\ \\mathbf{x}(n) \\in C_1\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Suppose the worst case\n",
    "$$\n",
    "\\eta = 1, \\,\\ \\mathbf{w}(0) = 0, \\,\\ \\mathbf{w}^T(n) \\mathbf{x}(n) < 0, \\,\\ \\mathbf{x}(n) \\in C_1 , \\,\\ _{}^{\\forall}n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(i) \\qquad \\mathbf{w}(n + 1) &= \\mathbf{w}(n) + \\mathbf{x}(n) \\\\\n",
    "                            &= \\mathbf{x}(1) + \\mathbf{x}(2) + \\cdots + \\mathbf{x}(n) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, \n",
    "$ \\exists \\,\\ \\mathbf{w}_o \\quad s.t. \\quad \\mathbf{w}^T \\mathbf{x}(n) > 0 \\,\\ ( \\because $\n",
    "The training samples are linearly separable\n",
    "$ ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{align*}\n",
    "\\mathbf{w}_o^T \\mathbf{w}(n + 1) &= \\mathbf{w}_o^T \\mathbf{x}(1) + \\mathbf{w}_o^T \\mathbf{x}(2) + \\cdots + \\mathbf{w}_o^T \\mathbf{x}(n) \\\\\n",
    "                                 &\\ge n \\cdot \\min_{\\mathbf{x}(n)} \\mathbf{w}_o^T \\mathbf{x} (n) \\\\\n",
    "                                 &= n \\alpha \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "|| \\mathbf{w}_o ||^2 || \\mathbf{w}(n + 1) ||^2 \\ge n^2 \\alpha^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\therefore \\quad || \\mathbf{w}(n + 1) ||^2 \\ge \\frac{n^2 \\alpha^2}{|| \\mathbf{w}_o ||^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(ii) \\qquad \\mathbf{w}(k + 1) = \\mathbf{w}(k) + \\mathbf{x}(k), \\quad k = 1, 2, \\cdots , n, \\,\\ \\mathbf{x}(k) \\in C_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "|| \\mathbf{w}(k + 1) ||^2 &= || \\mathbf{w}(k) ||^2 + || \\mathbf{x}(k) ||^2 + 2 \\mathbf{w}^T(k) \\mathbf{x}(k) \\\\\n",
    "                          &\\le || \\mathbf{w}(k) ||^2 + || \\mathbf{x}(k) ||^2 \\quad (\\because \\mathbf{w}^T (k) \\mathbf{x} (k) < 0, \\,\\ ^\\forall k) \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "|| \\mathbf{w}(k + 1) ||^2 - | \\mathbf{w}(k) ||^2 \\le || \\mathbf{x}(k) ||^2, \\quad k = 1, 2, \\cdots , n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{k = 1}^{n} || \\mathbf{x} (k) ||^2 \\ge &|| \\mathbf{w}(k + 1) ||^2 - || \\mathbf{w}(k) ||^2 \\\\\n",
    "                                            & \\vdots \\\\\n",
    "                                            &+ || \\mathbf{w}(3) ||^2 - || \\mathbf{w}(2) ||^2 \\\\\n",
    "                                            &+ || \\mathbf{w}(2) ||^2 - || \\mathbf{w}(1) ||^2 \\\\ \n",
    "                                            &+ || \\mathbf{w}(1) ||^2 - || \\mathbf{w}(0) ||^2 \\ge || \\mathbf{w}(k + 1) ||^2 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "|| \\mathbf{w}(k + 1) ||^2 \\le \\sum_{k = 1}^{n} || \\mathbf{x}(k) ||^2 \\le n \\cdot \\max_{\\mathbf{x}(k)} || \\mathbf{x}(k) ||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\therefore \\quad || \\mathbf{w}(k + 1) ||^2 \\le n \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{n^2 \\alpha^2}{|| \\mathbf{w}_o ||^2} \\le || \\mathbf{w}(k + 1) ||^2 \\le n \\beta \\quad (\\because (i), \\,\\ (ii))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\therefore \\quad n_{max} = \\max \\{ n = 1, 2, \\cdots, : \\frac{n^2 \\alpha^2}{|| \\mathbf{w}_o ||^2} = n \\beta \\} = \\frac{\\beta || \\mathbf{w}_o ||^2}{ \\alpha^2 } \\qquad \\blacksquare\n",
    "$$\n",
    "<br>\n",
    "\n",
    "- The upper bound exceeds the lower bound as $ n \\rightarrow \\infty $.\n",
    "- The derived upper and lower bounds should be satisfied for a finite value of $n$.\n",
    "- Thus, $ \\exists $ limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reference.</strong><br>\n",
    "Simon Haykin, Neural networks and learning machines<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
